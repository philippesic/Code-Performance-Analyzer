{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 7227,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.004151100041511001,
      "grad_norm": 4.3059797286987305,
      "learning_rate": 4.993773349937734e-05,
      "loss": 2.3651,
      "step": 10
    },
    {
      "epoch": 0.008302200083022002,
      "grad_norm": 5.32832670211792,
      "learning_rate": 4.987546699875467e-05,
      "loss": 2.0273,
      "step": 20
    },
    {
      "epoch": 0.012453300124533,
      "grad_norm": 5.461641788482666,
      "learning_rate": 4.980628199806282e-05,
      "loss": 2.1037,
      "step": 30
    },
    {
      "epoch": 0.016604400166044003,
      "grad_norm": 5.145603656768799,
      "learning_rate": 4.973709699737097e-05,
      "loss": 1.7298,
      "step": 40
    },
    {
      "epoch": 0.020755500207555,
      "grad_norm": 4.261973857879639,
      "learning_rate": 4.966791199667912e-05,
      "loss": 2.0019,
      "step": 50
    },
    {
      "epoch": 0.024906600249066,
      "grad_norm": 8.475198745727539,
      "learning_rate": 4.959872699598727e-05,
      "loss": 1.6866,
      "step": 60
    },
    {
      "epoch": 0.029057700290577002,
      "grad_norm": 4.758548736572266,
      "learning_rate": 4.952954199529542e-05,
      "loss": 1.7693,
      "step": 70
    },
    {
      "epoch": 0.033208800332088007,
      "grad_norm": 7.128340721130371,
      "learning_rate": 4.9460356994603574e-05,
      "loss": 1.6236,
      "step": 80
    },
    {
      "epoch": 0.037359900373599,
      "grad_norm": 2.7894136905670166,
      "learning_rate": 4.939809049398091e-05,
      "loss": 1.5289,
      "step": 90
    },
    {
      "epoch": 0.04151100041511,
      "grad_norm": 8.56430435180664,
      "learning_rate": 4.932890549328906e-05,
      "loss": 1.4763,
      "step": 100
    },
    {
      "epoch": 0.045662100456621,
      "grad_norm": 8.165678024291992,
      "learning_rate": 4.925972049259721e-05,
      "loss": 1.4228,
      "step": 110
    },
    {
      "epoch": 0.049813200498132,
      "grad_norm": 6.671384811401367,
      "learning_rate": 4.9190535491905357e-05,
      "loss": 1.1917,
      "step": 120
    },
    {
      "epoch": 0.053964300539643004,
      "grad_norm": 5.171438694000244,
      "learning_rate": 4.9121350491213505e-05,
      "loss": 1.1527,
      "step": 130
    },
    {
      "epoch": 0.058115400581154004,
      "grad_norm": 4.018444061279297,
      "learning_rate": 4.9052165490521654e-05,
      "loss": 1.0755,
      "step": 140
    },
    {
      "epoch": 0.062266500622665005,
      "grad_norm": 3.6970653533935547,
      "learning_rate": 4.89829804898298e-05,
      "loss": 1.0145,
      "step": 150
    },
    {
      "epoch": 0.06641760066417601,
      "grad_norm": 5.286396026611328,
      "learning_rate": 4.891379548913796e-05,
      "loss": 1.087,
      "step": 160
    },
    {
      "epoch": 0.070568700705687,
      "grad_norm": 3.5865163803100586,
      "learning_rate": 4.884461048844611e-05,
      "loss": 0.9438,
      "step": 170
    },
    {
      "epoch": 0.074719800747198,
      "grad_norm": 7.176278114318848,
      "learning_rate": 4.8775425487754255e-05,
      "loss": 0.9598,
      "step": 180
    },
    {
      "epoch": 0.07887090078870901,
      "grad_norm": 5.436966896057129,
      "learning_rate": 4.8706240487062404e-05,
      "loss": 0.8652,
      "step": 190
    },
    {
      "epoch": 0.08302200083022,
      "grad_norm": 11.377484321594238,
      "learning_rate": 4.863705548637056e-05,
      "loss": 0.9998,
      "step": 200
    },
    {
      "epoch": 0.08717310087173101,
      "grad_norm": 6.021442890167236,
      "learning_rate": 4.856787048567871e-05,
      "loss": 0.8509,
      "step": 210
    },
    {
      "epoch": 0.091324200913242,
      "grad_norm": 4.540760517120361,
      "learning_rate": 4.849868548498686e-05,
      "loss": 0.9948,
      "step": 220
    },
    {
      "epoch": 0.09547530095475301,
      "grad_norm": 4.142073154449463,
      "learning_rate": 4.842950048429501e-05,
      "loss": 0.9416,
      "step": 230
    },
    {
      "epoch": 0.099626400996264,
      "grad_norm": 3.2602484226226807,
      "learning_rate": 4.836031548360316e-05,
      "loss": 0.8154,
      "step": 240
    },
    {
      "epoch": 0.10377750103777501,
      "grad_norm": 3.810708999633789,
      "learning_rate": 4.829113048291131e-05,
      "loss": 0.7992,
      "step": 250
    },
    {
      "epoch": 0.10792860107928601,
      "grad_norm": 6.513706684112549,
      "learning_rate": 4.822194548221946e-05,
      "loss": 0.9033,
      "step": 260
    },
    {
      "epoch": 0.11207970112079702,
      "grad_norm": 2.9157445430755615,
      "learning_rate": 4.815276048152761e-05,
      "loss": 0.8887,
      "step": 270
    },
    {
      "epoch": 0.11623080116230801,
      "grad_norm": 5.460794925689697,
      "learning_rate": 4.8083575480835755e-05,
      "loss": 0.8226,
      "step": 280
    },
    {
      "epoch": 0.12038190120381902,
      "grad_norm": 4.34400749206543,
      "learning_rate": 4.8014390480143904e-05,
      "loss": 0.8097,
      "step": 290
    },
    {
      "epoch": 0.12453300124533001,
      "grad_norm": 5.633107662200928,
      "learning_rate": 4.794520547945205e-05,
      "loss": 0.8627,
      "step": 300
    },
    {
      "epoch": 0.12868410128684102,
      "grad_norm": 6.58763313293457,
      "learning_rate": 4.787602047876021e-05,
      "loss": 0.8072,
      "step": 310
    },
    {
      "epoch": 0.13283520132835203,
      "grad_norm": 5.8954386711120605,
      "learning_rate": 4.780683547806836e-05,
      "loss": 0.8613,
      "step": 320
    },
    {
      "epoch": 0.136986301369863,
      "grad_norm": 3.6711199283599854,
      "learning_rate": 4.7737650477376505e-05,
      "loss": 0.8413,
      "step": 330
    },
    {
      "epoch": 0.141137401411374,
      "grad_norm": 3.5405843257904053,
      "learning_rate": 4.766846547668466e-05,
      "loss": 0.7788,
      "step": 340
    },
    {
      "epoch": 0.14528850145288502,
      "grad_norm": 6.783627510070801,
      "learning_rate": 4.759928047599281e-05,
      "loss": 0.7846,
      "step": 350
    },
    {
      "epoch": 0.149439601494396,
      "grad_norm": 6.4385457038879395,
      "learning_rate": 4.753009547530096e-05,
      "loss": 0.8146,
      "step": 360
    },
    {
      "epoch": 0.153590701535907,
      "grad_norm": 3.5638692378997803,
      "learning_rate": 4.746091047460911e-05,
      "loss": 0.7538,
      "step": 370
    },
    {
      "epoch": 0.15774180157741802,
      "grad_norm": 2.7514185905456543,
      "learning_rate": 4.7391725473917256e-05,
      "loss": 0.7705,
      "step": 380
    },
    {
      "epoch": 0.16189290161892902,
      "grad_norm": 4.323678493499756,
      "learning_rate": 4.7322540473225404e-05,
      "loss": 0.7534,
      "step": 390
    },
    {
      "epoch": 0.16604400166044,
      "grad_norm": 20.162662506103516,
      "learning_rate": 4.725335547253355e-05,
      "loss": 0.7557,
      "step": 400
    },
    {
      "epoch": 0.170195101701951,
      "grad_norm": 3.529186248779297,
      "learning_rate": 4.71841704718417e-05,
      "loss": 0.7951,
      "step": 410
    },
    {
      "epoch": 0.17434620174346202,
      "grad_norm": 3.9123666286468506,
      "learning_rate": 4.711498547114986e-05,
      "loss": 0.7135,
      "step": 420
    },
    {
      "epoch": 0.17849730178497303,
      "grad_norm": 4.587635517120361,
      "learning_rate": 4.7045800470458006e-05,
      "loss": 0.7668,
      "step": 430
    },
    {
      "epoch": 0.182648401826484,
      "grad_norm": 3.369563579559326,
      "learning_rate": 4.6976615469766154e-05,
      "loss": 0.7018,
      "step": 440
    },
    {
      "epoch": 0.18679950186799502,
      "grad_norm": 6.754910469055176,
      "learning_rate": 4.690743046907431e-05,
      "loss": 0.8047,
      "step": 450
    },
    {
      "epoch": 0.19095060190950602,
      "grad_norm": 3.4845809936523438,
      "learning_rate": 4.683824546838246e-05,
      "loss": 0.6669,
      "step": 460
    },
    {
      "epoch": 0.19510170195101703,
      "grad_norm": 4.645039081573486,
      "learning_rate": 4.676906046769061e-05,
      "loss": 0.7772,
      "step": 470
    },
    {
      "epoch": 0.199252801992528,
      "grad_norm": 3.17895770072937,
      "learning_rate": 4.6699875466998756e-05,
      "loss": 0.7371,
      "step": 480
    },
    {
      "epoch": 0.20340390203403902,
      "grad_norm": 4.671679496765137,
      "learning_rate": 4.663069046630691e-05,
      "loss": 0.6668,
      "step": 490
    },
    {
      "epoch": 0.20755500207555003,
      "grad_norm": 5.362880706787109,
      "learning_rate": 4.656150546561506e-05,
      "loss": 0.7273,
      "step": 500
    },
    {
      "epoch": 0.21170610211706103,
      "grad_norm": 4.700826644897461,
      "learning_rate": 4.649232046492321e-05,
      "loss": 0.7526,
      "step": 510
    },
    {
      "epoch": 0.21585720215857201,
      "grad_norm": 3.257575273513794,
      "learning_rate": 4.642313546423136e-05,
      "loss": 0.6881,
      "step": 520
    },
    {
      "epoch": 0.22000830220008302,
      "grad_norm": 5.936589241027832,
      "learning_rate": 4.6353950463539506e-05,
      "loss": 0.6056,
      "step": 530
    },
    {
      "epoch": 0.22415940224159403,
      "grad_norm": 6.10017204284668,
      "learning_rate": 4.6284765462847654e-05,
      "loss": 0.7686,
      "step": 540
    },
    {
      "epoch": 0.228310502283105,
      "grad_norm": 2.9968924522399902,
      "learning_rate": 4.62155804621558e-05,
      "loss": 0.6752,
      "step": 550
    },
    {
      "epoch": 0.23246160232461602,
      "grad_norm": 3.506612539291382,
      "learning_rate": 4.614639546146396e-05,
      "loss": 0.681,
      "step": 560
    },
    {
      "epoch": 0.23661270236612703,
      "grad_norm": 3.0725836753845215,
      "learning_rate": 4.607721046077211e-05,
      "loss": 0.6352,
      "step": 570
    },
    {
      "epoch": 0.24076380240763803,
      "grad_norm": 4.44767951965332,
      "learning_rate": 4.6008025460080256e-05,
      "loss": 0.6645,
      "step": 580
    },
    {
      "epoch": 0.244914902449149,
      "grad_norm": 7.384735107421875,
      "learning_rate": 4.5938840459388404e-05,
      "loss": 0.7761,
      "step": 590
    },
    {
      "epoch": 0.24906600249066002,
      "grad_norm": 3.868206262588501,
      "learning_rate": 4.586965545869656e-05,
      "loss": 0.7425,
      "step": 600
    },
    {
      "epoch": 0.253217102532171,
      "grad_norm": 3.5629899501800537,
      "learning_rate": 4.580047045800471e-05,
      "loss": 0.7049,
      "step": 610
    },
    {
      "epoch": 0.25736820257368204,
      "grad_norm": 4.086206912994385,
      "learning_rate": 4.573128545731286e-05,
      "loss": 0.6563,
      "step": 620
    },
    {
      "epoch": 0.261519302615193,
      "grad_norm": 3.499788284301758,
      "learning_rate": 4.5662100456621006e-05,
      "loss": 0.6942,
      "step": 630
    },
    {
      "epoch": 0.26567040265670405,
      "grad_norm": 6.5791497230529785,
      "learning_rate": 4.559291545592916e-05,
      "loss": 0.7918,
      "step": 640
    },
    {
      "epoch": 0.26982150269821503,
      "grad_norm": 15.683422088623047,
      "learning_rate": 4.552373045523731e-05,
      "loss": 0.5997,
      "step": 650
    },
    {
      "epoch": 0.273972602739726,
      "grad_norm": 3.044811725616455,
      "learning_rate": 4.545454545454546e-05,
      "loss": 0.6764,
      "step": 660
    },
    {
      "epoch": 0.27812370278123705,
      "grad_norm": 3.596010684967041,
      "learning_rate": 4.538536045385361e-05,
      "loss": 0.6226,
      "step": 670
    },
    {
      "epoch": 0.282274802822748,
      "grad_norm": 7.694983005523682,
      "learning_rate": 4.5316175453161756e-05,
      "loss": 0.7318,
      "step": 680
    },
    {
      "epoch": 0.286425902864259,
      "grad_norm": 5.023863315582275,
      "learning_rate": 4.525390895253909e-05,
      "loss": 0.6891,
      "step": 690
    },
    {
      "epoch": 0.29057700290577004,
      "grad_norm": 3.712481737136841,
      "learning_rate": 4.518472395184724e-05,
      "loss": 0.6016,
      "step": 700
    },
    {
      "epoch": 0.294728102947281,
      "grad_norm": 4.44331693649292,
      "learning_rate": 4.511553895115539e-05,
      "loss": 0.7168,
      "step": 710
    },
    {
      "epoch": 0.298879202988792,
      "grad_norm": 4.928530216217041,
      "learning_rate": 4.5046353950463545e-05,
      "loss": 0.6803,
      "step": 720
    },
    {
      "epoch": 0.30303030303030304,
      "grad_norm": 4.051958084106445,
      "learning_rate": 4.4977168949771694e-05,
      "loss": 0.6847,
      "step": 730
    },
    {
      "epoch": 0.307181403071814,
      "grad_norm": 2.4930758476257324,
      "learning_rate": 4.490798394907984e-05,
      "loss": 0.6885,
      "step": 740
    },
    {
      "epoch": 0.31133250311332505,
      "grad_norm": 5.4142842292785645,
      "learning_rate": 4.483879894838799e-05,
      "loss": 0.653,
      "step": 750
    },
    {
      "epoch": 0.31548360315483603,
      "grad_norm": 5.446802139282227,
      "learning_rate": 4.476961394769614e-05,
      "loss": 0.5972,
      "step": 760
    },
    {
      "epoch": 0.319634703196347,
      "grad_norm": 4.371009826660156,
      "learning_rate": 4.470042894700429e-05,
      "loss": 0.6769,
      "step": 770
    },
    {
      "epoch": 0.32378580323785805,
      "grad_norm": 5.210651874542236,
      "learning_rate": 4.463124394631244e-05,
      "loss": 0.7557,
      "step": 780
    },
    {
      "epoch": 0.32793690327936903,
      "grad_norm": 2.723313093185425,
      "learning_rate": 4.4568977445689774e-05,
      "loss": 0.592,
      "step": 790
    },
    {
      "epoch": 0.33208800332088,
      "grad_norm": 3.291943073272705,
      "learning_rate": 4.449979244499793e-05,
      "loss": 0.6899,
      "step": 800
    },
    {
      "epoch": 0.33623910336239105,
      "grad_norm": 4.254903316497803,
      "learning_rate": 4.443060744430608e-05,
      "loss": 0.6572,
      "step": 810
    },
    {
      "epoch": 0.340390203403902,
      "grad_norm": 3.4049999713897705,
      "learning_rate": 4.4361422443614226e-05,
      "loss": 0.613,
      "step": 820
    },
    {
      "epoch": 0.34454130344541306,
      "grad_norm": 5.427980899810791,
      "learning_rate": 4.4292237442922375e-05,
      "loss": 0.6448,
      "step": 830
    },
    {
      "epoch": 0.34869240348692404,
      "grad_norm": 2.812769651412964,
      "learning_rate": 4.422305244223053e-05,
      "loss": 0.6096,
      "step": 840
    },
    {
      "epoch": 0.352843503528435,
      "grad_norm": 4.750669956207275,
      "learning_rate": 4.415386744153868e-05,
      "loss": 0.7245,
      "step": 850
    },
    {
      "epoch": 0.35699460356994606,
      "grad_norm": 5.736929416656494,
      "learning_rate": 4.408468244084683e-05,
      "loss": 0.6604,
      "step": 860
    },
    {
      "epoch": 0.36114570361145704,
      "grad_norm": 3.6056346893310547,
      "learning_rate": 4.4015497440154976e-05,
      "loss": 0.6988,
      "step": 870
    },
    {
      "epoch": 0.365296803652968,
      "grad_norm": 4.56833553314209,
      "learning_rate": 4.3946312439463125e-05,
      "loss": 0.6013,
      "step": 880
    },
    {
      "epoch": 0.36944790369447905,
      "grad_norm": 3.882204055786133,
      "learning_rate": 4.3877127438771274e-05,
      "loss": 0.6053,
      "step": 890
    },
    {
      "epoch": 0.37359900373599003,
      "grad_norm": 3.30889892578125,
      "learning_rate": 4.380794243807942e-05,
      "loss": 0.6499,
      "step": 900
    },
    {
      "epoch": 0.377750103777501,
      "grad_norm": 7.274662971496582,
      "learning_rate": 4.373875743738758e-05,
      "loss": 0.7054,
      "step": 910
    },
    {
      "epoch": 0.38190120381901205,
      "grad_norm": 3.6469690799713135,
      "learning_rate": 4.3669572436695726e-05,
      "loss": 0.6477,
      "step": 920
    },
    {
      "epoch": 0.386052303860523,
      "grad_norm": 4.538723945617676,
      "learning_rate": 4.3600387436003875e-05,
      "loss": 0.6273,
      "step": 930
    },
    {
      "epoch": 0.39020340390203406,
      "grad_norm": 3.23056960105896,
      "learning_rate": 4.3531202435312024e-05,
      "loss": 0.613,
      "step": 940
    },
    {
      "epoch": 0.39435450394354504,
      "grad_norm": 7.642653942108154,
      "learning_rate": 4.346201743462018e-05,
      "loss": 0.633,
      "step": 950
    },
    {
      "epoch": 0.398505603985056,
      "grad_norm": 3.3546154499053955,
      "learning_rate": 4.339283243392833e-05,
      "loss": 0.6089,
      "step": 960
    },
    {
      "epoch": 0.40265670402656706,
      "grad_norm": 4.321519374847412,
      "learning_rate": 4.3323647433236476e-05,
      "loss": 0.6923,
      "step": 970
    },
    {
      "epoch": 0.40680780406807804,
      "grad_norm": 2.3863086700439453,
      "learning_rate": 4.325446243254463e-05,
      "loss": 0.5658,
      "step": 980
    },
    {
      "epoch": 0.410958904109589,
      "grad_norm": 3.6434895992279053,
      "learning_rate": 4.318527743185278e-05,
      "loss": 0.6028,
      "step": 990
    },
    {
      "epoch": 0.41511000415110005,
      "grad_norm": 5.725226402282715,
      "learning_rate": 4.311609243116093e-05,
      "loss": 0.6325,
      "step": 1000
    },
    {
      "epoch": 0.41926110419261103,
      "grad_norm": 4.76446533203125,
      "learning_rate": 4.304690743046907e-05,
      "loss": 0.5909,
      "step": 1010
    },
    {
      "epoch": 0.42341220423412207,
      "grad_norm": 4.214288711547852,
      "learning_rate": 4.2977722429777227e-05,
      "loss": 0.6004,
      "step": 1020
    },
    {
      "epoch": 0.42756330427563305,
      "grad_norm": 4.735945224761963,
      "learning_rate": 4.2908537429085375e-05,
      "loss": 0.5784,
      "step": 1030
    },
    {
      "epoch": 0.43171440431714403,
      "grad_norm": 4.180710315704346,
      "learning_rate": 4.2839352428393524e-05,
      "loss": 0.6428,
      "step": 1040
    },
    {
      "epoch": 0.43586550435865506,
      "grad_norm": 4.279062747955322,
      "learning_rate": 4.277016742770167e-05,
      "loss": 0.6835,
      "step": 1050
    },
    {
      "epoch": 0.44001660440016604,
      "grad_norm": 3.9372341632843018,
      "learning_rate": 4.270098242700983e-05,
      "loss": 0.6512,
      "step": 1060
    },
    {
      "epoch": 0.444167704441677,
      "grad_norm": 3.824455738067627,
      "learning_rate": 4.2631797426317977e-05,
      "loss": 0.6949,
      "step": 1070
    },
    {
      "epoch": 0.44831880448318806,
      "grad_norm": 4.789069652557373,
      "learning_rate": 4.2562612425626125e-05,
      "loss": 0.5922,
      "step": 1080
    },
    {
      "epoch": 0.45246990452469904,
      "grad_norm": 5.5115966796875,
      "learning_rate": 4.249342742493428e-05,
      "loss": 0.6381,
      "step": 1090
    },
    {
      "epoch": 0.45662100456621,
      "grad_norm": 4.42945671081543,
      "learning_rate": 4.242424242424243e-05,
      "loss": 0.61,
      "step": 1100
    },
    {
      "epoch": 0.46077210460772106,
      "grad_norm": 2.34271502494812,
      "learning_rate": 4.235505742355058e-05,
      "loss": 0.5765,
      "step": 1110
    },
    {
      "epoch": 0.46492320464923204,
      "grad_norm": 2.8148207664489746,
      "learning_rate": 4.228587242285873e-05,
      "loss": 0.5204,
      "step": 1120
    },
    {
      "epoch": 0.46907430469074307,
      "grad_norm": 3.144775867462158,
      "learning_rate": 4.2216687422166875e-05,
      "loss": 0.6475,
      "step": 1130
    },
    {
      "epoch": 0.47322540473225405,
      "grad_norm": 4.02853536605835,
      "learning_rate": 4.2147502421475024e-05,
      "loss": 0.6311,
      "step": 1140
    },
    {
      "epoch": 0.47737650477376503,
      "grad_norm": 4.796026229858398,
      "learning_rate": 4.207831742078317e-05,
      "loss": 0.5695,
      "step": 1150
    },
    {
      "epoch": 0.48152760481527607,
      "grad_norm": 3.2646241188049316,
      "learning_rate": 4.200913242009132e-05,
      "loss": 0.6818,
      "step": 1160
    },
    {
      "epoch": 0.48567870485678705,
      "grad_norm": 5.142189025878906,
      "learning_rate": 4.193994741939948e-05,
      "loss": 0.6938,
      "step": 1170
    },
    {
      "epoch": 0.489829804898298,
      "grad_norm": 2.798579216003418,
      "learning_rate": 4.1870762418707625e-05,
      "loss": 0.5887,
      "step": 1180
    },
    {
      "epoch": 0.49398090493980906,
      "grad_norm": 4.962470531463623,
      "learning_rate": 4.1801577418015774e-05,
      "loss": 0.5715,
      "step": 1190
    },
    {
      "epoch": 0.49813200498132004,
      "grad_norm": 5.0474443435668945,
      "learning_rate": 4.173239241732393e-05,
      "loss": 0.533,
      "step": 1200
    },
    {
      "epoch": 0.502283105022831,
      "grad_norm": 4.618270397186279,
      "learning_rate": 4.166320741663208e-05,
      "loss": 0.6175,
      "step": 1210
    },
    {
      "epoch": 0.506434205064342,
      "grad_norm": 3.824035406112671,
      "learning_rate": 4.159402241594023e-05,
      "loss": 0.6191,
      "step": 1220
    },
    {
      "epoch": 0.5105853051058531,
      "grad_norm": 3.0424234867095947,
      "learning_rate": 4.1524837415248375e-05,
      "loss": 0.5774,
      "step": 1230
    },
    {
      "epoch": 0.5147364051473641,
      "grad_norm": 2.5544328689575195,
      "learning_rate": 4.145565241455653e-05,
      "loss": 0.6728,
      "step": 1240
    },
    {
      "epoch": 0.518887505188875,
      "grad_norm": 5.171296119689941,
      "learning_rate": 4.138646741386468e-05,
      "loss": 0.6127,
      "step": 1250
    },
    {
      "epoch": 0.523038605230386,
      "grad_norm": 3.2016706466674805,
      "learning_rate": 4.131728241317283e-05,
      "loss": 0.6162,
      "step": 1260
    },
    {
      "epoch": 0.527189705271897,
      "grad_norm": 3.616546154022217,
      "learning_rate": 4.124809741248098e-05,
      "loss": 0.5987,
      "step": 1270
    },
    {
      "epoch": 0.5313408053134081,
      "grad_norm": 2.56830096244812,
      "learning_rate": 4.1178912411789126e-05,
      "loss": 0.6124,
      "step": 1280
    },
    {
      "epoch": 0.5354919053549191,
      "grad_norm": 2.901643753051758,
      "learning_rate": 4.1109727411097274e-05,
      "loss": 0.5658,
      "step": 1290
    },
    {
      "epoch": 0.5396430053964301,
      "grad_norm": 5.079092979431152,
      "learning_rate": 4.104054241040542e-05,
      "loss": 0.6031,
      "step": 1300
    },
    {
      "epoch": 0.543794105437941,
      "grad_norm": 4.434463977813721,
      "learning_rate": 4.097135740971358e-05,
      "loss": 0.6169,
      "step": 1310
    },
    {
      "epoch": 0.547945205479452,
      "grad_norm": 3.5564939975738525,
      "learning_rate": 4.090217240902173e-05,
      "loss": 0.5398,
      "step": 1320
    },
    {
      "epoch": 0.552096305520963,
      "grad_norm": 5.306328773498535,
      "learning_rate": 4.0832987408329876e-05,
      "loss": 0.6875,
      "step": 1330
    },
    {
      "epoch": 0.5562474055624741,
      "grad_norm": 3.3287551403045654,
      "learning_rate": 4.0763802407638024e-05,
      "loss": 0.4883,
      "step": 1340
    },
    {
      "epoch": 0.5603985056039851,
      "grad_norm": 11.713359832763672,
      "learning_rate": 4.069461740694618e-05,
      "loss": 0.5605,
      "step": 1350
    },
    {
      "epoch": 0.564549605645496,
      "grad_norm": 3.2552103996276855,
      "learning_rate": 4.062543240625433e-05,
      "loss": 0.604,
      "step": 1360
    },
    {
      "epoch": 0.568700705687007,
      "grad_norm": 5.5841546058654785,
      "learning_rate": 4.055624740556248e-05,
      "loss": 0.5902,
      "step": 1370
    },
    {
      "epoch": 0.572851805728518,
      "grad_norm": 3.5645060539245605,
      "learning_rate": 4.0487062404870626e-05,
      "loss": 0.6335,
      "step": 1380
    },
    {
      "epoch": 0.5770029057700291,
      "grad_norm": 3.364152669906616,
      "learning_rate": 4.041787740417878e-05,
      "loss": 0.597,
      "step": 1390
    },
    {
      "epoch": 0.5811540058115401,
      "grad_norm": 4.414831161499023,
      "learning_rate": 4.034869240348692e-05,
      "loss": 0.5469,
      "step": 1400
    },
    {
      "epoch": 0.5853051058530511,
      "grad_norm": 2.3530948162078857,
      "learning_rate": 4.027950740279507e-05,
      "loss": 0.6027,
      "step": 1410
    },
    {
      "epoch": 0.589456205894562,
      "grad_norm": 5.299890518188477,
      "learning_rate": 4.021032240210322e-05,
      "loss": 0.6032,
      "step": 1420
    },
    {
      "epoch": 0.593607305936073,
      "grad_norm": 2.674837589263916,
      "learning_rate": 4.0141137401411376e-05,
      "loss": 0.4951,
      "step": 1430
    },
    {
      "epoch": 0.597758405977584,
      "grad_norm": 4.216536998748779,
      "learning_rate": 4.0071952400719524e-05,
      "loss": 0.5074,
      "step": 1440
    },
    {
      "epoch": 0.6019095060190951,
      "grad_norm": 3.873297691345215,
      "learning_rate": 4.000276740002767e-05,
      "loss": 0.4705,
      "step": 1450
    },
    {
      "epoch": 0.6060606060606061,
      "grad_norm": 5.626718044281006,
      "learning_rate": 3.993358239933583e-05,
      "loss": 0.6115,
      "step": 1460
    },
    {
      "epoch": 0.6102117061021171,
      "grad_norm": 2.7141988277435303,
      "learning_rate": 3.986439739864398e-05,
      "loss": 0.5846,
      "step": 1470
    },
    {
      "epoch": 0.614362806143628,
      "grad_norm": 3.8230459690093994,
      "learning_rate": 3.9795212397952126e-05,
      "loss": 0.4977,
      "step": 1480
    },
    {
      "epoch": 0.618513906185139,
      "grad_norm": 2.778031587600708,
      "learning_rate": 3.9726027397260274e-05,
      "loss": 0.5231,
      "step": 1490
    },
    {
      "epoch": 0.6226650062266501,
      "grad_norm": 6.485655784606934,
      "learning_rate": 3.965684239656843e-05,
      "loss": 0.6124,
      "step": 1500
    },
    {
      "epoch": 0.6268161062681611,
      "grad_norm": 2.7792491912841797,
      "learning_rate": 3.958765739587658e-05,
      "loss": 0.5245,
      "step": 1510
    },
    {
      "epoch": 0.6309672063096721,
      "grad_norm": 4.119534492492676,
      "learning_rate": 3.951847239518473e-05,
      "loss": 0.5727,
      "step": 1520
    },
    {
      "epoch": 0.635118306351183,
      "grad_norm": 4.6580305099487305,
      "learning_rate": 3.9449287394492876e-05,
      "loss": 0.5483,
      "step": 1530
    },
    {
      "epoch": 0.639269406392694,
      "grad_norm": 3.592411994934082,
      "learning_rate": 3.9380102393801024e-05,
      "loss": 0.5688,
      "step": 1540
    },
    {
      "epoch": 0.6434205064342051,
      "grad_norm": 4.661876201629639,
      "learning_rate": 3.931091739310917e-05,
      "loss": 0.5815,
      "step": 1550
    },
    {
      "epoch": 0.6475716064757161,
      "grad_norm": 3.276867628097534,
      "learning_rate": 3.924173239241732e-05,
      "loss": 0.534,
      "step": 1560
    },
    {
      "epoch": 0.6517227065172271,
      "grad_norm": 4.045099258422852,
      "learning_rate": 3.917254739172548e-05,
      "loss": 0.5771,
      "step": 1570
    },
    {
      "epoch": 0.6558738065587381,
      "grad_norm": 5.725621223449707,
      "learning_rate": 3.9103362391033626e-05,
      "loss": 0.5693,
      "step": 1580
    },
    {
      "epoch": 0.660024906600249,
      "grad_norm": 4.906647205352783,
      "learning_rate": 3.9034177390341775e-05,
      "loss": 0.5232,
      "step": 1590
    },
    {
      "epoch": 0.66417600664176,
      "grad_norm": 4.201799392700195,
      "learning_rate": 3.896499238964992e-05,
      "loss": 0.5031,
      "step": 1600
    },
    {
      "epoch": 0.6683271066832711,
      "grad_norm": 3.6639750003814697,
      "learning_rate": 3.889580738895808e-05,
      "loss": 0.5229,
      "step": 1610
    },
    {
      "epoch": 0.6724782067247821,
      "grad_norm": 3.770975112915039,
      "learning_rate": 3.882662238826623e-05,
      "loss": 0.6617,
      "step": 1620
    },
    {
      "epoch": 0.6766293067662931,
      "grad_norm": 3.1657261848449707,
      "learning_rate": 3.8757437387574376e-05,
      "loss": 0.5683,
      "step": 1630
    },
    {
      "epoch": 0.680780406807804,
      "grad_norm": 2.6498820781707764,
      "learning_rate": 3.8688252386882525e-05,
      "loss": 0.5697,
      "step": 1640
    },
    {
      "epoch": 0.684931506849315,
      "grad_norm": 4.305773735046387,
      "learning_rate": 3.861906738619068e-05,
      "loss": 0.5939,
      "step": 1650
    },
    {
      "epoch": 0.6890826068908261,
      "grad_norm": 3.7766902446746826,
      "learning_rate": 3.854988238549883e-05,
      "loss": 0.5642,
      "step": 1660
    },
    {
      "epoch": 0.6932337069323371,
      "grad_norm": 3.049074172973633,
      "learning_rate": 3.848069738480698e-05,
      "loss": 0.6214,
      "step": 1670
    },
    {
      "epoch": 0.6973848069738481,
      "grad_norm": 3.0247316360473633,
      "learning_rate": 3.8411512384115126e-05,
      "loss": 0.6367,
      "step": 1680
    },
    {
      "epoch": 0.7015359070153591,
      "grad_norm": 3.8550875186920166,
      "learning_rate": 3.8342327383423275e-05,
      "loss": 0.5248,
      "step": 1690
    },
    {
      "epoch": 0.70568700705687,
      "grad_norm": 2.5520851612091064,
      "learning_rate": 3.827314238273142e-05,
      "loss": 0.4606,
      "step": 1700
    },
    {
      "epoch": 0.709838107098381,
      "grad_norm": 3.9750661849975586,
      "learning_rate": 3.820395738203957e-05,
      "loss": 0.6054,
      "step": 1710
    },
    {
      "epoch": 0.7139892071398921,
      "grad_norm": 3.8585760593414307,
      "learning_rate": 3.813477238134773e-05,
      "loss": 0.5427,
      "step": 1720
    },
    {
      "epoch": 0.7181403071814031,
      "grad_norm": 6.423068046569824,
      "learning_rate": 3.8065587380655876e-05,
      "loss": 0.5669,
      "step": 1730
    },
    {
      "epoch": 0.7222914072229141,
      "grad_norm": 3.4498531818389893,
      "learning_rate": 3.7996402379964025e-05,
      "loss": 0.6031,
      "step": 1740
    },
    {
      "epoch": 0.726442507264425,
      "grad_norm": 5.3525614738464355,
      "learning_rate": 3.792721737927217e-05,
      "loss": 0.5492,
      "step": 1750
    },
    {
      "epoch": 0.730593607305936,
      "grad_norm": 4.396335601806641,
      "learning_rate": 3.785803237858033e-05,
      "loss": 0.5912,
      "step": 1760
    },
    {
      "epoch": 0.7347447073474471,
      "grad_norm": 4.381235599517822,
      "learning_rate": 3.778884737788848e-05,
      "loss": 0.4955,
      "step": 1770
    },
    {
      "epoch": 0.7388958073889581,
      "grad_norm": 3.1759283542633057,
      "learning_rate": 3.7719662377196626e-05,
      "loss": 0.5498,
      "step": 1780
    },
    {
      "epoch": 0.7430469074304691,
      "grad_norm": 4.651916027069092,
      "learning_rate": 3.7650477376504775e-05,
      "loss": 0.565,
      "step": 1790
    },
    {
      "epoch": 0.7471980074719801,
      "grad_norm": 4.621772766113281,
      "learning_rate": 3.7581292375812923e-05,
      "loss": 0.5565,
      "step": 1800
    },
    {
      "epoch": 0.751349107513491,
      "grad_norm": 6.332614898681641,
      "learning_rate": 3.751210737512107e-05,
      "loss": 0.5819,
      "step": 1810
    },
    {
      "epoch": 0.755500207555002,
      "grad_norm": 2.5502419471740723,
      "learning_rate": 3.744292237442922e-05,
      "loss": 0.6675,
      "step": 1820
    },
    {
      "epoch": 0.7596513075965131,
      "grad_norm": 2.797708749771118,
      "learning_rate": 3.7373737373737376e-05,
      "loss": 0.4733,
      "step": 1830
    },
    {
      "epoch": 0.7638024076380241,
      "grad_norm": 3.719714879989624,
      "learning_rate": 3.7304552373045525e-05,
      "loss": 0.5567,
      "step": 1840
    },
    {
      "epoch": 0.7679535076795351,
      "grad_norm": 3.7618963718414307,
      "learning_rate": 3.7235367372353673e-05,
      "loss": 0.5111,
      "step": 1850
    },
    {
      "epoch": 0.772104607721046,
      "grad_norm": 3.2469334602355957,
      "learning_rate": 3.716618237166182e-05,
      "loss": 0.5171,
      "step": 1860
    },
    {
      "epoch": 0.776255707762557,
      "grad_norm": 3.2936851978302,
      "learning_rate": 3.709699737096998e-05,
      "loss": 0.5467,
      "step": 1870
    },
    {
      "epoch": 0.7804068078040681,
      "grad_norm": 3.216890335083008,
      "learning_rate": 3.7027812370278126e-05,
      "loss": 0.5156,
      "step": 1880
    },
    {
      "epoch": 0.7845579078455791,
      "grad_norm": 4.568055629730225,
      "learning_rate": 3.6958627369586275e-05,
      "loss": 0.5537,
      "step": 1890
    },
    {
      "epoch": 0.7887090078870901,
      "grad_norm": 2.3792173862457275,
      "learning_rate": 3.688944236889443e-05,
      "loss": 0.5039,
      "step": 1900
    },
    {
      "epoch": 0.7928601079286011,
      "grad_norm": 2.519968032836914,
      "learning_rate": 3.682025736820258e-05,
      "loss": 0.5656,
      "step": 1910
    },
    {
      "epoch": 0.797011207970112,
      "grad_norm": 3.2420809268951416,
      "learning_rate": 3.675107236751073e-05,
      "loss": 0.5904,
      "step": 1920
    },
    {
      "epoch": 0.801162308011623,
      "grad_norm": 4.1765546798706055,
      "learning_rate": 3.6681887366818876e-05,
      "loss": 0.5191,
      "step": 1930
    },
    {
      "epoch": 0.8053134080531341,
      "grad_norm": 4.531266689300537,
      "learning_rate": 3.6612702366127025e-05,
      "loss": 0.4903,
      "step": 1940
    },
    {
      "epoch": 0.8094645080946451,
      "grad_norm": 5.840338706970215,
      "learning_rate": 3.6543517365435174e-05,
      "loss": 0.5772,
      "step": 1950
    },
    {
      "epoch": 0.8136156081361561,
      "grad_norm": 3.1464059352874756,
      "learning_rate": 3.647433236474332e-05,
      "loss": 0.5316,
      "step": 1960
    },
    {
      "epoch": 0.8177667081776671,
      "grad_norm": 3.299571990966797,
      "learning_rate": 3.640514736405147e-05,
      "loss": 0.5494,
      "step": 1970
    },
    {
      "epoch": 0.821917808219178,
      "grad_norm": 3.856428623199463,
      "learning_rate": 3.6335962363359626e-05,
      "loss": 0.5421,
      "step": 1980
    },
    {
      "epoch": 0.8260689082606891,
      "grad_norm": 3.9884438514709473,
      "learning_rate": 3.6266777362667775e-05,
      "loss": 0.4803,
      "step": 1990
    },
    {
      "epoch": 0.8302200083022001,
      "grad_norm": 3.518778085708618,
      "learning_rate": 3.6197592361975924e-05,
      "loss": 0.547,
      "step": 2000
    },
    {
      "epoch": 0.8343711083437111,
      "grad_norm": 4.280359268188477,
      "learning_rate": 3.612840736128408e-05,
      "loss": 0.5567,
      "step": 2010
    },
    {
      "epoch": 0.8385222083852221,
      "grad_norm": 3.213529348373413,
      "learning_rate": 3.605922236059223e-05,
      "loss": 0.5665,
      "step": 2020
    },
    {
      "epoch": 0.842673308426733,
      "grad_norm": 2.712693929672241,
      "learning_rate": 3.5990037359900376e-05,
      "loss": 0.5842,
      "step": 2030
    },
    {
      "epoch": 0.8468244084682441,
      "grad_norm": 4.216360092163086,
      "learning_rate": 3.5920852359208525e-05,
      "loss": 0.6182,
      "step": 2040
    },
    {
      "epoch": 0.8509755085097551,
      "grad_norm": 2.9933273792266846,
      "learning_rate": 3.585166735851668e-05,
      "loss": 0.4744,
      "step": 2050
    },
    {
      "epoch": 0.8551266085512661,
      "grad_norm": 3.5488507747650146,
      "learning_rate": 3.578248235782483e-05,
      "loss": 0.5429,
      "step": 2060
    },
    {
      "epoch": 0.8592777085927771,
      "grad_norm": 3.691793203353882,
      "learning_rate": 3.571329735713297e-05,
      "loss": 0.5185,
      "step": 2070
    },
    {
      "epoch": 0.8634288086342881,
      "grad_norm": 2.6514699459075928,
      "learning_rate": 3.564411235644112e-05,
      "loss": 0.6152,
      "step": 2080
    },
    {
      "epoch": 0.867579908675799,
      "grad_norm": 2.8602375984191895,
      "learning_rate": 3.5574927355749275e-05,
      "loss": 0.5007,
      "step": 2090
    },
    {
      "epoch": 0.8717310087173101,
      "grad_norm": 4.239212512969971,
      "learning_rate": 3.5505742355057424e-05,
      "loss": 0.6189,
      "step": 2100
    },
    {
      "epoch": 0.8758821087588211,
      "grad_norm": 3.1877667903900146,
      "learning_rate": 3.543655735436557e-05,
      "loss": 0.5713,
      "step": 2110
    },
    {
      "epoch": 0.8800332088003321,
      "grad_norm": 4.832635402679443,
      "learning_rate": 3.536737235367373e-05,
      "loss": 0.5141,
      "step": 2120
    },
    {
      "epoch": 0.8841843088418431,
      "grad_norm": 3.5700490474700928,
      "learning_rate": 3.5298187352981876e-05,
      "loss": 0.5148,
      "step": 2130
    },
    {
      "epoch": 0.888335408883354,
      "grad_norm": 3.058683156967163,
      "learning_rate": 3.5229002352290025e-05,
      "loss": 0.5014,
      "step": 2140
    },
    {
      "epoch": 0.8924865089248651,
      "grad_norm": 7.682463645935059,
      "learning_rate": 3.5159817351598174e-05,
      "loss": 0.5827,
      "step": 2150
    },
    {
      "epoch": 0.8966376089663761,
      "grad_norm": 5.000410079956055,
      "learning_rate": 3.509063235090633e-05,
      "loss": 0.55,
      "step": 2160
    },
    {
      "epoch": 0.9007887090078871,
      "grad_norm": 2.5824763774871826,
      "learning_rate": 3.502144735021448e-05,
      "loss": 0.5482,
      "step": 2170
    },
    {
      "epoch": 0.9049398090493981,
      "grad_norm": 3.9778177738189697,
      "learning_rate": 3.4952262349522627e-05,
      "loss": 0.5441,
      "step": 2180
    },
    {
      "epoch": 0.9090909090909091,
      "grad_norm": 3.404181480407715,
      "learning_rate": 3.4883077348830775e-05,
      "loss": 0.5046,
      "step": 2190
    },
    {
      "epoch": 0.91324200913242,
      "grad_norm": 2.8496434688568115,
      "learning_rate": 3.4813892348138924e-05,
      "loss": 0.4673,
      "step": 2200
    },
    {
      "epoch": 0.9173931091739311,
      "grad_norm": 3.077585458755493,
      "learning_rate": 3.474470734744707e-05,
      "loss": 0.5104,
      "step": 2210
    },
    {
      "epoch": 0.9215442092154421,
      "grad_norm": 3.2280683517456055,
      "learning_rate": 3.467552234675522e-05,
      "loss": 0.5147,
      "step": 2220
    },
    {
      "epoch": 0.9256953092569531,
      "grad_norm": 2.244978904724121,
      "learning_rate": 3.460633734606338e-05,
      "loss": 0.5124,
      "step": 2230
    },
    {
      "epoch": 0.9298464092984641,
      "grad_norm": 3.613041639328003,
      "learning_rate": 3.4537152345371525e-05,
      "loss": 0.5542,
      "step": 2240
    },
    {
      "epoch": 0.933997509339975,
      "grad_norm": 5.867563724517822,
      "learning_rate": 3.4467967344679674e-05,
      "loss": 0.5562,
      "step": 2250
    },
    {
      "epoch": 0.9381486093814861,
      "grad_norm": 2.5372133255004883,
      "learning_rate": 3.439878234398782e-05,
      "loss": 0.5013,
      "step": 2260
    },
    {
      "epoch": 0.9422997094229971,
      "grad_norm": 2.4842917919158936,
      "learning_rate": 3.432959734329598e-05,
      "loss": 0.4733,
      "step": 2270
    },
    {
      "epoch": 0.9464508094645081,
      "grad_norm": 3.036280393600464,
      "learning_rate": 3.426041234260413e-05,
      "loss": 0.552,
      "step": 2280
    },
    {
      "epoch": 0.9506019095060191,
      "grad_norm": 3.1855993270874023,
      "learning_rate": 3.4191227341912275e-05,
      "loss": 0.5105,
      "step": 2290
    },
    {
      "epoch": 0.9547530095475301,
      "grad_norm": 3.0526599884033203,
      "learning_rate": 3.4122042341220424e-05,
      "loss": 0.5611,
      "step": 2300
    },
    {
      "epoch": 0.958904109589041,
      "grad_norm": 2.126492500305176,
      "learning_rate": 3.405285734052858e-05,
      "loss": 0.537,
      "step": 2310
    },
    {
      "epoch": 0.9630552096305521,
      "grad_norm": 3.7778708934783936,
      "learning_rate": 3.398367233983673e-05,
      "loss": 0.4898,
      "step": 2320
    },
    {
      "epoch": 0.9672063096720631,
      "grad_norm": 2.8898062705993652,
      "learning_rate": 3.391448733914488e-05,
      "loss": 0.4784,
      "step": 2330
    },
    {
      "epoch": 0.9713574097135741,
      "grad_norm": 2.3593060970306396,
      "learning_rate": 3.3845302338453025e-05,
      "loss": 0.4986,
      "step": 2340
    },
    {
      "epoch": 0.9755085097550851,
      "grad_norm": 3.8027396202087402,
      "learning_rate": 3.3776117337761174e-05,
      "loss": 0.5604,
      "step": 2350
    },
    {
      "epoch": 0.979659609796596,
      "grad_norm": 2.623788356781006,
      "learning_rate": 3.370693233706932e-05,
      "loss": 0.4588,
      "step": 2360
    },
    {
      "epoch": 0.9838107098381071,
      "grad_norm": 2.903848648071289,
      "learning_rate": 3.363774733637747e-05,
      "loss": 0.4622,
      "step": 2370
    },
    {
      "epoch": 0.9879618098796181,
      "grad_norm": 2.6867644786834717,
      "learning_rate": 3.356856233568563e-05,
      "loss": 0.535,
      "step": 2380
    },
    {
      "epoch": 0.9921129099211291,
      "grad_norm": 3.5993893146514893,
      "learning_rate": 3.3499377334993775e-05,
      "loss": 0.4805,
      "step": 2390
    },
    {
      "epoch": 0.9962640099626401,
      "grad_norm": 3.97347092628479,
      "learning_rate": 3.3430192334301924e-05,
      "loss": 0.5754,
      "step": 2400
    },
    {
      "epoch": 1.0004151100041512,
      "grad_norm": 4.304186820983887,
      "learning_rate": 3.336100733361007e-05,
      "loss": 0.4976,
      "step": 2410
    },
    {
      "epoch": 1.004566210045662,
      "grad_norm": 5.591305732727051,
      "learning_rate": 3.329182233291823e-05,
      "loss": 0.5514,
      "step": 2420
    },
    {
      "epoch": 1.0087173100871731,
      "grad_norm": 2.7831006050109863,
      "learning_rate": 3.322263733222638e-05,
      "loss": 0.553,
      "step": 2430
    },
    {
      "epoch": 1.012868410128684,
      "grad_norm": 2.9220006465911865,
      "learning_rate": 3.3153452331534526e-05,
      "loss": 0.507,
      "step": 2440
    },
    {
      "epoch": 1.017019510170195,
      "grad_norm": 3.963641881942749,
      "learning_rate": 3.308426733084268e-05,
      "loss": 0.5579,
      "step": 2450
    },
    {
      "epoch": 1.0211706102117062,
      "grad_norm": 2.416908025741577,
      "learning_rate": 3.301508233015082e-05,
      "loss": 0.5464,
      "step": 2460
    },
    {
      "epoch": 1.025321710253217,
      "grad_norm": 2.2209174633026123,
      "learning_rate": 3.294589732945897e-05,
      "loss": 0.5997,
      "step": 2470
    },
    {
      "epoch": 1.0294728102947281,
      "grad_norm": 2.6523501873016357,
      "learning_rate": 3.287671232876712e-05,
      "loss": 0.4827,
      "step": 2480
    },
    {
      "epoch": 1.033623910336239,
      "grad_norm": 4.021456241607666,
      "learning_rate": 3.2807527328075276e-05,
      "loss": 0.4872,
      "step": 2490
    },
    {
      "epoch": 1.03777501037775,
      "grad_norm": 2.5249481201171875,
      "learning_rate": 3.2738342327383424e-05,
      "loss": 0.5165,
      "step": 2500
    },
    {
      "epoch": 1.0419261104192612,
      "grad_norm": 2.8554301261901855,
      "learning_rate": 3.266915732669157e-05,
      "loss": 0.4426,
      "step": 2510
    },
    {
      "epoch": 1.046077210460772,
      "grad_norm": 3.446474552154541,
      "learning_rate": 3.259997232599972e-05,
      "loss": 0.5261,
      "step": 2520
    },
    {
      "epoch": 1.0502283105022832,
      "grad_norm": 6.63249397277832,
      "learning_rate": 3.253078732530788e-05,
      "loss": 0.516,
      "step": 2530
    },
    {
      "epoch": 1.054379410543794,
      "grad_norm": 5.043332576751709,
      "learning_rate": 3.2461602324616026e-05,
      "loss": 0.5094,
      "step": 2540
    },
    {
      "epoch": 1.0585305105853051,
      "grad_norm": 4.553330898284912,
      "learning_rate": 3.2392417323924174e-05,
      "loss": 0.4113,
      "step": 2550
    },
    {
      "epoch": 1.0626816106268162,
      "grad_norm": 2.109650135040283,
      "learning_rate": 3.232323232323233e-05,
      "loss": 0.4867,
      "step": 2560
    },
    {
      "epoch": 1.066832710668327,
      "grad_norm": 2.617969274520874,
      "learning_rate": 3.225404732254048e-05,
      "loss": 0.4893,
      "step": 2570
    },
    {
      "epoch": 1.0709838107098382,
      "grad_norm": 3.71372652053833,
      "learning_rate": 3.218486232184863e-05,
      "loss": 0.5111,
      "step": 2580
    },
    {
      "epoch": 1.075134910751349,
      "grad_norm": 4.290191173553467,
      "learning_rate": 3.2115677321156776e-05,
      "loss": 0.499,
      "step": 2590
    },
    {
      "epoch": 1.0792860107928601,
      "grad_norm": 4.875636100769043,
      "learning_rate": 3.2046492320464924e-05,
      "loss": 0.6178,
      "step": 2600
    },
    {
      "epoch": 1.083437110834371,
      "grad_norm": 3.876492500305176,
      "learning_rate": 3.197730731977307e-05,
      "loss": 0.618,
      "step": 2610
    },
    {
      "epoch": 1.087588210875882,
      "grad_norm": 4.715938091278076,
      "learning_rate": 3.190812231908122e-05,
      "loss": 0.5847,
      "step": 2620
    },
    {
      "epoch": 1.0917393109173932,
      "grad_norm": 3.2122962474823,
      "learning_rate": 3.183893731838937e-05,
      "loss": 0.5768,
      "step": 2630
    },
    {
      "epoch": 1.095890410958904,
      "grad_norm": 3.346376657485962,
      "learning_rate": 3.1769752317697526e-05,
      "loss": 0.5175,
      "step": 2640
    },
    {
      "epoch": 1.1000415110004151,
      "grad_norm": 3.489844560623169,
      "learning_rate": 3.1700567317005674e-05,
      "loss": 0.4656,
      "step": 2650
    },
    {
      "epoch": 1.104192611041926,
      "grad_norm": 5.142818450927734,
      "learning_rate": 3.163138231631382e-05,
      "loss": 0.5063,
      "step": 2660
    },
    {
      "epoch": 1.108343711083437,
      "grad_norm": 5.815212249755859,
      "learning_rate": 3.156219731562197e-05,
      "loss": 0.5305,
      "step": 2670
    },
    {
      "epoch": 1.1124948111249482,
      "grad_norm": 2.125457525253296,
      "learning_rate": 3.149301231493013e-05,
      "loss": 0.4499,
      "step": 2680
    },
    {
      "epoch": 1.116645911166459,
      "grad_norm": 3.6747913360595703,
      "learning_rate": 3.1423827314238276e-05,
      "loss": 0.509,
      "step": 2690
    },
    {
      "epoch": 1.1207970112079702,
      "grad_norm": 2.598531723022461,
      "learning_rate": 3.1354642313546424e-05,
      "loss": 0.5454,
      "step": 2700
    },
    {
      "epoch": 1.124948111249481,
      "grad_norm": 3.8159067630767822,
      "learning_rate": 3.128545731285458e-05,
      "loss": 0.5135,
      "step": 2710
    },
    {
      "epoch": 1.129099211290992,
      "grad_norm": 3.280277729034424,
      "learning_rate": 3.121627231216273e-05,
      "loss": 0.4693,
      "step": 2720
    },
    {
      "epoch": 1.1332503113325032,
      "grad_norm": 2.37439227104187,
      "learning_rate": 3.114708731147088e-05,
      "loss": 0.49,
      "step": 2730
    },
    {
      "epoch": 1.137401411374014,
      "grad_norm": 8.941268920898438,
      "learning_rate": 3.107790231077902e-05,
      "loss": 0.5478,
      "step": 2740
    },
    {
      "epoch": 1.1415525114155252,
      "grad_norm": 3.172161102294922,
      "learning_rate": 3.1008717310087175e-05,
      "loss": 0.465,
      "step": 2750
    },
    {
      "epoch": 1.145703611457036,
      "grad_norm": 4.372394561767578,
      "learning_rate": 3.093953230939532e-05,
      "loss": 0.5254,
      "step": 2760
    },
    {
      "epoch": 1.1498547114985471,
      "grad_norm": 4.51657247543335,
      "learning_rate": 3.087034730870347e-05,
      "loss": 0.5225,
      "step": 2770
    },
    {
      "epoch": 1.154005811540058,
      "grad_norm": 3.657958745956421,
      "learning_rate": 3.080116230801162e-05,
      "loss": 0.4845,
      "step": 2780
    },
    {
      "epoch": 1.158156911581569,
      "grad_norm": 2.0525758266448975,
      "learning_rate": 3.0731977307319776e-05,
      "loss": 0.5328,
      "step": 2790
    },
    {
      "epoch": 1.1623080116230802,
      "grad_norm": 3.6827893257141113,
      "learning_rate": 3.0662792306627925e-05,
      "loss": 0.4176,
      "step": 2800
    },
    {
      "epoch": 1.166459111664591,
      "grad_norm": 2.6164309978485107,
      "learning_rate": 3.059360730593607e-05,
      "loss": 0.465,
      "step": 2810
    },
    {
      "epoch": 1.1706102117061021,
      "grad_norm": 2.8729286193847656,
      "learning_rate": 3.052442230524423e-05,
      "loss": 0.4254,
      "step": 2820
    },
    {
      "epoch": 1.1747613117476132,
      "grad_norm": 3.655730724334717,
      "learning_rate": 3.0455237304552374e-05,
      "loss": 0.5126,
      "step": 2830
    },
    {
      "epoch": 1.178912411789124,
      "grad_norm": 2.782867431640625,
      "learning_rate": 3.0386052303860523e-05,
      "loss": 0.5223,
      "step": 2840
    },
    {
      "epoch": 1.1830635118306352,
      "grad_norm": 3.3053581714630127,
      "learning_rate": 3.031686730316867e-05,
      "loss": 0.578,
      "step": 2850
    },
    {
      "epoch": 1.187214611872146,
      "grad_norm": 2.4032413959503174,
      "learning_rate": 3.0247682302476827e-05,
      "loss": 0.4353,
      "step": 2860
    },
    {
      "epoch": 1.1913657119136571,
      "grad_norm": 6.448305606842041,
      "learning_rate": 3.0178497301784975e-05,
      "loss": 0.5475,
      "step": 2870
    },
    {
      "epoch": 1.195516811955168,
      "grad_norm": 3.858548402786255,
      "learning_rate": 3.0109312301093124e-05,
      "loss": 0.5285,
      "step": 2880
    },
    {
      "epoch": 1.199667911996679,
      "grad_norm": 4.858962535858154,
      "learning_rate": 3.0040127300401273e-05,
      "loss": 0.4986,
      "step": 2890
    },
    {
      "epoch": 1.2038190120381902,
      "grad_norm": 2.360757350921631,
      "learning_rate": 2.9970942299709425e-05,
      "loss": 0.4883,
      "step": 2900
    },
    {
      "epoch": 1.207970112079701,
      "grad_norm": 3.652284622192383,
      "learning_rate": 2.9901757299017573e-05,
      "loss": 0.4959,
      "step": 2910
    },
    {
      "epoch": 1.2121212121212122,
      "grad_norm": 3.9303696155548096,
      "learning_rate": 2.9832572298325722e-05,
      "loss": 0.5115,
      "step": 2920
    },
    {
      "epoch": 1.2162723121627232,
      "grad_norm": 2.151797294616699,
      "learning_rate": 2.9763387297633877e-05,
      "loss": 0.4849,
      "step": 2930
    },
    {
      "epoch": 1.2204234122042341,
      "grad_norm": 3.5998244285583496,
      "learning_rate": 2.9694202296942026e-05,
      "loss": 0.4393,
      "step": 2940
    },
    {
      "epoch": 1.2245745122457452,
      "grad_norm": 3.647873878479004,
      "learning_rate": 2.9625017296250175e-05,
      "loss": 0.491,
      "step": 2950
    },
    {
      "epoch": 1.228725612287256,
      "grad_norm": 4.104638576507568,
      "learning_rate": 2.9555832295558323e-05,
      "loss": 0.5123,
      "step": 2960
    },
    {
      "epoch": 1.2328767123287672,
      "grad_norm": 3.8879618644714355,
      "learning_rate": 2.9486647294866475e-05,
      "loss": 0.5087,
      "step": 2970
    },
    {
      "epoch": 1.237027812370278,
      "grad_norm": 2.9498629570007324,
      "learning_rate": 2.9417462294174624e-05,
      "loss": 0.5211,
      "step": 2980
    },
    {
      "epoch": 1.2411789124117891,
      "grad_norm": 4.457188606262207,
      "learning_rate": 2.9348277293482773e-05,
      "loss": 0.4496,
      "step": 2990
    },
    {
      "epoch": 1.2453300124533002,
      "grad_norm": 3.371277332305908,
      "learning_rate": 2.927909229279092e-05,
      "loss": 0.5387,
      "step": 3000
    },
    {
      "epoch": 1.249481112494811,
      "grad_norm": 2.8865771293640137,
      "learning_rate": 2.9209907292099077e-05,
      "loss": 0.5465,
      "step": 3010
    },
    {
      "epoch": 1.2536322125363222,
      "grad_norm": 3.6447484493255615,
      "learning_rate": 2.9140722291407226e-05,
      "loss": 0.5061,
      "step": 3020
    },
    {
      "epoch": 1.257783312577833,
      "grad_norm": 4.657071113586426,
      "learning_rate": 2.9071537290715374e-05,
      "loss": 0.5667,
      "step": 3030
    },
    {
      "epoch": 1.2619344126193441,
      "grad_norm": 2.82145094871521,
      "learning_rate": 2.9002352290023526e-05,
      "loss": 0.5512,
      "step": 3040
    },
    {
      "epoch": 1.266085512660855,
      "grad_norm": 2.8189523220062256,
      "learning_rate": 2.8933167289331675e-05,
      "loss": 0.4674,
      "step": 3050
    },
    {
      "epoch": 1.270236612702366,
      "grad_norm": 2.9887049198150635,
      "learning_rate": 2.8863982288639824e-05,
      "loss": 0.4452,
      "step": 3060
    },
    {
      "epoch": 1.2743877127438772,
      "grad_norm": 6.290050983428955,
      "learning_rate": 2.8794797287947972e-05,
      "loss": 0.4939,
      "step": 3070
    },
    {
      "epoch": 1.278538812785388,
      "grad_norm": 2.6980485916137695,
      "learning_rate": 2.8725612287256128e-05,
      "loss": 0.5214,
      "step": 3080
    },
    {
      "epoch": 1.2826899128268991,
      "grad_norm": 3.136431932449341,
      "learning_rate": 2.8656427286564276e-05,
      "loss": 0.5495,
      "step": 3090
    },
    {
      "epoch": 1.2868410128684102,
      "grad_norm": 2.490834951400757,
      "learning_rate": 2.858724228587242e-05,
      "loss": 0.4893,
      "step": 3100
    },
    {
      "epoch": 1.290992112909921,
      "grad_norm": 4.337430000305176,
      "learning_rate": 2.851805728518057e-05,
      "loss": 0.5324,
      "step": 3110
    },
    {
      "epoch": 1.2951432129514322,
      "grad_norm": 3.1242997646331787,
      "learning_rate": 2.8448872284488726e-05,
      "loss": 0.4942,
      "step": 3120
    },
    {
      "epoch": 1.299294312992943,
      "grad_norm": 2.2274773120880127,
      "learning_rate": 2.8379687283796874e-05,
      "loss": 0.4384,
      "step": 3130
    },
    {
      "epoch": 1.3034454130344542,
      "grad_norm": 3.023829460144043,
      "learning_rate": 2.8310502283105023e-05,
      "loss": 0.4118,
      "step": 3140
    },
    {
      "epoch": 1.307596513075965,
      "grad_norm": 2.5447072982788086,
      "learning_rate": 2.8241317282413175e-05,
      "loss": 0.4482,
      "step": 3150
    },
    {
      "epoch": 1.3117476131174761,
      "grad_norm": 5.983236312866211,
      "learning_rate": 2.8172132281721324e-05,
      "loss": 0.5175,
      "step": 3160
    },
    {
      "epoch": 1.3158987131589872,
      "grad_norm": 6.880541801452637,
      "learning_rate": 2.8102947281029472e-05,
      "loss": 0.5724,
      "step": 3170
    },
    {
      "epoch": 1.320049813200498,
      "grad_norm": 2.925522804260254,
      "learning_rate": 2.803376228033762e-05,
      "loss": 0.4722,
      "step": 3180
    },
    {
      "epoch": 1.3242009132420092,
      "grad_norm": 6.545306205749512,
      "learning_rate": 2.7964577279645776e-05,
      "loss": 0.5242,
      "step": 3190
    },
    {
      "epoch": 1.3283520132835203,
      "grad_norm": 2.2871408462524414,
      "learning_rate": 2.7895392278953925e-05,
      "loss": 0.4411,
      "step": 3200
    },
    {
      "epoch": 1.3325031133250311,
      "grad_norm": 5.107059001922607,
      "learning_rate": 2.7826207278262074e-05,
      "loss": 0.5106,
      "step": 3210
    },
    {
      "epoch": 1.336654213366542,
      "grad_norm": 11.262338638305664,
      "learning_rate": 2.7757022277570222e-05,
      "loss": 0.3963,
      "step": 3220
    },
    {
      "epoch": 1.340805313408053,
      "grad_norm": 3.0895330905914307,
      "learning_rate": 2.7687837276878374e-05,
      "loss": 0.4716,
      "step": 3230
    },
    {
      "epoch": 1.3449564134495642,
      "grad_norm": 5.905062675476074,
      "learning_rate": 2.7618652276186523e-05,
      "loss": 0.5384,
      "step": 3240
    },
    {
      "epoch": 1.349107513491075,
      "grad_norm": 3.084061861038208,
      "learning_rate": 2.7549467275494672e-05,
      "loss": 0.4789,
      "step": 3250
    },
    {
      "epoch": 1.3532586135325861,
      "grad_norm": 3.5462963581085205,
      "learning_rate": 2.7480282274802827e-05,
      "loss": 0.5103,
      "step": 3260
    },
    {
      "epoch": 1.3574097135740972,
      "grad_norm": 4.329009532928467,
      "learning_rate": 2.7411097274110976e-05,
      "loss": 0.5753,
      "step": 3270
    },
    {
      "epoch": 1.361560813615608,
      "grad_norm": 5.1803717613220215,
      "learning_rate": 2.7341912273419124e-05,
      "loss": 0.4535,
      "step": 3280
    },
    {
      "epoch": 1.3657119136571192,
      "grad_norm": 3.8179423809051514,
      "learning_rate": 2.7272727272727273e-05,
      "loss": 0.49,
      "step": 3290
    },
    {
      "epoch": 1.36986301369863,
      "grad_norm": 3.47190523147583,
      "learning_rate": 2.7203542272035425e-05,
      "loss": 0.4914,
      "step": 3300
    },
    {
      "epoch": 1.3740141137401412,
      "grad_norm": 4.385995388031006,
      "learning_rate": 2.7134357271343574e-05,
      "loss": 0.4403,
      "step": 3310
    },
    {
      "epoch": 1.378165213781652,
      "grad_norm": 5.03425407409668,
      "learning_rate": 2.7065172270651722e-05,
      "loss": 0.4601,
      "step": 3320
    },
    {
      "epoch": 1.3823163138231631,
      "grad_norm": 4.051652908325195,
      "learning_rate": 2.699598726995987e-05,
      "loss": 0.5401,
      "step": 3330
    },
    {
      "epoch": 1.3864674138646742,
      "grad_norm": 3.8655834197998047,
      "learning_rate": 2.6926802269268027e-05,
      "loss": 0.4535,
      "step": 3340
    },
    {
      "epoch": 1.390618513906185,
      "grad_norm": 2.4460043907165527,
      "learning_rate": 2.6857617268576175e-05,
      "loss": 0.4916,
      "step": 3350
    },
    {
      "epoch": 1.3947696139476962,
      "grad_norm": 3.3797848224639893,
      "learning_rate": 2.6788432267884324e-05,
      "loss": 0.4906,
      "step": 3360
    },
    {
      "epoch": 1.3989207139892073,
      "grad_norm": 5.079919815063477,
      "learning_rate": 2.6719247267192476e-05,
      "loss": 0.484,
      "step": 3370
    },
    {
      "epoch": 1.4030718140307181,
      "grad_norm": 4.211511135101318,
      "learning_rate": 2.6650062266500625e-05,
      "loss": 0.4388,
      "step": 3380
    },
    {
      "epoch": 1.4072229140722292,
      "grad_norm": 4.602624893188477,
      "learning_rate": 2.6580877265808773e-05,
      "loss": 0.4765,
      "step": 3390
    },
    {
      "epoch": 1.41137401411374,
      "grad_norm": 4.662788391113281,
      "learning_rate": 2.6511692265116922e-05,
      "loss": 0.4623,
      "step": 3400
    },
    {
      "epoch": 1.4155251141552512,
      "grad_norm": 3.6016316413879395,
      "learning_rate": 2.6442507264425077e-05,
      "loss": 0.5004,
      "step": 3410
    },
    {
      "epoch": 1.419676214196762,
      "grad_norm": 2.7366676330566406,
      "learning_rate": 2.6373322263733226e-05,
      "loss": 0.4696,
      "step": 3420
    },
    {
      "epoch": 1.4238273142382731,
      "grad_norm": 5.770282745361328,
      "learning_rate": 2.6304137263041375e-05,
      "loss": 0.5219,
      "step": 3430
    },
    {
      "epoch": 1.4279784142797842,
      "grad_norm": 2.441094398498535,
      "learning_rate": 2.623495226234952e-05,
      "loss": 0.4402,
      "step": 3440
    },
    {
      "epoch": 1.432129514321295,
      "grad_norm": 2.657057285308838,
      "learning_rate": 2.6165767261657675e-05,
      "loss": 0.4641,
      "step": 3450
    },
    {
      "epoch": 1.4362806143628062,
      "grad_norm": 4.049190998077393,
      "learning_rate": 2.6096582260965824e-05,
      "loss": 0.4886,
      "step": 3460
    },
    {
      "epoch": 1.4404317144043173,
      "grad_norm": 2.494494915008545,
      "learning_rate": 2.6027397260273973e-05,
      "loss": 0.5744,
      "step": 3470
    },
    {
      "epoch": 1.4445828144458281,
      "grad_norm": 4.504703998565674,
      "learning_rate": 2.5958212259582128e-05,
      "loss": 0.4675,
      "step": 3480
    },
    {
      "epoch": 1.448733914487339,
      "grad_norm": 4.387242794036865,
      "learning_rate": 2.5889027258890273e-05,
      "loss": 0.5393,
      "step": 3490
    },
    {
      "epoch": 1.45288501452885,
      "grad_norm": 2.9786009788513184,
      "learning_rate": 2.5819842258198422e-05,
      "loss": 0.4241,
      "step": 3500
    },
    {
      "epoch": 1.4570361145703612,
      "grad_norm": 3.171816110610962,
      "learning_rate": 2.575065725750657e-05,
      "loss": 0.4881,
      "step": 3510
    },
    {
      "epoch": 1.461187214611872,
      "grad_norm": 4.072003364562988,
      "learning_rate": 2.5681472256814726e-05,
      "loss": 0.5372,
      "step": 3520
    },
    {
      "epoch": 1.4653383146533832,
      "grad_norm": 4.1055684089660645,
      "learning_rate": 2.5612287256122875e-05,
      "loss": 0.4388,
      "step": 3530
    },
    {
      "epoch": 1.4694894146948942,
      "grad_norm": 3.635331630706787,
      "learning_rate": 2.5543102255431023e-05,
      "loss": 0.5309,
      "step": 3540
    },
    {
      "epoch": 1.4736405147364051,
      "grad_norm": 5.258504867553711,
      "learning_rate": 2.5473917254739172e-05,
      "loss": 0.5544,
      "step": 3550
    },
    {
      "epoch": 1.4777916147779162,
      "grad_norm": 2.7480783462524414,
      "learning_rate": 2.5404732254047324e-05,
      "loss": 0.5588,
      "step": 3560
    },
    {
      "epoch": 1.481942714819427,
      "grad_norm": 3.8335678577423096,
      "learning_rate": 2.5335547253355473e-05,
      "loss": 0.5115,
      "step": 3570
    },
    {
      "epoch": 1.4860938148609382,
      "grad_norm": 5.351369380950928,
      "learning_rate": 2.526636225266362e-05,
      "loss": 0.5363,
      "step": 3580
    },
    {
      "epoch": 1.490244914902449,
      "grad_norm": 2.976675033569336,
      "learning_rate": 2.5197177251971777e-05,
      "loss": 0.4548,
      "step": 3590
    },
    {
      "epoch": 1.4943960149439601,
      "grad_norm": 2.504666566848755,
      "learning_rate": 2.5127992251279926e-05,
      "loss": 0.4361,
      "step": 3600
    },
    {
      "epoch": 1.4985471149854712,
      "grad_norm": 3.6445953845977783,
      "learning_rate": 2.5058807250588074e-05,
      "loss": 0.5417,
      "step": 3610
    },
    {
      "epoch": 1.502698215026982,
      "grad_norm": 3.2326903343200684,
      "learning_rate": 2.4989622249896226e-05,
      "loss": 0.5275,
      "step": 3620
    },
    {
      "epoch": 1.5068493150684932,
      "grad_norm": 2.323982000350952,
      "learning_rate": 2.492043724920437e-05,
      "loss": 0.4855,
      "step": 3630
    },
    {
      "epoch": 1.5110004151100043,
      "grad_norm": 3.308077573776245,
      "learning_rate": 2.4851252248512524e-05,
      "loss": 0.5015,
      "step": 3640
    },
    {
      "epoch": 1.5151515151515151,
      "grad_norm": 4.399362564086914,
      "learning_rate": 2.4782067247820672e-05,
      "loss": 0.4341,
      "step": 3650
    },
    {
      "epoch": 1.519302615193026,
      "grad_norm": 3.3171310424804688,
      "learning_rate": 2.4712882247128824e-05,
      "loss": 0.4147,
      "step": 3660
    },
    {
      "epoch": 1.523453715234537,
      "grad_norm": 3.6509106159210205,
      "learning_rate": 2.4643697246436973e-05,
      "loss": 0.485,
      "step": 3670
    },
    {
      "epoch": 1.5276048152760482,
      "grad_norm": 2.0368564128875732,
      "learning_rate": 2.4574512245745125e-05,
      "loss": 0.4992,
      "step": 3680
    },
    {
      "epoch": 1.531755915317559,
      "grad_norm": 7.529146671295166,
      "learning_rate": 2.4505327245053274e-05,
      "loss": 0.5345,
      "step": 3690
    },
    {
      "epoch": 1.5359070153590701,
      "grad_norm": 2.497404098510742,
      "learning_rate": 2.4436142244361422e-05,
      "loss": 0.4331,
      "step": 3700
    },
    {
      "epoch": 1.5400581154005812,
      "grad_norm": 4.1800432205200195,
      "learning_rate": 2.4366957243669574e-05,
      "loss": 0.4975,
      "step": 3710
    },
    {
      "epoch": 1.544209215442092,
      "grad_norm": 2.9349629878997803,
      "learning_rate": 2.4297772242977723e-05,
      "loss": 0.5045,
      "step": 3720
    },
    {
      "epoch": 1.5483603154836032,
      "grad_norm": 3.6447126865386963,
      "learning_rate": 2.4228587242285875e-05,
      "loss": 0.4663,
      "step": 3730
    },
    {
      "epoch": 1.5525114155251143,
      "grad_norm": 2.678473711013794,
      "learning_rate": 2.4159402241594024e-05,
      "loss": 0.5257,
      "step": 3740
    },
    {
      "epoch": 1.5566625155666252,
      "grad_norm": 2.372154712677002,
      "learning_rate": 2.4090217240902176e-05,
      "loss": 0.4602,
      "step": 3750
    },
    {
      "epoch": 1.560813615608136,
      "grad_norm": 2.5584843158721924,
      "learning_rate": 2.4021032240210324e-05,
      "loss": 0.4669,
      "step": 3760
    },
    {
      "epoch": 1.5649647156496471,
      "grad_norm": 2.9351401329040527,
      "learning_rate": 2.3951847239518473e-05,
      "loss": 0.5064,
      "step": 3770
    },
    {
      "epoch": 1.5691158156911582,
      "grad_norm": 2.7761528491973877,
      "learning_rate": 2.388266223882662e-05,
      "loss": 0.4317,
      "step": 3780
    },
    {
      "epoch": 1.573266915732669,
      "grad_norm": 3.70578932762146,
      "learning_rate": 2.3813477238134774e-05,
      "loss": 0.4548,
      "step": 3790
    },
    {
      "epoch": 1.5774180157741802,
      "grad_norm": 3.023688316345215,
      "learning_rate": 2.3744292237442922e-05,
      "loss": 0.4668,
      "step": 3800
    },
    {
      "epoch": 1.5815691158156913,
      "grad_norm": 3.631138801574707,
      "learning_rate": 2.3675107236751074e-05,
      "loss": 0.5686,
      "step": 3810
    },
    {
      "epoch": 1.5857202158572021,
      "grad_norm": 2.551762104034424,
      "learning_rate": 2.3605922236059223e-05,
      "loss": 0.4466,
      "step": 3820
    },
    {
      "epoch": 1.589871315898713,
      "grad_norm": 3.0315229892730713,
      "learning_rate": 2.3536737235367372e-05,
      "loss": 0.4764,
      "step": 3830
    },
    {
      "epoch": 1.5940224159402243,
      "grad_norm": 2.955089807510376,
      "learning_rate": 2.3467552234675524e-05,
      "loss": 0.5141,
      "step": 3840
    },
    {
      "epoch": 1.5981735159817352,
      "grad_norm": 2.38803768157959,
      "learning_rate": 2.3398367233983672e-05,
      "loss": 0.4553,
      "step": 3850
    },
    {
      "epoch": 1.602324616023246,
      "grad_norm": 2.01351261138916,
      "learning_rate": 2.3329182233291824e-05,
      "loss": 0.5,
      "step": 3860
    },
    {
      "epoch": 1.6064757160647571,
      "grad_norm": 2.6418347358703613,
      "learning_rate": 2.3259997232599973e-05,
      "loss": 0.4753,
      "step": 3870
    },
    {
      "epoch": 1.6106268161062682,
      "grad_norm": 4.747217178344727,
      "learning_rate": 2.3190812231908125e-05,
      "loss": 0.5402,
      "step": 3880
    },
    {
      "epoch": 1.614777916147779,
      "grad_norm": 3.0014986991882324,
      "learning_rate": 2.3121627231216274e-05,
      "loss": 0.477,
      "step": 3890
    },
    {
      "epoch": 1.6189290161892902,
      "grad_norm": 2.975444793701172,
      "learning_rate": 2.3052442230524422e-05,
      "loss": 0.4378,
      "step": 3900
    },
    {
      "epoch": 1.6230801162308013,
      "grad_norm": 3.7212331295013428,
      "learning_rate": 2.298325722983257e-05,
      "loss": 0.4294,
      "step": 3910
    },
    {
      "epoch": 1.6272312162723122,
      "grad_norm": 3.0202884674072266,
      "learning_rate": 2.2914072229140723e-05,
      "loss": 0.5074,
      "step": 3920
    },
    {
      "epoch": 1.631382316313823,
      "grad_norm": 3.3361990451812744,
      "learning_rate": 2.2844887228448875e-05,
      "loss": 0.4742,
      "step": 3930
    },
    {
      "epoch": 1.6355334163553341,
      "grad_norm": 3.974116802215576,
      "learning_rate": 2.2775702227757024e-05,
      "loss": 0.5264,
      "step": 3940
    },
    {
      "epoch": 1.6396845163968452,
      "grad_norm": 2.6266732215881348,
      "learning_rate": 2.2706517227065176e-05,
      "loss": 0.5348,
      "step": 3950
    },
    {
      "epoch": 1.643835616438356,
      "grad_norm": 2.8658087253570557,
      "learning_rate": 2.263733222637332e-05,
      "loss": 0.4524,
      "step": 3960
    },
    {
      "epoch": 1.6479867164798672,
      "grad_norm": 3.4651336669921875,
      "learning_rate": 2.2568147225681473e-05,
      "loss": 0.4244,
      "step": 3970
    },
    {
      "epoch": 1.6521378165213783,
      "grad_norm": 3.443150758743286,
      "learning_rate": 2.2498962224989622e-05,
      "loss": 0.5137,
      "step": 3980
    },
    {
      "epoch": 1.6562889165628891,
      "grad_norm": 1.7506388425827026,
      "learning_rate": 2.2429777224297774e-05,
      "loss": 0.4277,
      "step": 3990
    },
    {
      "epoch": 1.6604400166044002,
      "grad_norm": 4.870709419250488,
      "learning_rate": 2.2360592223605923e-05,
      "loss": 0.476,
      "step": 4000
    },
    {
      "epoch": 1.6645911166459113,
      "grad_norm": 2.2128655910491943,
      "learning_rate": 2.2291407222914075e-05,
      "loss": 0.4786,
      "step": 4010
    },
    {
      "epoch": 1.6687422166874222,
      "grad_norm": 2.4165539741516113,
      "learning_rate": 2.2222222222222223e-05,
      "loss": 0.4916,
      "step": 4020
    },
    {
      "epoch": 1.672893316728933,
      "grad_norm": 2.468628406524658,
      "learning_rate": 2.2153037221530372e-05,
      "loss": 0.5276,
      "step": 4030
    },
    {
      "epoch": 1.6770444167704441,
      "grad_norm": 5.909372806549072,
      "learning_rate": 2.2083852220838524e-05,
      "loss": 0.4658,
      "step": 4040
    },
    {
      "epoch": 1.6811955168119552,
      "grad_norm": 3.0192410945892334,
      "learning_rate": 2.2014667220146673e-05,
      "loss": 0.4778,
      "step": 4050
    },
    {
      "epoch": 1.685346616853466,
      "grad_norm": 2.403557300567627,
      "learning_rate": 2.1945482219454825e-05,
      "loss": 0.5164,
      "step": 4060
    },
    {
      "epoch": 1.6894977168949772,
      "grad_norm": 3.0352718830108643,
      "learning_rate": 2.1876297218762973e-05,
      "loss": 0.481,
      "step": 4070
    },
    {
      "epoch": 1.6936488169364883,
      "grad_norm": 4.090526580810547,
      "learning_rate": 2.1807112218071125e-05,
      "loss": 0.4728,
      "step": 4080
    },
    {
      "epoch": 1.6977999169779991,
      "grad_norm": 2.145803689956665,
      "learning_rate": 2.1737927217379274e-05,
      "loss": 0.4965,
      "step": 4090
    },
    {
      "epoch": 1.70195101701951,
      "grad_norm": 3.061441421508789,
      "learning_rate": 2.1668742216687423e-05,
      "loss": 0.5015,
      "step": 4100
    },
    {
      "epoch": 1.7061021170610213,
      "grad_norm": 3.752147912979126,
      "learning_rate": 2.159955721599557e-05,
      "loss": 0.5112,
      "step": 4110
    },
    {
      "epoch": 1.7102532171025322,
      "grad_norm": 3.3422789573669434,
      "learning_rate": 2.1530372215303723e-05,
      "loss": 0.4726,
      "step": 4120
    },
    {
      "epoch": 1.714404317144043,
      "grad_norm": 2.9641103744506836,
      "learning_rate": 2.1461187214611872e-05,
      "loss": 0.4681,
      "step": 4130
    },
    {
      "epoch": 1.7185554171855542,
      "grad_norm": 2.278608798980713,
      "learning_rate": 2.1392002213920024e-05,
      "loss": 0.4445,
      "step": 4140
    },
    {
      "epoch": 1.7227065172270652,
      "grad_norm": 3.4319615364074707,
      "learning_rate": 2.1322817213228176e-05,
      "loss": 0.41,
      "step": 4150
    },
    {
      "epoch": 1.7268576172685761,
      "grad_norm": 3.001133680343628,
      "learning_rate": 2.125363221253632e-05,
      "loss": 0.532,
      "step": 4160
    },
    {
      "epoch": 1.7310087173100872,
      "grad_norm": 3.2691376209259033,
      "learning_rate": 2.1184447211844473e-05,
      "loss": 0.4146,
      "step": 4170
    },
    {
      "epoch": 1.7351598173515983,
      "grad_norm": 2.0704023838043213,
      "learning_rate": 2.1115262211152622e-05,
      "loss": 0.4526,
      "step": 4180
    },
    {
      "epoch": 1.7393109173931092,
      "grad_norm": 2.0458595752716064,
      "learning_rate": 2.1046077210460774e-05,
      "loss": 0.4722,
      "step": 4190
    },
    {
      "epoch": 1.74346201743462,
      "grad_norm": 2.8366994857788086,
      "learning_rate": 2.0976892209768923e-05,
      "loss": 0.5019,
      "step": 4200
    },
    {
      "epoch": 1.7476131174761311,
      "grad_norm": 3.2579550743103027,
      "learning_rate": 2.0907707209077075e-05,
      "loss": 0.4725,
      "step": 4210
    },
    {
      "epoch": 1.7517642175176422,
      "grad_norm": 4.379652976989746,
      "learning_rate": 2.0838522208385224e-05,
      "loss": 0.5114,
      "step": 4220
    },
    {
      "epoch": 1.755915317559153,
      "grad_norm": 2.166032314300537,
      "learning_rate": 2.0769337207693372e-05,
      "loss": 0.4317,
      "step": 4230
    },
    {
      "epoch": 1.7600664176006642,
      "grad_norm": 2.945664644241333,
      "learning_rate": 2.070015220700152e-05,
      "loss": 0.5159,
      "step": 4240
    },
    {
      "epoch": 1.7642175176421753,
      "grad_norm": 3.420860767364502,
      "learning_rate": 2.0630967206309673e-05,
      "loss": 0.4949,
      "step": 4250
    },
    {
      "epoch": 1.7683686176836861,
      "grad_norm": 1.7721894979476929,
      "learning_rate": 2.0561782205617825e-05,
      "loss": 0.4559,
      "step": 4260
    },
    {
      "epoch": 1.7725197177251972,
      "grad_norm": 2.1961071491241455,
      "learning_rate": 2.0492597204925974e-05,
      "loss": 0.4477,
      "step": 4270
    },
    {
      "epoch": 1.7766708177667083,
      "grad_norm": 3.1930465698242188,
      "learning_rate": 2.0423412204234126e-05,
      "loss": 0.5124,
      "step": 4280
    },
    {
      "epoch": 1.7808219178082192,
      "grad_norm": 4.523341655731201,
      "learning_rate": 2.0354227203542274e-05,
      "loss": 0.432,
      "step": 4290
    },
    {
      "epoch": 1.78497301784973,
      "grad_norm": 3.2622928619384766,
      "learning_rate": 2.0285042202850423e-05,
      "loss": 0.4671,
      "step": 4300
    },
    {
      "epoch": 1.7891241178912412,
      "grad_norm": 4.29892110824585,
      "learning_rate": 2.021585720215857e-05,
      "loss": 0.4671,
      "step": 4310
    },
    {
      "epoch": 1.7932752179327522,
      "grad_norm": 4.290506362915039,
      "learning_rate": 2.0146672201466724e-05,
      "loss": 0.5046,
      "step": 4320
    },
    {
      "epoch": 1.797426317974263,
      "grad_norm": 3.5313756465911865,
      "learning_rate": 2.0077487200774872e-05,
      "loss": 0.453,
      "step": 4330
    },
    {
      "epoch": 1.8015774180157742,
      "grad_norm": 2.703151226043701,
      "learning_rate": 2.0008302200083024e-05,
      "loss": 0.4884,
      "step": 4340
    },
    {
      "epoch": 1.8057285180572853,
      "grad_norm": 2.2956504821777344,
      "learning_rate": 1.9939117199391173e-05,
      "loss": 0.4935,
      "step": 4350
    },
    {
      "epoch": 1.8098796180987962,
      "grad_norm": 3.2790794372558594,
      "learning_rate": 1.986993219869932e-05,
      "loss": 0.4558,
      "step": 4360
    },
    {
      "epoch": 1.814030718140307,
      "grad_norm": 2.8048112392425537,
      "learning_rate": 1.980074719800747e-05,
      "loss": 0.4206,
      "step": 4370
    },
    {
      "epoch": 1.8181818181818183,
      "grad_norm": 4.562915325164795,
      "learning_rate": 1.9731562197315622e-05,
      "loss": 0.4967,
      "step": 4380
    },
    {
      "epoch": 1.8223329182233292,
      "grad_norm": 2.1316170692443848,
      "learning_rate": 1.9662377196623774e-05,
      "loss": 0.3977,
      "step": 4390
    },
    {
      "epoch": 1.82648401826484,
      "grad_norm": 4.293644428253174,
      "learning_rate": 1.9593192195931923e-05,
      "loss": 0.5131,
      "step": 4400
    },
    {
      "epoch": 1.8306351183063512,
      "grad_norm": 4.13437557220459,
      "learning_rate": 1.9524007195240075e-05,
      "loss": 0.5144,
      "step": 4410
    },
    {
      "epoch": 1.8347862183478623,
      "grad_norm": 4.4387335777282715,
      "learning_rate": 1.9454822194548224e-05,
      "loss": 0.4702,
      "step": 4420
    },
    {
      "epoch": 1.8389373183893731,
      "grad_norm": 3.928175926208496,
      "learning_rate": 1.9385637193856372e-05,
      "loss": 0.4851,
      "step": 4430
    },
    {
      "epoch": 1.8430884184308842,
      "grad_norm": 4.335146903991699,
      "learning_rate": 1.931645219316452e-05,
      "loss": 0.4327,
      "step": 4440
    },
    {
      "epoch": 1.8472395184723953,
      "grad_norm": 3.875185489654541,
      "learning_rate": 1.9247267192472673e-05,
      "loss": 0.4896,
      "step": 4450
    },
    {
      "epoch": 1.8513906185139062,
      "grad_norm": 3.57842755317688,
      "learning_rate": 1.9178082191780822e-05,
      "loss": 0.4265,
      "step": 4460
    },
    {
      "epoch": 1.855541718555417,
      "grad_norm": 2.508277654647827,
      "learning_rate": 1.9108897191088974e-05,
      "loss": 0.4419,
      "step": 4470
    },
    {
      "epoch": 1.8596928185969281,
      "grad_norm": 1.753085732460022,
      "learning_rate": 1.9039712190397122e-05,
      "loss": 0.4545,
      "step": 4480
    },
    {
      "epoch": 1.8638439186384392,
      "grad_norm": 4.329154968261719,
      "learning_rate": 1.897052718970527e-05,
      "loss": 0.4404,
      "step": 4490
    },
    {
      "epoch": 1.86799501867995,
      "grad_norm": 2.7261669635772705,
      "learning_rate": 1.8901342189013423e-05,
      "loss": 0.4635,
      "step": 4500
    },
    {
      "epoch": 1.8721461187214612,
      "grad_norm": 2.192056655883789,
      "learning_rate": 1.8832157188321572e-05,
      "loss": 0.4237,
      "step": 4510
    },
    {
      "epoch": 1.8762972187629723,
      "grad_norm": 3.366236925125122,
      "learning_rate": 1.8762972187629724e-05,
      "loss": 0.4747,
      "step": 4520
    },
    {
      "epoch": 1.8804483188044832,
      "grad_norm": 2.831012487411499,
      "learning_rate": 1.8693787186937873e-05,
      "loss": 0.4122,
      "step": 4530
    },
    {
      "epoch": 1.884599418845994,
      "grad_norm": 3.7511518001556396,
      "learning_rate": 1.8624602186246025e-05,
      "loss": 0.5064,
      "step": 4540
    },
    {
      "epoch": 1.8887505188875053,
      "grad_norm": 3.593442916870117,
      "learning_rate": 1.8555417185554173e-05,
      "loss": 0.4349,
      "step": 4550
    },
    {
      "epoch": 1.8929016189290162,
      "grad_norm": 2.2117984294891357,
      "learning_rate": 1.8486232184862322e-05,
      "loss": 0.3843,
      "step": 4560
    },
    {
      "epoch": 1.897052718970527,
      "grad_norm": 3.3972513675689697,
      "learning_rate": 1.841704718417047e-05,
      "loss": 0.461,
      "step": 4570
    },
    {
      "epoch": 1.9012038190120382,
      "grad_norm": 2.971210241317749,
      "learning_rate": 1.8347862183478623e-05,
      "loss": 0.3656,
      "step": 4580
    },
    {
      "epoch": 1.9053549190535493,
      "grad_norm": 5.071174144744873,
      "learning_rate": 1.827867718278677e-05,
      "loss": 0.5005,
      "step": 4590
    },
    {
      "epoch": 1.9095060190950601,
      "grad_norm": 2.9394466876983643,
      "learning_rate": 1.8209492182094923e-05,
      "loss": 0.4645,
      "step": 4600
    },
    {
      "epoch": 1.9136571191365712,
      "grad_norm": 3.9474709033966064,
      "learning_rate": 1.8140307181403075e-05,
      "loss": 0.4691,
      "step": 4610
    },
    {
      "epoch": 1.9178082191780823,
      "grad_norm": 2.3412551879882812,
      "learning_rate": 1.8071122180711224e-05,
      "loss": 0.4283,
      "step": 4620
    },
    {
      "epoch": 1.9219593192195932,
      "grad_norm": 5.521317005157471,
      "learning_rate": 1.8001937180019373e-05,
      "loss": 0.4075,
      "step": 4630
    },
    {
      "epoch": 1.926110419261104,
      "grad_norm": 3.3319509029388428,
      "learning_rate": 1.793275217932752e-05,
      "loss": 0.4199,
      "step": 4640
    },
    {
      "epoch": 1.9302615193026154,
      "grad_norm": 3.251572847366333,
      "learning_rate": 1.7863567178635673e-05,
      "loss": 0.4359,
      "step": 4650
    },
    {
      "epoch": 1.9344126193441262,
      "grad_norm": 3.2913904190063477,
      "learning_rate": 1.7794382177943822e-05,
      "loss": 0.48,
      "step": 4660
    },
    {
      "epoch": 1.938563719385637,
      "grad_norm": 2.418109655380249,
      "learning_rate": 1.7725197177251974e-05,
      "loss": 0.4255,
      "step": 4670
    },
    {
      "epoch": 1.9427148194271482,
      "grad_norm": 3.5363433361053467,
      "learning_rate": 1.7656012176560123e-05,
      "loss": 0.4675,
      "step": 4680
    },
    {
      "epoch": 1.9468659194686593,
      "grad_norm": 4.129697799682617,
      "learning_rate": 1.758682717586827e-05,
      "loss": 0.466,
      "step": 4690
    },
    {
      "epoch": 1.9510170195101701,
      "grad_norm": 5.1724958419799805,
      "learning_rate": 1.751764217517642e-05,
      "loss": 0.4215,
      "step": 4700
    },
    {
      "epoch": 1.9551681195516812,
      "grad_norm": 2.6208887100219727,
      "learning_rate": 1.7448457174484572e-05,
      "loss": 0.4136,
      "step": 4710
    },
    {
      "epoch": 1.9593192195931923,
      "grad_norm": 3.7060048580169678,
      "learning_rate": 1.7379272173792724e-05,
      "loss": 0.4844,
      "step": 4720
    },
    {
      "epoch": 1.9634703196347032,
      "grad_norm": 3.005657196044922,
      "learning_rate": 1.7310087173100873e-05,
      "loss": 0.4955,
      "step": 4730
    },
    {
      "epoch": 1.967621419676214,
      "grad_norm": 3.3614261150360107,
      "learning_rate": 1.7240902172409025e-05,
      "loss": 0.4373,
      "step": 4740
    },
    {
      "epoch": 1.9717725197177252,
      "grad_norm": 2.143282651901245,
      "learning_rate": 1.7171717171717173e-05,
      "loss": 0.4561,
      "step": 4750
    },
    {
      "epoch": 1.9759236197592362,
      "grad_norm": 3.9337239265441895,
      "learning_rate": 1.7102532171025322e-05,
      "loss": 0.453,
      "step": 4760
    },
    {
      "epoch": 1.9800747198007471,
      "grad_norm": 4.027608394622803,
      "learning_rate": 1.703334717033347e-05,
      "loss": 0.464,
      "step": 4770
    },
    {
      "epoch": 1.9842258198422582,
      "grad_norm": 3.4445927143096924,
      "learning_rate": 1.6964162169641623e-05,
      "loss": 0.4801,
      "step": 4780
    },
    {
      "epoch": 1.9883769198837693,
      "grad_norm": 4.445283889770508,
      "learning_rate": 1.689497716894977e-05,
      "loss": 0.4626,
      "step": 4790
    },
    {
      "epoch": 1.9925280199252802,
      "grad_norm": 4.049284934997559,
      "learning_rate": 1.6825792168257924e-05,
      "loss": 0.4423,
      "step": 4800
    },
    {
      "epoch": 1.996679119966791,
      "grad_norm": 5.037245273590088,
      "learning_rate": 1.6756607167566072e-05,
      "loss": 0.4784,
      "step": 4810
    },
    {
      "epoch": 2.0008302200083024,
      "grad_norm": 2.9443376064300537,
      "learning_rate": 1.6687422166874224e-05,
      "loss": 0.4098,
      "step": 4820
    },
    {
      "epoch": 2.004981320049813,
      "grad_norm": 2.3369195461273193,
      "learning_rate": 1.6618237166182373e-05,
      "loss": 0.3633,
      "step": 4830
    },
    {
      "epoch": 2.009132420091324,
      "grad_norm": 4.057060718536377,
      "learning_rate": 1.654905216549052e-05,
      "loss": 0.4692,
      "step": 4840
    },
    {
      "epoch": 2.0132835201328354,
      "grad_norm": 3.3897745609283447,
      "learning_rate": 1.6479867164798674e-05,
      "loss": 0.5191,
      "step": 4850
    },
    {
      "epoch": 2.0174346201743463,
      "grad_norm": 3.2372398376464844,
      "learning_rate": 1.6410682164106822e-05,
      "loss": 0.4679,
      "step": 4860
    },
    {
      "epoch": 2.021585720215857,
      "grad_norm": 3.103949785232544,
      "learning_rate": 1.6341497163414974e-05,
      "loss": 0.4925,
      "step": 4870
    },
    {
      "epoch": 2.025736820257368,
      "grad_norm": 3.9291675090789795,
      "learning_rate": 1.6272312162723123e-05,
      "loss": 0.4665,
      "step": 4880
    },
    {
      "epoch": 2.0298879202988793,
      "grad_norm": 2.899688482284546,
      "learning_rate": 1.620312716203127e-05,
      "loss": 0.4345,
      "step": 4890
    },
    {
      "epoch": 2.03403902034039,
      "grad_norm": 3.2933595180511475,
      "learning_rate": 1.613394216133942e-05,
      "loss": 0.4487,
      "step": 4900
    },
    {
      "epoch": 2.038190120381901,
      "grad_norm": 2.6849043369293213,
      "learning_rate": 1.6064757160647572e-05,
      "loss": 0.4062,
      "step": 4910
    },
    {
      "epoch": 2.0423412204234124,
      "grad_norm": 3.189920425415039,
      "learning_rate": 1.599557215995572e-05,
      "loss": 0.4302,
      "step": 4920
    },
    {
      "epoch": 2.0464923204649232,
      "grad_norm": 2.707552433013916,
      "learning_rate": 1.5926387159263873e-05,
      "loss": 0.4791,
      "step": 4930
    },
    {
      "epoch": 2.050643420506434,
      "grad_norm": 3.616179943084717,
      "learning_rate": 1.585720215857202e-05,
      "loss": 0.5283,
      "step": 4940
    },
    {
      "epoch": 2.0547945205479454,
      "grad_norm": 2.7592642307281494,
      "learning_rate": 1.5788017157880174e-05,
      "loss": 0.411,
      "step": 4950
    },
    {
      "epoch": 2.0589456205894563,
      "grad_norm": 2.2957160472869873,
      "learning_rate": 1.5718832157188322e-05,
      "loss": 0.4491,
      "step": 4960
    },
    {
      "epoch": 2.063096720630967,
      "grad_norm": 5.709911823272705,
      "learning_rate": 1.564964715649647e-05,
      "loss": 0.4731,
      "step": 4970
    },
    {
      "epoch": 2.067247820672478,
      "grad_norm": 4.535197734832764,
      "learning_rate": 1.5580462155804623e-05,
      "loss": 0.4623,
      "step": 4980
    },
    {
      "epoch": 2.0713989207139893,
      "grad_norm": 3.4346923828125,
      "learning_rate": 1.5511277155112772e-05,
      "loss": 0.418,
      "step": 4990
    },
    {
      "epoch": 2.0755500207555,
      "grad_norm": 3.0463788509368896,
      "learning_rate": 1.5442092154420924e-05,
      "loss": 0.4024,
      "step": 5000
    },
    {
      "epoch": 2.079701120797011,
      "grad_norm": 2.9879138469696045,
      "learning_rate": 1.5372907153729072e-05,
      "loss": 0.5009,
      "step": 5010
    },
    {
      "epoch": 2.0838522208385224,
      "grad_norm": 5.1002607345581055,
      "learning_rate": 1.530372215303722e-05,
      "loss": 0.4676,
      "step": 5020
    },
    {
      "epoch": 2.0880033208800333,
      "grad_norm": 3.2224366664886475,
      "learning_rate": 1.5234537152345371e-05,
      "loss": 0.4997,
      "step": 5030
    },
    {
      "epoch": 2.092154420921544,
      "grad_norm": 4.794107437133789,
      "learning_rate": 1.5165352151653522e-05,
      "loss": 0.4754,
      "step": 5040
    },
    {
      "epoch": 2.096305520963055,
      "grad_norm": 3.2707958221435547,
      "learning_rate": 1.509616715096167e-05,
      "loss": 0.4391,
      "step": 5050
    },
    {
      "epoch": 2.1004566210045663,
      "grad_norm": 2.136232852935791,
      "learning_rate": 1.5026982150269822e-05,
      "loss": 0.4057,
      "step": 5060
    },
    {
      "epoch": 2.104607721046077,
      "grad_norm": 2.8630294799804688,
      "learning_rate": 1.4957797149577973e-05,
      "loss": 0.4667,
      "step": 5070
    },
    {
      "epoch": 2.108758821087588,
      "grad_norm": 4.684547424316406,
      "learning_rate": 1.4888612148886121e-05,
      "loss": 0.4478,
      "step": 5080
    },
    {
      "epoch": 2.1129099211290994,
      "grad_norm": 2.4985640048980713,
      "learning_rate": 1.4819427148194274e-05,
      "loss": 0.4211,
      "step": 5090
    },
    {
      "epoch": 2.1170610211706102,
      "grad_norm": 3.4977755546569824,
      "learning_rate": 1.4750242147502422e-05,
      "loss": 0.5012,
      "step": 5100
    },
    {
      "epoch": 2.121212121212121,
      "grad_norm": 3.2890844345092773,
      "learning_rate": 1.4681057146810573e-05,
      "loss": 0.4252,
      "step": 5110
    },
    {
      "epoch": 2.1253632212536324,
      "grad_norm": 3.9586901664733887,
      "learning_rate": 1.4611872146118721e-05,
      "loss": 0.4046,
      "step": 5120
    },
    {
      "epoch": 2.1295143212951433,
      "grad_norm": 2.485661506652832,
      "learning_rate": 1.4542687145426873e-05,
      "loss": 0.4691,
      "step": 5130
    },
    {
      "epoch": 2.133665421336654,
      "grad_norm": 3.3622615337371826,
      "learning_rate": 1.4473502144735022e-05,
      "loss": 0.4884,
      "step": 5140
    },
    {
      "epoch": 2.137816521378165,
      "grad_norm": 2.984328031539917,
      "learning_rate": 1.4404317144043172e-05,
      "loss": 0.4081,
      "step": 5150
    },
    {
      "epoch": 2.1419676214196763,
      "grad_norm": 4.390371799468994,
      "learning_rate": 1.4335132143351321e-05,
      "loss": 0.4477,
      "step": 5160
    },
    {
      "epoch": 2.146118721461187,
      "grad_norm": 2.920158624649048,
      "learning_rate": 1.4265947142659471e-05,
      "loss": 0.4608,
      "step": 5170
    },
    {
      "epoch": 2.150269821502698,
      "grad_norm": 2.0966081619262695,
      "learning_rate": 1.4196762141967623e-05,
      "loss": 0.461,
      "step": 5180
    },
    {
      "epoch": 2.1544209215442094,
      "grad_norm": 2.427083969116211,
      "learning_rate": 1.4127577141275772e-05,
      "loss": 0.3548,
      "step": 5190
    },
    {
      "epoch": 2.1585720215857203,
      "grad_norm": 2.6995766162872314,
      "learning_rate": 1.4058392140583922e-05,
      "loss": 0.4511,
      "step": 5200
    },
    {
      "epoch": 2.162723121627231,
      "grad_norm": 2.4964699745178223,
      "learning_rate": 1.3989207139892071e-05,
      "loss": 0.4378,
      "step": 5210
    },
    {
      "epoch": 2.166874221668742,
      "grad_norm": 2.4725136756896973,
      "learning_rate": 1.3920022139200223e-05,
      "loss": 0.4829,
      "step": 5220
    },
    {
      "epoch": 2.1710253217102533,
      "grad_norm": 3.4686760902404785,
      "learning_rate": 1.3850837138508372e-05,
      "loss": 0.5545,
      "step": 5230
    },
    {
      "epoch": 2.175176421751764,
      "grad_norm": 2.6969380378723145,
      "learning_rate": 1.3781652137816522e-05,
      "loss": 0.4619,
      "step": 5240
    },
    {
      "epoch": 2.179327521793275,
      "grad_norm": 2.649153709411621,
      "learning_rate": 1.371246713712467e-05,
      "loss": 0.469,
      "step": 5250
    },
    {
      "epoch": 2.1834786218347864,
      "grad_norm": 3.0994322299957275,
      "learning_rate": 1.3643282136432823e-05,
      "loss": 0.4095,
      "step": 5260
    },
    {
      "epoch": 2.1876297218762972,
      "grad_norm": 1.626128077507019,
      "learning_rate": 1.3574097135740971e-05,
      "loss": 0.5378,
      "step": 5270
    },
    {
      "epoch": 2.191780821917808,
      "grad_norm": 3.29648494720459,
      "learning_rate": 1.3504912135049122e-05,
      "loss": 0.45,
      "step": 5280
    },
    {
      "epoch": 2.1959319219593194,
      "grad_norm": 3.851625919342041,
      "learning_rate": 1.3435727134357274e-05,
      "loss": 0.4402,
      "step": 5290
    },
    {
      "epoch": 2.2000830220008303,
      "grad_norm": 2.866898775100708,
      "learning_rate": 1.3366542133665422e-05,
      "loss": 0.4551,
      "step": 5300
    },
    {
      "epoch": 2.204234122042341,
      "grad_norm": 2.8651320934295654,
      "learning_rate": 1.3297357132973573e-05,
      "loss": 0.4474,
      "step": 5310
    },
    {
      "epoch": 2.208385222083852,
      "grad_norm": 2.146641492843628,
      "learning_rate": 1.3228172132281721e-05,
      "loss": 0.3531,
      "step": 5320
    },
    {
      "epoch": 2.2125363221253633,
      "grad_norm": 4.262712478637695,
      "learning_rate": 1.3158987131589873e-05,
      "loss": 0.4326,
      "step": 5330
    },
    {
      "epoch": 2.216687422166874,
      "grad_norm": 2.257841110229492,
      "learning_rate": 1.308980213089802e-05,
      "loss": 0.4876,
      "step": 5340
    },
    {
      "epoch": 2.220838522208385,
      "grad_norm": 5.933916091918945,
      "learning_rate": 1.3020617130206172e-05,
      "loss": 0.4164,
      "step": 5350
    },
    {
      "epoch": 2.2249896222498964,
      "grad_norm": 2.280763626098633,
      "learning_rate": 1.2951432129514321e-05,
      "loss": 0.3872,
      "step": 5360
    },
    {
      "epoch": 2.2291407222914073,
      "grad_norm": 4.468048572540283,
      "learning_rate": 1.2882247128822471e-05,
      "loss": 0.4608,
      "step": 5370
    },
    {
      "epoch": 2.233291822332918,
      "grad_norm": 2.6709768772125244,
      "learning_rate": 1.281306212813062e-05,
      "loss": 0.4413,
      "step": 5380
    },
    {
      "epoch": 2.237442922374429,
      "grad_norm": 2.918421745300293,
      "learning_rate": 1.2743877127438772e-05,
      "loss": 0.4603,
      "step": 5390
    },
    {
      "epoch": 2.2415940224159403,
      "grad_norm": 4.782987117767334,
      "learning_rate": 1.2674692126746923e-05,
      "loss": 0.4027,
      "step": 5400
    },
    {
      "epoch": 2.245745122457451,
      "grad_norm": 3.4787838459014893,
      "learning_rate": 1.2605507126055071e-05,
      "loss": 0.4447,
      "step": 5410
    },
    {
      "epoch": 2.249896222498962,
      "grad_norm": 6.949077606201172,
      "learning_rate": 1.2536322125363223e-05,
      "loss": 0.4717,
      "step": 5420
    },
    {
      "epoch": 2.2540473225404734,
      "grad_norm": 3.1603660583496094,
      "learning_rate": 1.2467137124671372e-05,
      "loss": 0.4001,
      "step": 5430
    },
    {
      "epoch": 2.258198422581984,
      "grad_norm": 2.7676761150360107,
      "learning_rate": 1.239795212397952e-05,
      "loss": 0.4075,
      "step": 5440
    },
    {
      "epoch": 2.262349522623495,
      "grad_norm": 4.183143138885498,
      "learning_rate": 1.2328767123287671e-05,
      "loss": 0.5156,
      "step": 5450
    },
    {
      "epoch": 2.2665006226650064,
      "grad_norm": 2.637239694595337,
      "learning_rate": 1.2259582122595823e-05,
      "loss": 0.4793,
      "step": 5460
    },
    {
      "epoch": 2.2706517227065173,
      "grad_norm": 2.6023313999176025,
      "learning_rate": 1.2190397121903972e-05,
      "loss": 0.4403,
      "step": 5470
    },
    {
      "epoch": 2.274802822748028,
      "grad_norm": 3.300352096557617,
      "learning_rate": 1.2121212121212122e-05,
      "loss": 0.3887,
      "step": 5480
    },
    {
      "epoch": 2.2789539227895395,
      "grad_norm": 2.8680922985076904,
      "learning_rate": 1.2052027120520272e-05,
      "loss": 0.4713,
      "step": 5490
    },
    {
      "epoch": 2.2831050228310503,
      "grad_norm": 6.143626689910889,
      "learning_rate": 1.1982842119828423e-05,
      "loss": 0.4616,
      "step": 5500
    },
    {
      "epoch": 2.287256122872561,
      "grad_norm": 3.7579472064971924,
      "learning_rate": 1.1913657119136571e-05,
      "loss": 0.4622,
      "step": 5510
    },
    {
      "epoch": 2.291407222914072,
      "grad_norm": 2.653409481048584,
      "learning_rate": 1.1844472118444722e-05,
      "loss": 0.4716,
      "step": 5520
    },
    {
      "epoch": 2.2955583229555834,
      "grad_norm": 3.4117043018341064,
      "learning_rate": 1.1775287117752872e-05,
      "loss": 0.4601,
      "step": 5530
    },
    {
      "epoch": 2.2997094229970942,
      "grad_norm": 3.981003761291504,
      "learning_rate": 1.170610211706102e-05,
      "loss": 0.4903,
      "step": 5540
    },
    {
      "epoch": 2.303860523038605,
      "grad_norm": 5.674710750579834,
      "learning_rate": 1.1636917116369171e-05,
      "loss": 0.484,
      "step": 5550
    },
    {
      "epoch": 2.308011623080116,
      "grad_norm": 2.580678701400757,
      "learning_rate": 1.1567732115677321e-05,
      "loss": 0.489,
      "step": 5560
    },
    {
      "epoch": 2.3121627231216273,
      "grad_norm": 8.793874740600586,
      "learning_rate": 1.1498547114985472e-05,
      "loss": 0.4876,
      "step": 5570
    },
    {
      "epoch": 2.316313823163138,
      "grad_norm": 3.541922092437744,
      "learning_rate": 1.1429362114293622e-05,
      "loss": 0.4679,
      "step": 5580
    },
    {
      "epoch": 2.320464923204649,
      "grad_norm": 2.8863272666931152,
      "learning_rate": 1.1360177113601772e-05,
      "loss": 0.4831,
      "step": 5590
    },
    {
      "epoch": 2.3246160232461603,
      "grad_norm": 2.7638864517211914,
      "learning_rate": 1.1290992112909923e-05,
      "loss": 0.5316,
      "step": 5600
    },
    {
      "epoch": 2.328767123287671,
      "grad_norm": 2.3765323162078857,
      "learning_rate": 1.1221807112218071e-05,
      "loss": 0.4721,
      "step": 5610
    },
    {
      "epoch": 2.332918223329182,
      "grad_norm": 4.051947593688965,
      "learning_rate": 1.1152622111526222e-05,
      "loss": 0.4686,
      "step": 5620
    },
    {
      "epoch": 2.3370693233706934,
      "grad_norm": 2.9044101238250732,
      "learning_rate": 1.1083437110834372e-05,
      "loss": 0.4686,
      "step": 5630
    },
    {
      "epoch": 2.3412204234122043,
      "grad_norm": 2.906606435775757,
      "learning_rate": 1.101425211014252e-05,
      "loss": 0.481,
      "step": 5640
    },
    {
      "epoch": 2.345371523453715,
      "grad_norm": 2.02329158782959,
      "learning_rate": 1.0945067109450671e-05,
      "loss": 0.4334,
      "step": 5650
    },
    {
      "epoch": 2.3495226234952264,
      "grad_norm": 2.591993808746338,
      "learning_rate": 1.0875882108758821e-05,
      "loss": 0.4598,
      "step": 5660
    },
    {
      "epoch": 2.3536737235367373,
      "grad_norm": 3.680168390274048,
      "learning_rate": 1.0806697108066972e-05,
      "loss": 0.4691,
      "step": 5670
    },
    {
      "epoch": 2.357824823578248,
      "grad_norm": 2.3479156494140625,
      "learning_rate": 1.0737512107375122e-05,
      "loss": 0.5109,
      "step": 5680
    },
    {
      "epoch": 2.361975923619759,
      "grad_norm": 6.593276023864746,
      "learning_rate": 1.0668327106683273e-05,
      "loss": 0.5169,
      "step": 5690
    },
    {
      "epoch": 2.3661270236612704,
      "grad_norm": 3.4656004905700684,
      "learning_rate": 1.0599142105991421e-05,
      "loss": 0.4224,
      "step": 5700
    },
    {
      "epoch": 2.3702781237027812,
      "grad_norm": 5.293570518493652,
      "learning_rate": 1.0529957105299572e-05,
      "loss": 0.455,
      "step": 5710
    },
    {
      "epoch": 2.374429223744292,
      "grad_norm": 5.360357761383057,
      "learning_rate": 1.0460772104607722e-05,
      "loss": 0.5007,
      "step": 5720
    },
    {
      "epoch": 2.3785803237858034,
      "grad_norm": 2.5879924297332764,
      "learning_rate": 1.0391587103915872e-05,
      "loss": 0.4338,
      "step": 5730
    },
    {
      "epoch": 2.3827314238273143,
      "grad_norm": 2.3003549575805664,
      "learning_rate": 1.0322402103224021e-05,
      "loss": 0.5251,
      "step": 5740
    },
    {
      "epoch": 2.386882523868825,
      "grad_norm": 3.6486411094665527,
      "learning_rate": 1.0253217102532171e-05,
      "loss": 0.5209,
      "step": 5750
    },
    {
      "epoch": 2.391033623910336,
      "grad_norm": 2.771336555480957,
      "learning_rate": 1.0184032101840322e-05,
      "loss": 0.3798,
      "step": 5760
    },
    {
      "epoch": 2.3951847239518473,
      "grad_norm": 2.0580625534057617,
      "learning_rate": 1.011484710114847e-05,
      "loss": 0.4501,
      "step": 5770
    },
    {
      "epoch": 2.399335823993358,
      "grad_norm": 4.4126996994018555,
      "learning_rate": 1.004566210045662e-05,
      "loss": 0.4916,
      "step": 5780
    },
    {
      "epoch": 2.403486924034869,
      "grad_norm": 2.6664481163024902,
      "learning_rate": 9.976477099764771e-06,
      "loss": 0.4363,
      "step": 5790
    },
    {
      "epoch": 2.4076380240763804,
      "grad_norm": 2.730625629425049,
      "learning_rate": 9.907292099072921e-06,
      "loss": 0.3868,
      "step": 5800
    },
    {
      "epoch": 2.4117891241178913,
      "grad_norm": 3.3219361305236816,
      "learning_rate": 9.838107098381072e-06,
      "loss": 0.4653,
      "step": 5810
    },
    {
      "epoch": 2.415940224159402,
      "grad_norm": 2.793632745742798,
      "learning_rate": 9.768922097689222e-06,
      "loss": 0.4597,
      "step": 5820
    },
    {
      "epoch": 2.4200913242009134,
      "grad_norm": 3.0918211936950684,
      "learning_rate": 9.699737096997372e-06,
      "loss": 0.4961,
      "step": 5830
    },
    {
      "epoch": 2.4242424242424243,
      "grad_norm": 3.1937458515167236,
      "learning_rate": 9.630552096305521e-06,
      "loss": 0.4168,
      "step": 5840
    },
    {
      "epoch": 2.428393524283935,
      "grad_norm": 4.612155914306641,
      "learning_rate": 9.561367095613671e-06,
      "loss": 0.5243,
      "step": 5850
    },
    {
      "epoch": 2.4325446243254465,
      "grad_norm": 4.9941630363464355,
      "learning_rate": 9.492182094921822e-06,
      "loss": 0.482,
      "step": 5860
    },
    {
      "epoch": 2.4366957243669574,
      "grad_norm": 3.5231988430023193,
      "learning_rate": 9.42299709422997e-06,
      "loss": 0.5064,
      "step": 5870
    },
    {
      "epoch": 2.4408468244084682,
      "grad_norm": 3.6995856761932373,
      "learning_rate": 9.35381209353812e-06,
      "loss": 0.4558,
      "step": 5880
    },
    {
      "epoch": 2.444997924449979,
      "grad_norm": 2.990368127822876,
      "learning_rate": 9.284627092846271e-06,
      "loss": 0.4625,
      "step": 5890
    },
    {
      "epoch": 2.4491490244914904,
      "grad_norm": 2.9481287002563477,
      "learning_rate": 9.215442092154421e-06,
      "loss": 0.4479,
      "step": 5900
    },
    {
      "epoch": 2.4533001245330013,
      "grad_norm": 3.3781497478485107,
      "learning_rate": 9.146257091462572e-06,
      "loss": 0.4688,
      "step": 5910
    },
    {
      "epoch": 2.457451224574512,
      "grad_norm": 5.280824661254883,
      "learning_rate": 9.077072090770722e-06,
      "loss": 0.4318,
      "step": 5920
    },
    {
      "epoch": 2.461602324616023,
      "grad_norm": 3.404956340789795,
      "learning_rate": 9.007887090078872e-06,
      "loss": 0.4646,
      "step": 5930
    },
    {
      "epoch": 2.4657534246575343,
      "grad_norm": 3.8963263034820557,
      "learning_rate": 8.938702089387021e-06,
      "loss": 0.4519,
      "step": 5940
    },
    {
      "epoch": 2.469904524699045,
      "grad_norm": 2.0575735569000244,
      "learning_rate": 8.869517088695171e-06,
      "loss": 0.4518,
      "step": 5950
    },
    {
      "epoch": 2.474055624740556,
      "grad_norm": 3.2936532497406006,
      "learning_rate": 8.800332088003322e-06,
      "loss": 0.4164,
      "step": 5960
    },
    {
      "epoch": 2.4782067247820674,
      "grad_norm": 5.495433330535889,
      "learning_rate": 8.73114708731147e-06,
      "loss": 0.4718,
      "step": 5970
    },
    {
      "epoch": 2.4823578248235783,
      "grad_norm": 2.464552402496338,
      "learning_rate": 8.66196208661962e-06,
      "loss": 0.4529,
      "step": 5980
    },
    {
      "epoch": 2.486508924865089,
      "grad_norm": 3.9284911155700684,
      "learning_rate": 8.592777085927771e-06,
      "loss": 0.4788,
      "step": 5990
    },
    {
      "epoch": 2.4906600249066004,
      "grad_norm": 2.9122936725616455,
      "learning_rate": 8.523592085235922e-06,
      "loss": 0.4367,
      "step": 6000
    },
    {
      "epoch": 2.4948111249481113,
      "grad_norm": 2.678426742553711,
      "learning_rate": 8.45440708454407e-06,
      "loss": 0.4062,
      "step": 6010
    },
    {
      "epoch": 2.498962224989622,
      "grad_norm": 3.040463447570801,
      "learning_rate": 8.385222083852222e-06,
      "loss": 0.4417,
      "step": 6020
    },
    {
      "epoch": 2.5031133250311335,
      "grad_norm": 3.2755279541015625,
      "learning_rate": 8.316037083160373e-06,
      "loss": 0.4113,
      "step": 6030
    },
    {
      "epoch": 2.5072644250726444,
      "grad_norm": 3.430704116821289,
      "learning_rate": 8.246852082468521e-06,
      "loss": 0.441,
      "step": 6040
    },
    {
      "epoch": 2.5114155251141552,
      "grad_norm": 3.6885712146759033,
      "learning_rate": 8.177667081776672e-06,
      "loss": 0.4924,
      "step": 6050
    },
    {
      "epoch": 2.515566625155666,
      "grad_norm": 3.629617691040039,
      "learning_rate": 8.108482081084822e-06,
      "loss": 0.4902,
      "step": 6060
    },
    {
      "epoch": 2.5197177251971774,
      "grad_norm": 2.274113655090332,
      "learning_rate": 8.03929708039297e-06,
      "loss": 0.3843,
      "step": 6070
    },
    {
      "epoch": 2.5238688252386883,
      "grad_norm": 2.3223698139190674,
      "learning_rate": 7.970112079701121e-06,
      "loss": 0.386,
      "step": 6080
    },
    {
      "epoch": 2.528019925280199,
      "grad_norm": 3.9965968132019043,
      "learning_rate": 7.900927079009271e-06,
      "loss": 0.429,
      "step": 6090
    },
    {
      "epoch": 2.53217102532171,
      "grad_norm": 2.594529867172241,
      "learning_rate": 7.831742078317422e-06,
      "loss": 0.4469,
      "step": 6100
    },
    {
      "epoch": 2.5363221253632213,
      "grad_norm": 3.2630367279052734,
      "learning_rate": 7.76255707762557e-06,
      "loss": 0.4847,
      "step": 6110
    },
    {
      "epoch": 2.540473225404732,
      "grad_norm": 4.542961120605469,
      "learning_rate": 7.69337207693372e-06,
      "loss": 0.4641,
      "step": 6120
    },
    {
      "epoch": 2.544624325446243,
      "grad_norm": 2.847761631011963,
      "learning_rate": 7.62418707624187e-06,
      "loss": 0.4367,
      "step": 6130
    },
    {
      "epoch": 2.5487754254877544,
      "grad_norm": 3.4170455932617188,
      "learning_rate": 7.555002075550021e-06,
      "loss": 0.3986,
      "step": 6140
    },
    {
      "epoch": 2.5529265255292652,
      "grad_norm": 4.360344886779785,
      "learning_rate": 7.485817074858172e-06,
      "loss": 0.417,
      "step": 6150
    },
    {
      "epoch": 2.557077625570776,
      "grad_norm": 2.5109264850616455,
      "learning_rate": 7.416632074166321e-06,
      "loss": 0.4845,
      "step": 6160
    },
    {
      "epoch": 2.5612287256122874,
      "grad_norm": 6.560712814331055,
      "learning_rate": 7.3474470734744716e-06,
      "loss": 0.537,
      "step": 6170
    },
    {
      "epoch": 2.5653798256537983,
      "grad_norm": 5.150742530822754,
      "learning_rate": 7.278262072782621e-06,
      "loss": 0.4428,
      "step": 6180
    },
    {
      "epoch": 2.569530925695309,
      "grad_norm": 3.1103274822235107,
      "learning_rate": 7.209077072090771e-06,
      "loss": 0.4334,
      "step": 6190
    },
    {
      "epoch": 2.5736820257368205,
      "grad_norm": 2.167994737625122,
      "learning_rate": 7.139892071398921e-06,
      "loss": 0.4212,
      "step": 6200
    },
    {
      "epoch": 2.5778331257783313,
      "grad_norm": 3.4881625175476074,
      "learning_rate": 7.0707070707070704e-06,
      "loss": 0.4899,
      "step": 6210
    },
    {
      "epoch": 2.581984225819842,
      "grad_norm": 3.0019888877868652,
      "learning_rate": 7.001522070015221e-06,
      "loss": 0.4948,
      "step": 6220
    },
    {
      "epoch": 2.5861353258613535,
      "grad_norm": 2.4730756282806396,
      "learning_rate": 6.93233706932337e-06,
      "loss": 0.4683,
      "step": 6230
    },
    {
      "epoch": 2.5902864259028644,
      "grad_norm": 2.6834146976470947,
      "learning_rate": 6.863152068631521e-06,
      "loss": 0.479,
      "step": 6240
    },
    {
      "epoch": 2.5944375259443753,
      "grad_norm": 3.8449008464813232,
      "learning_rate": 6.793967067939672e-06,
      "loss": 0.4603,
      "step": 6250
    },
    {
      "epoch": 2.598588625985886,
      "grad_norm": 2.873915672302246,
      "learning_rate": 6.724782067247821e-06,
      "loss": 0.4663,
      "step": 6260
    },
    {
      "epoch": 2.602739726027397,
      "grad_norm": 4.761617183685303,
      "learning_rate": 6.655597066555972e-06,
      "loss": 0.4657,
      "step": 6270
    },
    {
      "epoch": 2.6068908260689083,
      "grad_norm": 2.2609798908233643,
      "learning_rate": 6.586412065864121e-06,
      "loss": 0.4454,
      "step": 6280
    },
    {
      "epoch": 2.611041926110419,
      "grad_norm": 2.9962456226348877,
      "learning_rate": 6.517227065172271e-06,
      "loss": 0.4192,
      "step": 6290
    },
    {
      "epoch": 2.61519302615193,
      "grad_norm": 2.5869030952453613,
      "learning_rate": 6.448042064480421e-06,
      "loss": 0.4252,
      "step": 6300
    },
    {
      "epoch": 2.6193441261934414,
      "grad_norm": 3.5674705505371094,
      "learning_rate": 6.3788570637885705e-06,
      "loss": 0.4126,
      "step": 6310
    },
    {
      "epoch": 2.6234952262349522,
      "grad_norm": 2.212172746658325,
      "learning_rate": 6.309672063096721e-06,
      "loss": 0.4183,
      "step": 6320
    },
    {
      "epoch": 2.627646326276463,
      "grad_norm": 2.228533983230591,
      "learning_rate": 6.24048706240487e-06,
      "loss": 0.3907,
      "step": 6330
    },
    {
      "epoch": 2.6317974263179744,
      "grad_norm": 4.327417850494385,
      "learning_rate": 6.171302061713021e-06,
      "loss": 0.4341,
      "step": 6340
    },
    {
      "epoch": 2.6359485263594853,
      "grad_norm": 4.519217491149902,
      "learning_rate": 6.102117061021171e-06,
      "loss": 0.3817,
      "step": 6350
    },
    {
      "epoch": 2.640099626400996,
      "grad_norm": 5.0214643478393555,
      "learning_rate": 6.032932060329321e-06,
      "loss": 0.4677,
      "step": 6360
    },
    {
      "epoch": 2.6442507264425075,
      "grad_norm": 6.5699262619018555,
      "learning_rate": 5.963747059637471e-06,
      "loss": 0.4444,
      "step": 6370
    },
    {
      "epoch": 2.6484018264840183,
      "grad_norm": 3.0548365116119385,
      "learning_rate": 5.8945620589456205e-06,
      "loss": 0.4468,
      "step": 6380
    },
    {
      "epoch": 2.652552926525529,
      "grad_norm": 2.7243871688842773,
      "learning_rate": 5.825377058253771e-06,
      "loss": 0.4679,
      "step": 6390
    },
    {
      "epoch": 2.6567040265670405,
      "grad_norm": 1.9134180545806885,
      "learning_rate": 5.756192057561921e-06,
      "loss": 0.4737,
      "step": 6400
    },
    {
      "epoch": 2.6608551266085514,
      "grad_norm": 2.511350154876709,
      "learning_rate": 5.687007056870071e-06,
      "loss": 0.4758,
      "step": 6410
    },
    {
      "epoch": 2.6650062266500623,
      "grad_norm": 3.4166996479034424,
      "learning_rate": 5.617822056178221e-06,
      "loss": 0.4333,
      "step": 6420
    },
    {
      "epoch": 2.669157326691573,
      "grad_norm": 4.710052490234375,
      "learning_rate": 5.5486370554863705e-06,
      "loss": 0.4485,
      "step": 6430
    },
    {
      "epoch": 2.673308426733084,
      "grad_norm": 2.4525179862976074,
      "learning_rate": 5.479452054794521e-06,
      "loss": 0.4837,
      "step": 6440
    },
    {
      "epoch": 2.6774595267745953,
      "grad_norm": 3.322222948074341,
      "learning_rate": 5.410267054102671e-06,
      "loss": 0.4165,
      "step": 6450
    },
    {
      "epoch": 2.681610626816106,
      "grad_norm": 2.378962993621826,
      "learning_rate": 5.341082053410821e-06,
      "loss": 0.4133,
      "step": 6460
    },
    {
      "epoch": 2.685761726857617,
      "grad_norm": 4.307488918304443,
      "learning_rate": 5.271897052718971e-06,
      "loss": 0.4618,
      "step": 6470
    },
    {
      "epoch": 2.6899128268991284,
      "grad_norm": 3.583951234817505,
      "learning_rate": 5.202712052027121e-06,
      "loss": 0.5225,
      "step": 6480
    },
    {
      "epoch": 2.6940639269406392,
      "grad_norm": 2.7284934520721436,
      "learning_rate": 5.133527051335271e-06,
      "loss": 0.4338,
      "step": 6490
    },
    {
      "epoch": 2.69821502698215,
      "grad_norm": 4.315772533416748,
      "learning_rate": 5.0643420506434204e-06,
      "loss": 0.4739,
      "step": 6500
    },
    {
      "epoch": 2.7023661270236614,
      "grad_norm": 3.549102306365967,
      "learning_rate": 4.995157049951571e-06,
      "loss": 0.4741,
      "step": 6510
    },
    {
      "epoch": 2.7065172270651723,
      "grad_norm": 5.148817539215088,
      "learning_rate": 4.925972049259721e-06,
      "loss": 0.4485,
      "step": 6520
    },
    {
      "epoch": 2.710668327106683,
      "grad_norm": 2.524836301803589,
      "learning_rate": 4.856787048567871e-06,
      "loss": 0.4572,
      "step": 6530
    },
    {
      "epoch": 2.7148194271481945,
      "grad_norm": 3.6168622970581055,
      "learning_rate": 4.787602047876021e-06,
      "loss": 0.4978,
      "step": 6540
    },
    {
      "epoch": 2.7189705271897053,
      "grad_norm": 3.6505587100982666,
      "learning_rate": 4.7184170471841705e-06,
      "loss": 0.4511,
      "step": 6550
    },
    {
      "epoch": 2.723121627231216,
      "grad_norm": 4.366391658782959,
      "learning_rate": 4.649232046492321e-06,
      "loss": 0.4381,
      "step": 6560
    },
    {
      "epoch": 2.7272727272727275,
      "grad_norm": 2.865727663040161,
      "learning_rate": 4.580047045800471e-06,
      "loss": 0.509,
      "step": 6570
    },
    {
      "epoch": 2.7314238273142384,
      "grad_norm": 4.039093971252441,
      "learning_rate": 4.510862045108621e-06,
      "loss": 0.4307,
      "step": 6580
    },
    {
      "epoch": 2.7355749273557493,
      "grad_norm": 2.5852458477020264,
      "learning_rate": 4.44167704441677e-06,
      "loss": 0.4933,
      "step": 6590
    },
    {
      "epoch": 2.73972602739726,
      "grad_norm": 5.612527370452881,
      "learning_rate": 4.3724920437249205e-06,
      "loss": 0.4476,
      "step": 6600
    },
    {
      "epoch": 2.7438771274387714,
      "grad_norm": 1.9977903366088867,
      "learning_rate": 4.30330704303307e-06,
      "loss": 0.4361,
      "step": 6610
    },
    {
      "epoch": 2.7480282274802823,
      "grad_norm": 2.9236180782318115,
      "learning_rate": 4.234122042341221e-06,
      "loss": 0.3794,
      "step": 6620
    },
    {
      "epoch": 2.752179327521793,
      "grad_norm": 3.5530338287353516,
      "learning_rate": 4.164937041649371e-06,
      "loss": 0.414,
      "step": 6630
    },
    {
      "epoch": 2.756330427563304,
      "grad_norm": 3.308979034423828,
      "learning_rate": 4.09575204095752e-06,
      "loss": 0.4712,
      "step": 6640
    },
    {
      "epoch": 2.7604815276048154,
      "grad_norm": 2.717494249343872,
      "learning_rate": 4.026567040265671e-06,
      "loss": 0.4519,
      "step": 6650
    },
    {
      "epoch": 2.7646326276463262,
      "grad_norm": 3.17117977142334,
      "learning_rate": 3.95738203957382e-06,
      "loss": 0.4374,
      "step": 6660
    },
    {
      "epoch": 2.768783727687837,
      "grad_norm": 3.214792251586914,
      "learning_rate": 3.8881970388819705e-06,
      "loss": 0.4279,
      "step": 6670
    },
    {
      "epoch": 2.7729348277293484,
      "grad_norm": 3.7835938930511475,
      "learning_rate": 3.819012038190121e-06,
      "loss": 0.3938,
      "step": 6680
    },
    {
      "epoch": 2.7770859277708593,
      "grad_norm": 2.3033621311187744,
      "learning_rate": 3.7498270374982707e-06,
      "loss": 0.4223,
      "step": 6690
    },
    {
      "epoch": 2.78123702781237,
      "grad_norm": 2.400568723678589,
      "learning_rate": 3.6806420368064207e-06,
      "loss": 0.4673,
      "step": 6700
    },
    {
      "epoch": 2.7853881278538815,
      "grad_norm": 3.620907783508301,
      "learning_rate": 3.6114570361145706e-06,
      "loss": 0.463,
      "step": 6710
    },
    {
      "epoch": 2.7895392278953923,
      "grad_norm": 3.2111432552337646,
      "learning_rate": 3.54227203542272e-06,
      "loss": 0.4028,
      "step": 6720
    },
    {
      "epoch": 2.793690327936903,
      "grad_norm": 2.4716200828552246,
      "learning_rate": 3.473087034730871e-06,
      "loss": 0.4684,
      "step": 6730
    },
    {
      "epoch": 2.7978414279784145,
      "grad_norm": 2.99355411529541,
      "learning_rate": 3.403902034039021e-06,
      "loss": 0.4732,
      "step": 6740
    },
    {
      "epoch": 2.8019925280199254,
      "grad_norm": 3.241478204727173,
      "learning_rate": 3.3347170333471707e-06,
      "loss": 0.4013,
      "step": 6750
    },
    {
      "epoch": 2.8061436280614362,
      "grad_norm": 4.333837509155273,
      "learning_rate": 3.2655320326553202e-06,
      "loss": 0.524,
      "step": 6760
    },
    {
      "epoch": 2.810294728102947,
      "grad_norm": 5.258843898773193,
      "learning_rate": 3.19634703196347e-06,
      "loss": 0.4531,
      "step": 6770
    },
    {
      "epoch": 2.8144458281444584,
      "grad_norm": 3.148975372314453,
      "learning_rate": 3.12716203127162e-06,
      "loss": 0.4645,
      "step": 6780
    },
    {
      "epoch": 2.8185969281859693,
      "grad_norm": 2.6173315048217773,
      "learning_rate": 3.0579770305797704e-06,
      "loss": 0.4247,
      "step": 6790
    },
    {
      "epoch": 2.82274802822748,
      "grad_norm": 3.684619188308716,
      "learning_rate": 2.9887920298879208e-06,
      "loss": 0.4698,
      "step": 6800
    },
    {
      "epoch": 2.826899128268991,
      "grad_norm": 5.680983543395996,
      "learning_rate": 2.9196070291960703e-06,
      "loss": 0.4495,
      "step": 6810
    },
    {
      "epoch": 2.8310502283105023,
      "grad_norm": 2.838636636734009,
      "learning_rate": 2.8504220285042202e-06,
      "loss": 0.4502,
      "step": 6820
    },
    {
      "epoch": 2.835201328352013,
      "grad_norm": 3.9077465534210205,
      "learning_rate": 2.7812370278123706e-06,
      "loss": 0.5096,
      "step": 6830
    },
    {
      "epoch": 2.839352428393524,
      "grad_norm": 3.9931039810180664,
      "learning_rate": 2.7120520271205205e-06,
      "loss": 0.4733,
      "step": 6840
    },
    {
      "epoch": 2.8435035284350354,
      "grad_norm": 3.800952672958374,
      "learning_rate": 2.6428670264286704e-06,
      "loss": 0.4427,
      "step": 6850
    },
    {
      "epoch": 2.8476546284765463,
      "grad_norm": 3.220796585083008,
      "learning_rate": 2.5736820257368203e-06,
      "loss": 0.4155,
      "step": 6860
    },
    {
      "epoch": 2.851805728518057,
      "grad_norm": 3.3937206268310547,
      "learning_rate": 2.5044970250449703e-06,
      "loss": 0.419,
      "step": 6870
    },
    {
      "epoch": 2.8559568285595685,
      "grad_norm": 3.039151191711426,
      "learning_rate": 2.43531202435312e-06,
      "loss": 0.4514,
      "step": 6880
    },
    {
      "epoch": 2.8601079286010793,
      "grad_norm": 2.780179738998413,
      "learning_rate": 2.3661270236612705e-06,
      "loss": 0.4305,
      "step": 6890
    },
    {
      "epoch": 2.86425902864259,
      "grad_norm": 2.696377754211426,
      "learning_rate": 2.2969420229694205e-06,
      "loss": 0.6011,
      "step": 6900
    },
    {
      "epoch": 2.8684101286841015,
      "grad_norm": 2.5083508491516113,
      "learning_rate": 2.22775702227757e-06,
      "loss": 0.4228,
      "step": 6910
    },
    {
      "epoch": 2.8725612287256124,
      "grad_norm": 3.2688443660736084,
      "learning_rate": 2.1585720215857203e-06,
      "loss": 0.4459,
      "step": 6920
    },
    {
      "epoch": 2.8767123287671232,
      "grad_norm": 4.3028950691223145,
      "learning_rate": 2.0893870208938703e-06,
      "loss": 0.4148,
      "step": 6930
    },
    {
      "epoch": 2.8808634288086346,
      "grad_norm": 3.6446282863616943,
      "learning_rate": 2.0202020202020206e-06,
      "loss": 0.4307,
      "step": 6940
    },
    {
      "epoch": 2.8850145288501454,
      "grad_norm": 2.4966466426849365,
      "learning_rate": 1.9510170195101705e-06,
      "loss": 0.4689,
      "step": 6950
    },
    {
      "epoch": 2.8891656288916563,
      "grad_norm": 3.4870638847351074,
      "learning_rate": 1.8818320188183202e-06,
      "loss": 0.4464,
      "step": 6960
    },
    {
      "epoch": 2.893316728933167,
      "grad_norm": 4.463098526000977,
      "learning_rate": 1.8126470181264704e-06,
      "loss": 0.5187,
      "step": 6970
    },
    {
      "epoch": 2.897467828974678,
      "grad_norm": 3.49200177192688,
      "learning_rate": 1.7434620174346203e-06,
      "loss": 0.474,
      "step": 6980
    },
    {
      "epoch": 2.9016189290161893,
      "grad_norm": 6.396158218383789,
      "learning_rate": 1.6742770167427702e-06,
      "loss": 0.4896,
      "step": 6990
    },
    {
      "epoch": 2.9057700290577,
      "grad_norm": 1.4566043615341187,
      "learning_rate": 1.6050920160509204e-06,
      "loss": 0.3669,
      "step": 7000
    },
    {
      "epoch": 2.909921129099211,
      "grad_norm": 2.156660318374634,
      "learning_rate": 1.5359070153590703e-06,
      "loss": 0.4296,
      "step": 7010
    },
    {
      "epoch": 2.9140722291407224,
      "grad_norm": 3.0500850677490234,
      "learning_rate": 1.4667220146672202e-06,
      "loss": 0.444,
      "step": 7020
    },
    {
      "epoch": 2.9182233291822333,
      "grad_norm": 2.303926467895508,
      "learning_rate": 1.3975370139753702e-06,
      "loss": 0.4204,
      "step": 7030
    },
    {
      "epoch": 2.922374429223744,
      "grad_norm": 3.0600035190582275,
      "learning_rate": 1.32835201328352e-06,
      "loss": 0.4399,
      "step": 7040
    },
    {
      "epoch": 2.9265255292652554,
      "grad_norm": 2.0387275218963623,
      "learning_rate": 1.2591670125916702e-06,
      "loss": 0.4147,
      "step": 7050
    },
    {
      "epoch": 2.9306766293067663,
      "grad_norm": 2.690870761871338,
      "learning_rate": 1.1899820118998202e-06,
      "loss": 0.4981,
      "step": 7060
    },
    {
      "epoch": 2.934827729348277,
      "grad_norm": 3.158388614654541,
      "learning_rate": 1.12079701120797e-06,
      "loss": 0.5024,
      "step": 7070
    },
    {
      "epoch": 2.9389788293897885,
      "grad_norm": 4.154670715332031,
      "learning_rate": 1.0516120105161202e-06,
      "loss": 0.474,
      "step": 7080
    },
    {
      "epoch": 2.9431299294312994,
      "grad_norm": 2.541141986846924,
      "learning_rate": 9.824270098242701e-07,
      "loss": 0.3846,
      "step": 7090
    },
    {
      "epoch": 2.9472810294728102,
      "grad_norm": 3.5070183277130127,
      "learning_rate": 9.132420091324201e-07,
      "loss": 0.4894,
      "step": 7100
    },
    {
      "epoch": 2.9514321295143215,
      "grad_norm": 2.8181710243225098,
      "learning_rate": 8.440570084405701e-07,
      "loss": 0.4011,
      "step": 7110
    },
    {
      "epoch": 2.9555832295558324,
      "grad_norm": 3.9597697257995605,
      "learning_rate": 7.7487200774872e-07,
      "loss": 0.5214,
      "step": 7120
    },
    {
      "epoch": 2.9597343295973433,
      "grad_norm": 2.712794542312622,
      "learning_rate": 7.056870070568702e-07,
      "loss": 0.3871,
      "step": 7130
    },
    {
      "epoch": 2.963885429638854,
      "grad_norm": 3.251826047897339,
      "learning_rate": 6.365020063650201e-07,
      "loss": 0.4627,
      "step": 7140
    },
    {
      "epoch": 2.968036529680365,
      "grad_norm": 3.7257375717163086,
      "learning_rate": 5.673170056731701e-07,
      "loss": 0.4243,
      "step": 7150
    },
    {
      "epoch": 2.9721876297218763,
      "grad_norm": 2.7699031829833984,
      "learning_rate": 4.981320049813201e-07,
      "loss": 0.4278,
      "step": 7160
    },
    {
      "epoch": 2.976338729763387,
      "grad_norm": 2.5188350677490234,
      "learning_rate": 4.2894700428947004e-07,
      "loss": 0.4858,
      "step": 7170
    },
    {
      "epoch": 2.980489829804898,
      "grad_norm": 3.474560499191284,
      "learning_rate": 3.5976200359762007e-07,
      "loss": 0.4068,
      "step": 7180
    },
    {
      "epoch": 2.9846409298464094,
      "grad_norm": 3.953831195831299,
      "learning_rate": 2.9057700290577005e-07,
      "loss": 0.414,
      "step": 7190
    },
    {
      "epoch": 2.9887920298879203,
      "grad_norm": 2.807579278945923,
      "learning_rate": 2.2139200221392e-07,
      "loss": 0.4236,
      "step": 7200
    },
    {
      "epoch": 2.992943129929431,
      "grad_norm": 3.637894630432129,
      "learning_rate": 1.5220700152207e-07,
      "loss": 0.3729,
      "step": 7210
    },
    {
      "epoch": 2.9970942299709424,
      "grad_norm": 3.0563416481018066,
      "learning_rate": 8.302200083022e-08,
      "loss": 0.3961,
      "step": 7220
    }
  ],
  "logging_steps": 10,
  "max_steps": 7227,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.2793971231424512e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
