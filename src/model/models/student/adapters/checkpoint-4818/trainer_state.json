{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 4818,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.004151100041511001,
      "grad_norm": 4.3059797286987305,
      "learning_rate": 4.993773349937734e-05,
      "loss": 2.3651,
      "step": 10
    },
    {
      "epoch": 0.008302200083022002,
      "grad_norm": 5.32832670211792,
      "learning_rate": 4.987546699875467e-05,
      "loss": 2.0273,
      "step": 20
    },
    {
      "epoch": 0.012453300124533,
      "grad_norm": 5.461641788482666,
      "learning_rate": 4.980628199806282e-05,
      "loss": 2.1037,
      "step": 30
    },
    {
      "epoch": 0.016604400166044003,
      "grad_norm": 5.145603656768799,
      "learning_rate": 4.973709699737097e-05,
      "loss": 1.7298,
      "step": 40
    },
    {
      "epoch": 0.020755500207555,
      "grad_norm": 4.261973857879639,
      "learning_rate": 4.966791199667912e-05,
      "loss": 2.0019,
      "step": 50
    },
    {
      "epoch": 0.024906600249066,
      "grad_norm": 8.475198745727539,
      "learning_rate": 4.959872699598727e-05,
      "loss": 1.6866,
      "step": 60
    },
    {
      "epoch": 0.029057700290577002,
      "grad_norm": 4.758548736572266,
      "learning_rate": 4.952954199529542e-05,
      "loss": 1.7693,
      "step": 70
    },
    {
      "epoch": 0.033208800332088007,
      "grad_norm": 7.128340721130371,
      "learning_rate": 4.9460356994603574e-05,
      "loss": 1.6236,
      "step": 80
    },
    {
      "epoch": 0.037359900373599,
      "grad_norm": 2.7894136905670166,
      "learning_rate": 4.939809049398091e-05,
      "loss": 1.5289,
      "step": 90
    },
    {
      "epoch": 0.04151100041511,
      "grad_norm": 8.56430435180664,
      "learning_rate": 4.932890549328906e-05,
      "loss": 1.4763,
      "step": 100
    },
    {
      "epoch": 0.045662100456621,
      "grad_norm": 8.165678024291992,
      "learning_rate": 4.925972049259721e-05,
      "loss": 1.4228,
      "step": 110
    },
    {
      "epoch": 0.049813200498132,
      "grad_norm": 6.671384811401367,
      "learning_rate": 4.9190535491905357e-05,
      "loss": 1.1917,
      "step": 120
    },
    {
      "epoch": 0.053964300539643004,
      "grad_norm": 5.171438694000244,
      "learning_rate": 4.9121350491213505e-05,
      "loss": 1.1527,
      "step": 130
    },
    {
      "epoch": 0.058115400581154004,
      "grad_norm": 4.018444061279297,
      "learning_rate": 4.9052165490521654e-05,
      "loss": 1.0755,
      "step": 140
    },
    {
      "epoch": 0.062266500622665005,
      "grad_norm": 3.6970653533935547,
      "learning_rate": 4.89829804898298e-05,
      "loss": 1.0145,
      "step": 150
    },
    {
      "epoch": 0.06641760066417601,
      "grad_norm": 5.286396026611328,
      "learning_rate": 4.891379548913796e-05,
      "loss": 1.087,
      "step": 160
    },
    {
      "epoch": 0.070568700705687,
      "grad_norm": 3.5865163803100586,
      "learning_rate": 4.884461048844611e-05,
      "loss": 0.9438,
      "step": 170
    },
    {
      "epoch": 0.074719800747198,
      "grad_norm": 7.176278114318848,
      "learning_rate": 4.8775425487754255e-05,
      "loss": 0.9598,
      "step": 180
    },
    {
      "epoch": 0.07887090078870901,
      "grad_norm": 5.436966896057129,
      "learning_rate": 4.8706240487062404e-05,
      "loss": 0.8652,
      "step": 190
    },
    {
      "epoch": 0.08302200083022,
      "grad_norm": 11.377484321594238,
      "learning_rate": 4.863705548637056e-05,
      "loss": 0.9998,
      "step": 200
    },
    {
      "epoch": 0.08717310087173101,
      "grad_norm": 6.021442890167236,
      "learning_rate": 4.856787048567871e-05,
      "loss": 0.8509,
      "step": 210
    },
    {
      "epoch": 0.091324200913242,
      "grad_norm": 4.540760517120361,
      "learning_rate": 4.849868548498686e-05,
      "loss": 0.9948,
      "step": 220
    },
    {
      "epoch": 0.09547530095475301,
      "grad_norm": 4.142073154449463,
      "learning_rate": 4.842950048429501e-05,
      "loss": 0.9416,
      "step": 230
    },
    {
      "epoch": 0.099626400996264,
      "grad_norm": 3.2602484226226807,
      "learning_rate": 4.836031548360316e-05,
      "loss": 0.8154,
      "step": 240
    },
    {
      "epoch": 0.10377750103777501,
      "grad_norm": 3.810708999633789,
      "learning_rate": 4.829113048291131e-05,
      "loss": 0.7992,
      "step": 250
    },
    {
      "epoch": 0.10792860107928601,
      "grad_norm": 6.513706684112549,
      "learning_rate": 4.822194548221946e-05,
      "loss": 0.9033,
      "step": 260
    },
    {
      "epoch": 0.11207970112079702,
      "grad_norm": 2.9157445430755615,
      "learning_rate": 4.815276048152761e-05,
      "loss": 0.8887,
      "step": 270
    },
    {
      "epoch": 0.11623080116230801,
      "grad_norm": 5.460794925689697,
      "learning_rate": 4.8083575480835755e-05,
      "loss": 0.8226,
      "step": 280
    },
    {
      "epoch": 0.12038190120381902,
      "grad_norm": 4.34400749206543,
      "learning_rate": 4.8014390480143904e-05,
      "loss": 0.8097,
      "step": 290
    },
    {
      "epoch": 0.12453300124533001,
      "grad_norm": 5.633107662200928,
      "learning_rate": 4.794520547945205e-05,
      "loss": 0.8627,
      "step": 300
    },
    {
      "epoch": 0.12868410128684102,
      "grad_norm": 6.58763313293457,
      "learning_rate": 4.787602047876021e-05,
      "loss": 0.8072,
      "step": 310
    },
    {
      "epoch": 0.13283520132835203,
      "grad_norm": 5.8954386711120605,
      "learning_rate": 4.780683547806836e-05,
      "loss": 0.8613,
      "step": 320
    },
    {
      "epoch": 0.136986301369863,
      "grad_norm": 3.6711199283599854,
      "learning_rate": 4.7737650477376505e-05,
      "loss": 0.8413,
      "step": 330
    },
    {
      "epoch": 0.141137401411374,
      "grad_norm": 3.5405843257904053,
      "learning_rate": 4.766846547668466e-05,
      "loss": 0.7788,
      "step": 340
    },
    {
      "epoch": 0.14528850145288502,
      "grad_norm": 6.783627510070801,
      "learning_rate": 4.759928047599281e-05,
      "loss": 0.7846,
      "step": 350
    },
    {
      "epoch": 0.149439601494396,
      "grad_norm": 6.4385457038879395,
      "learning_rate": 4.753009547530096e-05,
      "loss": 0.8146,
      "step": 360
    },
    {
      "epoch": 0.153590701535907,
      "grad_norm": 3.5638692378997803,
      "learning_rate": 4.746091047460911e-05,
      "loss": 0.7538,
      "step": 370
    },
    {
      "epoch": 0.15774180157741802,
      "grad_norm": 2.7514185905456543,
      "learning_rate": 4.7391725473917256e-05,
      "loss": 0.7705,
      "step": 380
    },
    {
      "epoch": 0.16189290161892902,
      "grad_norm": 4.323678493499756,
      "learning_rate": 4.7322540473225404e-05,
      "loss": 0.7534,
      "step": 390
    },
    {
      "epoch": 0.16604400166044,
      "grad_norm": 20.162662506103516,
      "learning_rate": 4.725335547253355e-05,
      "loss": 0.7557,
      "step": 400
    },
    {
      "epoch": 0.170195101701951,
      "grad_norm": 3.529186248779297,
      "learning_rate": 4.71841704718417e-05,
      "loss": 0.7951,
      "step": 410
    },
    {
      "epoch": 0.17434620174346202,
      "grad_norm": 3.9123666286468506,
      "learning_rate": 4.711498547114986e-05,
      "loss": 0.7135,
      "step": 420
    },
    {
      "epoch": 0.17849730178497303,
      "grad_norm": 4.587635517120361,
      "learning_rate": 4.7045800470458006e-05,
      "loss": 0.7668,
      "step": 430
    },
    {
      "epoch": 0.182648401826484,
      "grad_norm": 3.369563579559326,
      "learning_rate": 4.6976615469766154e-05,
      "loss": 0.7018,
      "step": 440
    },
    {
      "epoch": 0.18679950186799502,
      "grad_norm": 6.754910469055176,
      "learning_rate": 4.690743046907431e-05,
      "loss": 0.8047,
      "step": 450
    },
    {
      "epoch": 0.19095060190950602,
      "grad_norm": 3.4845809936523438,
      "learning_rate": 4.683824546838246e-05,
      "loss": 0.6669,
      "step": 460
    },
    {
      "epoch": 0.19510170195101703,
      "grad_norm": 4.645039081573486,
      "learning_rate": 4.676906046769061e-05,
      "loss": 0.7772,
      "step": 470
    },
    {
      "epoch": 0.199252801992528,
      "grad_norm": 3.17895770072937,
      "learning_rate": 4.6699875466998756e-05,
      "loss": 0.7371,
      "step": 480
    },
    {
      "epoch": 0.20340390203403902,
      "grad_norm": 4.671679496765137,
      "learning_rate": 4.663069046630691e-05,
      "loss": 0.6668,
      "step": 490
    },
    {
      "epoch": 0.20755500207555003,
      "grad_norm": 5.362880706787109,
      "learning_rate": 4.656150546561506e-05,
      "loss": 0.7273,
      "step": 500
    },
    {
      "epoch": 0.21170610211706103,
      "grad_norm": 4.700826644897461,
      "learning_rate": 4.649232046492321e-05,
      "loss": 0.7526,
      "step": 510
    },
    {
      "epoch": 0.21585720215857201,
      "grad_norm": 3.257575273513794,
      "learning_rate": 4.642313546423136e-05,
      "loss": 0.6881,
      "step": 520
    },
    {
      "epoch": 0.22000830220008302,
      "grad_norm": 5.936589241027832,
      "learning_rate": 4.6353950463539506e-05,
      "loss": 0.6056,
      "step": 530
    },
    {
      "epoch": 0.22415940224159403,
      "grad_norm": 6.10017204284668,
      "learning_rate": 4.6284765462847654e-05,
      "loss": 0.7686,
      "step": 540
    },
    {
      "epoch": 0.228310502283105,
      "grad_norm": 2.9968924522399902,
      "learning_rate": 4.62155804621558e-05,
      "loss": 0.6752,
      "step": 550
    },
    {
      "epoch": 0.23246160232461602,
      "grad_norm": 3.506612539291382,
      "learning_rate": 4.614639546146396e-05,
      "loss": 0.681,
      "step": 560
    },
    {
      "epoch": 0.23661270236612703,
      "grad_norm": 3.0725836753845215,
      "learning_rate": 4.607721046077211e-05,
      "loss": 0.6352,
      "step": 570
    },
    {
      "epoch": 0.24076380240763803,
      "grad_norm": 4.44767951965332,
      "learning_rate": 4.6008025460080256e-05,
      "loss": 0.6645,
      "step": 580
    },
    {
      "epoch": 0.244914902449149,
      "grad_norm": 7.384735107421875,
      "learning_rate": 4.5938840459388404e-05,
      "loss": 0.7761,
      "step": 590
    },
    {
      "epoch": 0.24906600249066002,
      "grad_norm": 3.868206262588501,
      "learning_rate": 4.586965545869656e-05,
      "loss": 0.7425,
      "step": 600
    },
    {
      "epoch": 0.253217102532171,
      "grad_norm": 3.5629899501800537,
      "learning_rate": 4.580047045800471e-05,
      "loss": 0.7049,
      "step": 610
    },
    {
      "epoch": 0.25736820257368204,
      "grad_norm": 4.086206912994385,
      "learning_rate": 4.573128545731286e-05,
      "loss": 0.6563,
      "step": 620
    },
    {
      "epoch": 0.261519302615193,
      "grad_norm": 3.499788284301758,
      "learning_rate": 4.5662100456621006e-05,
      "loss": 0.6942,
      "step": 630
    },
    {
      "epoch": 0.26567040265670405,
      "grad_norm": 6.5791497230529785,
      "learning_rate": 4.559291545592916e-05,
      "loss": 0.7918,
      "step": 640
    },
    {
      "epoch": 0.26982150269821503,
      "grad_norm": 15.683422088623047,
      "learning_rate": 4.552373045523731e-05,
      "loss": 0.5997,
      "step": 650
    },
    {
      "epoch": 0.273972602739726,
      "grad_norm": 3.044811725616455,
      "learning_rate": 4.545454545454546e-05,
      "loss": 0.6764,
      "step": 660
    },
    {
      "epoch": 0.27812370278123705,
      "grad_norm": 3.596010684967041,
      "learning_rate": 4.538536045385361e-05,
      "loss": 0.6226,
      "step": 670
    },
    {
      "epoch": 0.282274802822748,
      "grad_norm": 7.694983005523682,
      "learning_rate": 4.5316175453161756e-05,
      "loss": 0.7318,
      "step": 680
    },
    {
      "epoch": 0.286425902864259,
      "grad_norm": 5.023863315582275,
      "learning_rate": 4.525390895253909e-05,
      "loss": 0.6891,
      "step": 690
    },
    {
      "epoch": 0.29057700290577004,
      "grad_norm": 3.712481737136841,
      "learning_rate": 4.518472395184724e-05,
      "loss": 0.6016,
      "step": 700
    },
    {
      "epoch": 0.294728102947281,
      "grad_norm": 4.44331693649292,
      "learning_rate": 4.511553895115539e-05,
      "loss": 0.7168,
      "step": 710
    },
    {
      "epoch": 0.298879202988792,
      "grad_norm": 4.928530216217041,
      "learning_rate": 4.5046353950463545e-05,
      "loss": 0.6803,
      "step": 720
    },
    {
      "epoch": 0.30303030303030304,
      "grad_norm": 4.051958084106445,
      "learning_rate": 4.4977168949771694e-05,
      "loss": 0.6847,
      "step": 730
    },
    {
      "epoch": 0.307181403071814,
      "grad_norm": 2.4930758476257324,
      "learning_rate": 4.490798394907984e-05,
      "loss": 0.6885,
      "step": 740
    },
    {
      "epoch": 0.31133250311332505,
      "grad_norm": 5.4142842292785645,
      "learning_rate": 4.483879894838799e-05,
      "loss": 0.653,
      "step": 750
    },
    {
      "epoch": 0.31548360315483603,
      "grad_norm": 5.446802139282227,
      "learning_rate": 4.476961394769614e-05,
      "loss": 0.5972,
      "step": 760
    },
    {
      "epoch": 0.319634703196347,
      "grad_norm": 4.371009826660156,
      "learning_rate": 4.470042894700429e-05,
      "loss": 0.6769,
      "step": 770
    },
    {
      "epoch": 0.32378580323785805,
      "grad_norm": 5.210651874542236,
      "learning_rate": 4.463124394631244e-05,
      "loss": 0.7557,
      "step": 780
    },
    {
      "epoch": 0.32793690327936903,
      "grad_norm": 2.723313093185425,
      "learning_rate": 4.4568977445689774e-05,
      "loss": 0.592,
      "step": 790
    },
    {
      "epoch": 0.33208800332088,
      "grad_norm": 3.291943073272705,
      "learning_rate": 4.449979244499793e-05,
      "loss": 0.6899,
      "step": 800
    },
    {
      "epoch": 0.33623910336239105,
      "grad_norm": 4.254903316497803,
      "learning_rate": 4.443060744430608e-05,
      "loss": 0.6572,
      "step": 810
    },
    {
      "epoch": 0.340390203403902,
      "grad_norm": 3.4049999713897705,
      "learning_rate": 4.4361422443614226e-05,
      "loss": 0.613,
      "step": 820
    },
    {
      "epoch": 0.34454130344541306,
      "grad_norm": 5.427980899810791,
      "learning_rate": 4.4292237442922375e-05,
      "loss": 0.6448,
      "step": 830
    },
    {
      "epoch": 0.34869240348692404,
      "grad_norm": 2.812769651412964,
      "learning_rate": 4.422305244223053e-05,
      "loss": 0.6096,
      "step": 840
    },
    {
      "epoch": 0.352843503528435,
      "grad_norm": 4.750669956207275,
      "learning_rate": 4.415386744153868e-05,
      "loss": 0.7245,
      "step": 850
    },
    {
      "epoch": 0.35699460356994606,
      "grad_norm": 5.736929416656494,
      "learning_rate": 4.408468244084683e-05,
      "loss": 0.6604,
      "step": 860
    },
    {
      "epoch": 0.36114570361145704,
      "grad_norm": 3.6056346893310547,
      "learning_rate": 4.4015497440154976e-05,
      "loss": 0.6988,
      "step": 870
    },
    {
      "epoch": 0.365296803652968,
      "grad_norm": 4.56833553314209,
      "learning_rate": 4.3946312439463125e-05,
      "loss": 0.6013,
      "step": 880
    },
    {
      "epoch": 0.36944790369447905,
      "grad_norm": 3.882204055786133,
      "learning_rate": 4.3877127438771274e-05,
      "loss": 0.6053,
      "step": 890
    },
    {
      "epoch": 0.37359900373599003,
      "grad_norm": 3.30889892578125,
      "learning_rate": 4.380794243807942e-05,
      "loss": 0.6499,
      "step": 900
    },
    {
      "epoch": 0.377750103777501,
      "grad_norm": 7.274662971496582,
      "learning_rate": 4.373875743738758e-05,
      "loss": 0.7054,
      "step": 910
    },
    {
      "epoch": 0.38190120381901205,
      "grad_norm": 3.6469690799713135,
      "learning_rate": 4.3669572436695726e-05,
      "loss": 0.6477,
      "step": 920
    },
    {
      "epoch": 0.386052303860523,
      "grad_norm": 4.538723945617676,
      "learning_rate": 4.3600387436003875e-05,
      "loss": 0.6273,
      "step": 930
    },
    {
      "epoch": 0.39020340390203406,
      "grad_norm": 3.23056960105896,
      "learning_rate": 4.3531202435312024e-05,
      "loss": 0.613,
      "step": 940
    },
    {
      "epoch": 0.39435450394354504,
      "grad_norm": 7.642653942108154,
      "learning_rate": 4.346201743462018e-05,
      "loss": 0.633,
      "step": 950
    },
    {
      "epoch": 0.398505603985056,
      "grad_norm": 3.3546154499053955,
      "learning_rate": 4.339283243392833e-05,
      "loss": 0.6089,
      "step": 960
    },
    {
      "epoch": 0.40265670402656706,
      "grad_norm": 4.321519374847412,
      "learning_rate": 4.3323647433236476e-05,
      "loss": 0.6923,
      "step": 970
    },
    {
      "epoch": 0.40680780406807804,
      "grad_norm": 2.3863086700439453,
      "learning_rate": 4.325446243254463e-05,
      "loss": 0.5658,
      "step": 980
    },
    {
      "epoch": 0.410958904109589,
      "grad_norm": 3.6434895992279053,
      "learning_rate": 4.318527743185278e-05,
      "loss": 0.6028,
      "step": 990
    },
    {
      "epoch": 0.41511000415110005,
      "grad_norm": 5.725226402282715,
      "learning_rate": 4.311609243116093e-05,
      "loss": 0.6325,
      "step": 1000
    },
    {
      "epoch": 0.41926110419261103,
      "grad_norm": 4.76446533203125,
      "learning_rate": 4.304690743046907e-05,
      "loss": 0.5909,
      "step": 1010
    },
    {
      "epoch": 0.42341220423412207,
      "grad_norm": 4.214288711547852,
      "learning_rate": 4.2977722429777227e-05,
      "loss": 0.6004,
      "step": 1020
    },
    {
      "epoch": 0.42756330427563305,
      "grad_norm": 4.735945224761963,
      "learning_rate": 4.2908537429085375e-05,
      "loss": 0.5784,
      "step": 1030
    },
    {
      "epoch": 0.43171440431714403,
      "grad_norm": 4.180710315704346,
      "learning_rate": 4.2839352428393524e-05,
      "loss": 0.6428,
      "step": 1040
    },
    {
      "epoch": 0.43586550435865506,
      "grad_norm": 4.279062747955322,
      "learning_rate": 4.277016742770167e-05,
      "loss": 0.6835,
      "step": 1050
    },
    {
      "epoch": 0.44001660440016604,
      "grad_norm": 3.9372341632843018,
      "learning_rate": 4.270098242700983e-05,
      "loss": 0.6512,
      "step": 1060
    },
    {
      "epoch": 0.444167704441677,
      "grad_norm": 3.824455738067627,
      "learning_rate": 4.2631797426317977e-05,
      "loss": 0.6949,
      "step": 1070
    },
    {
      "epoch": 0.44831880448318806,
      "grad_norm": 4.789069652557373,
      "learning_rate": 4.2562612425626125e-05,
      "loss": 0.5922,
      "step": 1080
    },
    {
      "epoch": 0.45246990452469904,
      "grad_norm": 5.5115966796875,
      "learning_rate": 4.249342742493428e-05,
      "loss": 0.6381,
      "step": 1090
    },
    {
      "epoch": 0.45662100456621,
      "grad_norm": 4.42945671081543,
      "learning_rate": 4.242424242424243e-05,
      "loss": 0.61,
      "step": 1100
    },
    {
      "epoch": 0.46077210460772106,
      "grad_norm": 2.34271502494812,
      "learning_rate": 4.235505742355058e-05,
      "loss": 0.5765,
      "step": 1110
    },
    {
      "epoch": 0.46492320464923204,
      "grad_norm": 2.8148207664489746,
      "learning_rate": 4.228587242285873e-05,
      "loss": 0.5204,
      "step": 1120
    },
    {
      "epoch": 0.46907430469074307,
      "grad_norm": 3.144775867462158,
      "learning_rate": 4.2216687422166875e-05,
      "loss": 0.6475,
      "step": 1130
    },
    {
      "epoch": 0.47322540473225405,
      "grad_norm": 4.02853536605835,
      "learning_rate": 4.2147502421475024e-05,
      "loss": 0.6311,
      "step": 1140
    },
    {
      "epoch": 0.47737650477376503,
      "grad_norm": 4.796026229858398,
      "learning_rate": 4.207831742078317e-05,
      "loss": 0.5695,
      "step": 1150
    },
    {
      "epoch": 0.48152760481527607,
      "grad_norm": 3.2646241188049316,
      "learning_rate": 4.200913242009132e-05,
      "loss": 0.6818,
      "step": 1160
    },
    {
      "epoch": 0.48567870485678705,
      "grad_norm": 5.142189025878906,
      "learning_rate": 4.193994741939948e-05,
      "loss": 0.6938,
      "step": 1170
    },
    {
      "epoch": 0.489829804898298,
      "grad_norm": 2.798579216003418,
      "learning_rate": 4.1870762418707625e-05,
      "loss": 0.5887,
      "step": 1180
    },
    {
      "epoch": 0.49398090493980906,
      "grad_norm": 4.962470531463623,
      "learning_rate": 4.1801577418015774e-05,
      "loss": 0.5715,
      "step": 1190
    },
    {
      "epoch": 0.49813200498132004,
      "grad_norm": 5.0474443435668945,
      "learning_rate": 4.173239241732393e-05,
      "loss": 0.533,
      "step": 1200
    },
    {
      "epoch": 0.502283105022831,
      "grad_norm": 4.618270397186279,
      "learning_rate": 4.166320741663208e-05,
      "loss": 0.6175,
      "step": 1210
    },
    {
      "epoch": 0.506434205064342,
      "grad_norm": 3.824035406112671,
      "learning_rate": 4.159402241594023e-05,
      "loss": 0.6191,
      "step": 1220
    },
    {
      "epoch": 0.5105853051058531,
      "grad_norm": 3.0424234867095947,
      "learning_rate": 4.1524837415248375e-05,
      "loss": 0.5774,
      "step": 1230
    },
    {
      "epoch": 0.5147364051473641,
      "grad_norm": 2.5544328689575195,
      "learning_rate": 4.145565241455653e-05,
      "loss": 0.6728,
      "step": 1240
    },
    {
      "epoch": 0.518887505188875,
      "grad_norm": 5.171296119689941,
      "learning_rate": 4.138646741386468e-05,
      "loss": 0.6127,
      "step": 1250
    },
    {
      "epoch": 0.523038605230386,
      "grad_norm": 3.2016706466674805,
      "learning_rate": 4.131728241317283e-05,
      "loss": 0.6162,
      "step": 1260
    },
    {
      "epoch": 0.527189705271897,
      "grad_norm": 3.616546154022217,
      "learning_rate": 4.124809741248098e-05,
      "loss": 0.5987,
      "step": 1270
    },
    {
      "epoch": 0.5313408053134081,
      "grad_norm": 2.56830096244812,
      "learning_rate": 4.1178912411789126e-05,
      "loss": 0.6124,
      "step": 1280
    },
    {
      "epoch": 0.5354919053549191,
      "grad_norm": 2.901643753051758,
      "learning_rate": 4.1109727411097274e-05,
      "loss": 0.5658,
      "step": 1290
    },
    {
      "epoch": 0.5396430053964301,
      "grad_norm": 5.079092979431152,
      "learning_rate": 4.104054241040542e-05,
      "loss": 0.6031,
      "step": 1300
    },
    {
      "epoch": 0.543794105437941,
      "grad_norm": 4.434463977813721,
      "learning_rate": 4.097135740971358e-05,
      "loss": 0.6169,
      "step": 1310
    },
    {
      "epoch": 0.547945205479452,
      "grad_norm": 3.5564939975738525,
      "learning_rate": 4.090217240902173e-05,
      "loss": 0.5398,
      "step": 1320
    },
    {
      "epoch": 0.552096305520963,
      "grad_norm": 5.306328773498535,
      "learning_rate": 4.0832987408329876e-05,
      "loss": 0.6875,
      "step": 1330
    },
    {
      "epoch": 0.5562474055624741,
      "grad_norm": 3.3287551403045654,
      "learning_rate": 4.0763802407638024e-05,
      "loss": 0.4883,
      "step": 1340
    },
    {
      "epoch": 0.5603985056039851,
      "grad_norm": 11.713359832763672,
      "learning_rate": 4.069461740694618e-05,
      "loss": 0.5605,
      "step": 1350
    },
    {
      "epoch": 0.564549605645496,
      "grad_norm": 3.2552103996276855,
      "learning_rate": 4.062543240625433e-05,
      "loss": 0.604,
      "step": 1360
    },
    {
      "epoch": 0.568700705687007,
      "grad_norm": 5.5841546058654785,
      "learning_rate": 4.055624740556248e-05,
      "loss": 0.5902,
      "step": 1370
    },
    {
      "epoch": 0.572851805728518,
      "grad_norm": 3.5645060539245605,
      "learning_rate": 4.0487062404870626e-05,
      "loss": 0.6335,
      "step": 1380
    },
    {
      "epoch": 0.5770029057700291,
      "grad_norm": 3.364152669906616,
      "learning_rate": 4.041787740417878e-05,
      "loss": 0.597,
      "step": 1390
    },
    {
      "epoch": 0.5811540058115401,
      "grad_norm": 4.414831161499023,
      "learning_rate": 4.034869240348692e-05,
      "loss": 0.5469,
      "step": 1400
    },
    {
      "epoch": 0.5853051058530511,
      "grad_norm": 2.3530948162078857,
      "learning_rate": 4.027950740279507e-05,
      "loss": 0.6027,
      "step": 1410
    },
    {
      "epoch": 0.589456205894562,
      "grad_norm": 5.299890518188477,
      "learning_rate": 4.021032240210322e-05,
      "loss": 0.6032,
      "step": 1420
    },
    {
      "epoch": 0.593607305936073,
      "grad_norm": 2.674837589263916,
      "learning_rate": 4.0141137401411376e-05,
      "loss": 0.4951,
      "step": 1430
    },
    {
      "epoch": 0.597758405977584,
      "grad_norm": 4.216536998748779,
      "learning_rate": 4.0071952400719524e-05,
      "loss": 0.5074,
      "step": 1440
    },
    {
      "epoch": 0.6019095060190951,
      "grad_norm": 3.873297691345215,
      "learning_rate": 4.000276740002767e-05,
      "loss": 0.4705,
      "step": 1450
    },
    {
      "epoch": 0.6060606060606061,
      "grad_norm": 5.626718044281006,
      "learning_rate": 3.993358239933583e-05,
      "loss": 0.6115,
      "step": 1460
    },
    {
      "epoch": 0.6102117061021171,
      "grad_norm": 2.7141988277435303,
      "learning_rate": 3.986439739864398e-05,
      "loss": 0.5846,
      "step": 1470
    },
    {
      "epoch": 0.614362806143628,
      "grad_norm": 3.8230459690093994,
      "learning_rate": 3.9795212397952126e-05,
      "loss": 0.4977,
      "step": 1480
    },
    {
      "epoch": 0.618513906185139,
      "grad_norm": 2.778031587600708,
      "learning_rate": 3.9726027397260274e-05,
      "loss": 0.5231,
      "step": 1490
    },
    {
      "epoch": 0.6226650062266501,
      "grad_norm": 6.485655784606934,
      "learning_rate": 3.965684239656843e-05,
      "loss": 0.6124,
      "step": 1500
    },
    {
      "epoch": 0.6268161062681611,
      "grad_norm": 2.7792491912841797,
      "learning_rate": 3.958765739587658e-05,
      "loss": 0.5245,
      "step": 1510
    },
    {
      "epoch": 0.6309672063096721,
      "grad_norm": 4.119534492492676,
      "learning_rate": 3.951847239518473e-05,
      "loss": 0.5727,
      "step": 1520
    },
    {
      "epoch": 0.635118306351183,
      "grad_norm": 4.6580305099487305,
      "learning_rate": 3.9449287394492876e-05,
      "loss": 0.5483,
      "step": 1530
    },
    {
      "epoch": 0.639269406392694,
      "grad_norm": 3.592411994934082,
      "learning_rate": 3.9380102393801024e-05,
      "loss": 0.5688,
      "step": 1540
    },
    {
      "epoch": 0.6434205064342051,
      "grad_norm": 4.661876201629639,
      "learning_rate": 3.931091739310917e-05,
      "loss": 0.5815,
      "step": 1550
    },
    {
      "epoch": 0.6475716064757161,
      "grad_norm": 3.276867628097534,
      "learning_rate": 3.924173239241732e-05,
      "loss": 0.534,
      "step": 1560
    },
    {
      "epoch": 0.6517227065172271,
      "grad_norm": 4.045099258422852,
      "learning_rate": 3.917254739172548e-05,
      "loss": 0.5771,
      "step": 1570
    },
    {
      "epoch": 0.6558738065587381,
      "grad_norm": 5.725621223449707,
      "learning_rate": 3.9103362391033626e-05,
      "loss": 0.5693,
      "step": 1580
    },
    {
      "epoch": 0.660024906600249,
      "grad_norm": 4.906647205352783,
      "learning_rate": 3.9034177390341775e-05,
      "loss": 0.5232,
      "step": 1590
    },
    {
      "epoch": 0.66417600664176,
      "grad_norm": 4.201799392700195,
      "learning_rate": 3.896499238964992e-05,
      "loss": 0.5031,
      "step": 1600
    },
    {
      "epoch": 0.6683271066832711,
      "grad_norm": 3.6639750003814697,
      "learning_rate": 3.889580738895808e-05,
      "loss": 0.5229,
      "step": 1610
    },
    {
      "epoch": 0.6724782067247821,
      "grad_norm": 3.770975112915039,
      "learning_rate": 3.882662238826623e-05,
      "loss": 0.6617,
      "step": 1620
    },
    {
      "epoch": 0.6766293067662931,
      "grad_norm": 3.1657261848449707,
      "learning_rate": 3.8757437387574376e-05,
      "loss": 0.5683,
      "step": 1630
    },
    {
      "epoch": 0.680780406807804,
      "grad_norm": 2.6498820781707764,
      "learning_rate": 3.8688252386882525e-05,
      "loss": 0.5697,
      "step": 1640
    },
    {
      "epoch": 0.684931506849315,
      "grad_norm": 4.305773735046387,
      "learning_rate": 3.861906738619068e-05,
      "loss": 0.5939,
      "step": 1650
    },
    {
      "epoch": 0.6890826068908261,
      "grad_norm": 3.7766902446746826,
      "learning_rate": 3.854988238549883e-05,
      "loss": 0.5642,
      "step": 1660
    },
    {
      "epoch": 0.6932337069323371,
      "grad_norm": 3.049074172973633,
      "learning_rate": 3.848069738480698e-05,
      "loss": 0.6214,
      "step": 1670
    },
    {
      "epoch": 0.6973848069738481,
      "grad_norm": 3.0247316360473633,
      "learning_rate": 3.8411512384115126e-05,
      "loss": 0.6367,
      "step": 1680
    },
    {
      "epoch": 0.7015359070153591,
      "grad_norm": 3.8550875186920166,
      "learning_rate": 3.8342327383423275e-05,
      "loss": 0.5248,
      "step": 1690
    },
    {
      "epoch": 0.70568700705687,
      "grad_norm": 2.5520851612091064,
      "learning_rate": 3.827314238273142e-05,
      "loss": 0.4606,
      "step": 1700
    },
    {
      "epoch": 0.709838107098381,
      "grad_norm": 3.9750661849975586,
      "learning_rate": 3.820395738203957e-05,
      "loss": 0.6054,
      "step": 1710
    },
    {
      "epoch": 0.7139892071398921,
      "grad_norm": 3.8585760593414307,
      "learning_rate": 3.813477238134773e-05,
      "loss": 0.5427,
      "step": 1720
    },
    {
      "epoch": 0.7181403071814031,
      "grad_norm": 6.423068046569824,
      "learning_rate": 3.8065587380655876e-05,
      "loss": 0.5669,
      "step": 1730
    },
    {
      "epoch": 0.7222914072229141,
      "grad_norm": 3.4498531818389893,
      "learning_rate": 3.7996402379964025e-05,
      "loss": 0.6031,
      "step": 1740
    },
    {
      "epoch": 0.726442507264425,
      "grad_norm": 5.3525614738464355,
      "learning_rate": 3.792721737927217e-05,
      "loss": 0.5492,
      "step": 1750
    },
    {
      "epoch": 0.730593607305936,
      "grad_norm": 4.396335601806641,
      "learning_rate": 3.785803237858033e-05,
      "loss": 0.5912,
      "step": 1760
    },
    {
      "epoch": 0.7347447073474471,
      "grad_norm": 4.381235599517822,
      "learning_rate": 3.778884737788848e-05,
      "loss": 0.4955,
      "step": 1770
    },
    {
      "epoch": 0.7388958073889581,
      "grad_norm": 3.1759283542633057,
      "learning_rate": 3.7719662377196626e-05,
      "loss": 0.5498,
      "step": 1780
    },
    {
      "epoch": 0.7430469074304691,
      "grad_norm": 4.651916027069092,
      "learning_rate": 3.7650477376504775e-05,
      "loss": 0.565,
      "step": 1790
    },
    {
      "epoch": 0.7471980074719801,
      "grad_norm": 4.621772766113281,
      "learning_rate": 3.7581292375812923e-05,
      "loss": 0.5565,
      "step": 1800
    },
    {
      "epoch": 0.751349107513491,
      "grad_norm": 6.332614898681641,
      "learning_rate": 3.751210737512107e-05,
      "loss": 0.5819,
      "step": 1810
    },
    {
      "epoch": 0.755500207555002,
      "grad_norm": 2.5502419471740723,
      "learning_rate": 3.744292237442922e-05,
      "loss": 0.6675,
      "step": 1820
    },
    {
      "epoch": 0.7596513075965131,
      "grad_norm": 2.797708749771118,
      "learning_rate": 3.7373737373737376e-05,
      "loss": 0.4733,
      "step": 1830
    },
    {
      "epoch": 0.7638024076380241,
      "grad_norm": 3.719714879989624,
      "learning_rate": 3.7304552373045525e-05,
      "loss": 0.5567,
      "step": 1840
    },
    {
      "epoch": 0.7679535076795351,
      "grad_norm": 3.7618963718414307,
      "learning_rate": 3.7235367372353673e-05,
      "loss": 0.5111,
      "step": 1850
    },
    {
      "epoch": 0.772104607721046,
      "grad_norm": 3.2469334602355957,
      "learning_rate": 3.716618237166182e-05,
      "loss": 0.5171,
      "step": 1860
    },
    {
      "epoch": 0.776255707762557,
      "grad_norm": 3.2936851978302,
      "learning_rate": 3.709699737096998e-05,
      "loss": 0.5467,
      "step": 1870
    },
    {
      "epoch": 0.7804068078040681,
      "grad_norm": 3.216890335083008,
      "learning_rate": 3.7027812370278126e-05,
      "loss": 0.5156,
      "step": 1880
    },
    {
      "epoch": 0.7845579078455791,
      "grad_norm": 4.568055629730225,
      "learning_rate": 3.6958627369586275e-05,
      "loss": 0.5537,
      "step": 1890
    },
    {
      "epoch": 0.7887090078870901,
      "grad_norm": 2.3792173862457275,
      "learning_rate": 3.688944236889443e-05,
      "loss": 0.5039,
      "step": 1900
    },
    {
      "epoch": 0.7928601079286011,
      "grad_norm": 2.519968032836914,
      "learning_rate": 3.682025736820258e-05,
      "loss": 0.5656,
      "step": 1910
    },
    {
      "epoch": 0.797011207970112,
      "grad_norm": 3.2420809268951416,
      "learning_rate": 3.675107236751073e-05,
      "loss": 0.5904,
      "step": 1920
    },
    {
      "epoch": 0.801162308011623,
      "grad_norm": 4.1765546798706055,
      "learning_rate": 3.6681887366818876e-05,
      "loss": 0.5191,
      "step": 1930
    },
    {
      "epoch": 0.8053134080531341,
      "grad_norm": 4.531266689300537,
      "learning_rate": 3.6612702366127025e-05,
      "loss": 0.4903,
      "step": 1940
    },
    {
      "epoch": 0.8094645080946451,
      "grad_norm": 5.840338706970215,
      "learning_rate": 3.6543517365435174e-05,
      "loss": 0.5772,
      "step": 1950
    },
    {
      "epoch": 0.8136156081361561,
      "grad_norm": 3.1464059352874756,
      "learning_rate": 3.647433236474332e-05,
      "loss": 0.5316,
      "step": 1960
    },
    {
      "epoch": 0.8177667081776671,
      "grad_norm": 3.299571990966797,
      "learning_rate": 3.640514736405147e-05,
      "loss": 0.5494,
      "step": 1970
    },
    {
      "epoch": 0.821917808219178,
      "grad_norm": 3.856428623199463,
      "learning_rate": 3.6335962363359626e-05,
      "loss": 0.5421,
      "step": 1980
    },
    {
      "epoch": 0.8260689082606891,
      "grad_norm": 3.9884438514709473,
      "learning_rate": 3.6266777362667775e-05,
      "loss": 0.4803,
      "step": 1990
    },
    {
      "epoch": 0.8302200083022001,
      "grad_norm": 3.518778085708618,
      "learning_rate": 3.6197592361975924e-05,
      "loss": 0.547,
      "step": 2000
    },
    {
      "epoch": 0.8343711083437111,
      "grad_norm": 4.280359268188477,
      "learning_rate": 3.612840736128408e-05,
      "loss": 0.5567,
      "step": 2010
    },
    {
      "epoch": 0.8385222083852221,
      "grad_norm": 3.213529348373413,
      "learning_rate": 3.605922236059223e-05,
      "loss": 0.5665,
      "step": 2020
    },
    {
      "epoch": 0.842673308426733,
      "grad_norm": 2.712693929672241,
      "learning_rate": 3.5990037359900376e-05,
      "loss": 0.5842,
      "step": 2030
    },
    {
      "epoch": 0.8468244084682441,
      "grad_norm": 4.216360092163086,
      "learning_rate": 3.5920852359208525e-05,
      "loss": 0.6182,
      "step": 2040
    },
    {
      "epoch": 0.8509755085097551,
      "grad_norm": 2.9933273792266846,
      "learning_rate": 3.585166735851668e-05,
      "loss": 0.4744,
      "step": 2050
    },
    {
      "epoch": 0.8551266085512661,
      "grad_norm": 3.5488507747650146,
      "learning_rate": 3.578248235782483e-05,
      "loss": 0.5429,
      "step": 2060
    },
    {
      "epoch": 0.8592777085927771,
      "grad_norm": 3.691793203353882,
      "learning_rate": 3.571329735713297e-05,
      "loss": 0.5185,
      "step": 2070
    },
    {
      "epoch": 0.8634288086342881,
      "grad_norm": 2.6514699459075928,
      "learning_rate": 3.564411235644112e-05,
      "loss": 0.6152,
      "step": 2080
    },
    {
      "epoch": 0.867579908675799,
      "grad_norm": 2.8602375984191895,
      "learning_rate": 3.5574927355749275e-05,
      "loss": 0.5007,
      "step": 2090
    },
    {
      "epoch": 0.8717310087173101,
      "grad_norm": 4.239212512969971,
      "learning_rate": 3.5505742355057424e-05,
      "loss": 0.6189,
      "step": 2100
    },
    {
      "epoch": 0.8758821087588211,
      "grad_norm": 3.1877667903900146,
      "learning_rate": 3.543655735436557e-05,
      "loss": 0.5713,
      "step": 2110
    },
    {
      "epoch": 0.8800332088003321,
      "grad_norm": 4.832635402679443,
      "learning_rate": 3.536737235367373e-05,
      "loss": 0.5141,
      "step": 2120
    },
    {
      "epoch": 0.8841843088418431,
      "grad_norm": 3.5700490474700928,
      "learning_rate": 3.5298187352981876e-05,
      "loss": 0.5148,
      "step": 2130
    },
    {
      "epoch": 0.888335408883354,
      "grad_norm": 3.058683156967163,
      "learning_rate": 3.5229002352290025e-05,
      "loss": 0.5014,
      "step": 2140
    },
    {
      "epoch": 0.8924865089248651,
      "grad_norm": 7.682463645935059,
      "learning_rate": 3.5159817351598174e-05,
      "loss": 0.5827,
      "step": 2150
    },
    {
      "epoch": 0.8966376089663761,
      "grad_norm": 5.000410079956055,
      "learning_rate": 3.509063235090633e-05,
      "loss": 0.55,
      "step": 2160
    },
    {
      "epoch": 0.9007887090078871,
      "grad_norm": 2.5824763774871826,
      "learning_rate": 3.502144735021448e-05,
      "loss": 0.5482,
      "step": 2170
    },
    {
      "epoch": 0.9049398090493981,
      "grad_norm": 3.9778177738189697,
      "learning_rate": 3.4952262349522627e-05,
      "loss": 0.5441,
      "step": 2180
    },
    {
      "epoch": 0.9090909090909091,
      "grad_norm": 3.404181480407715,
      "learning_rate": 3.4883077348830775e-05,
      "loss": 0.5046,
      "step": 2190
    },
    {
      "epoch": 0.91324200913242,
      "grad_norm": 2.8496434688568115,
      "learning_rate": 3.4813892348138924e-05,
      "loss": 0.4673,
      "step": 2200
    },
    {
      "epoch": 0.9173931091739311,
      "grad_norm": 3.077585458755493,
      "learning_rate": 3.474470734744707e-05,
      "loss": 0.5104,
      "step": 2210
    },
    {
      "epoch": 0.9215442092154421,
      "grad_norm": 3.2280683517456055,
      "learning_rate": 3.467552234675522e-05,
      "loss": 0.5147,
      "step": 2220
    },
    {
      "epoch": 0.9256953092569531,
      "grad_norm": 2.244978904724121,
      "learning_rate": 3.460633734606338e-05,
      "loss": 0.5124,
      "step": 2230
    },
    {
      "epoch": 0.9298464092984641,
      "grad_norm": 3.613041639328003,
      "learning_rate": 3.4537152345371525e-05,
      "loss": 0.5542,
      "step": 2240
    },
    {
      "epoch": 0.933997509339975,
      "grad_norm": 5.867563724517822,
      "learning_rate": 3.4467967344679674e-05,
      "loss": 0.5562,
      "step": 2250
    },
    {
      "epoch": 0.9381486093814861,
      "grad_norm": 2.5372133255004883,
      "learning_rate": 3.439878234398782e-05,
      "loss": 0.5013,
      "step": 2260
    },
    {
      "epoch": 0.9422997094229971,
      "grad_norm": 2.4842917919158936,
      "learning_rate": 3.432959734329598e-05,
      "loss": 0.4733,
      "step": 2270
    },
    {
      "epoch": 0.9464508094645081,
      "grad_norm": 3.036280393600464,
      "learning_rate": 3.426041234260413e-05,
      "loss": 0.552,
      "step": 2280
    },
    {
      "epoch": 0.9506019095060191,
      "grad_norm": 3.1855993270874023,
      "learning_rate": 3.4191227341912275e-05,
      "loss": 0.5105,
      "step": 2290
    },
    {
      "epoch": 0.9547530095475301,
      "grad_norm": 3.0526599884033203,
      "learning_rate": 3.4122042341220424e-05,
      "loss": 0.5611,
      "step": 2300
    },
    {
      "epoch": 0.958904109589041,
      "grad_norm": 2.126492500305176,
      "learning_rate": 3.405285734052858e-05,
      "loss": 0.537,
      "step": 2310
    },
    {
      "epoch": 0.9630552096305521,
      "grad_norm": 3.7778708934783936,
      "learning_rate": 3.398367233983673e-05,
      "loss": 0.4898,
      "step": 2320
    },
    {
      "epoch": 0.9672063096720631,
      "grad_norm": 2.8898062705993652,
      "learning_rate": 3.391448733914488e-05,
      "loss": 0.4784,
      "step": 2330
    },
    {
      "epoch": 0.9713574097135741,
      "grad_norm": 2.3593060970306396,
      "learning_rate": 3.3845302338453025e-05,
      "loss": 0.4986,
      "step": 2340
    },
    {
      "epoch": 0.9755085097550851,
      "grad_norm": 3.8027396202087402,
      "learning_rate": 3.3776117337761174e-05,
      "loss": 0.5604,
      "step": 2350
    },
    {
      "epoch": 0.979659609796596,
      "grad_norm": 2.623788356781006,
      "learning_rate": 3.370693233706932e-05,
      "loss": 0.4588,
      "step": 2360
    },
    {
      "epoch": 0.9838107098381071,
      "grad_norm": 2.903848648071289,
      "learning_rate": 3.363774733637747e-05,
      "loss": 0.4622,
      "step": 2370
    },
    {
      "epoch": 0.9879618098796181,
      "grad_norm": 2.6867644786834717,
      "learning_rate": 3.356856233568563e-05,
      "loss": 0.535,
      "step": 2380
    },
    {
      "epoch": 0.9921129099211291,
      "grad_norm": 3.5993893146514893,
      "learning_rate": 3.3499377334993775e-05,
      "loss": 0.4805,
      "step": 2390
    },
    {
      "epoch": 0.9962640099626401,
      "grad_norm": 3.97347092628479,
      "learning_rate": 3.3430192334301924e-05,
      "loss": 0.5754,
      "step": 2400
    },
    {
      "epoch": 1.0004151100041512,
      "grad_norm": 4.304186820983887,
      "learning_rate": 3.336100733361007e-05,
      "loss": 0.4976,
      "step": 2410
    },
    {
      "epoch": 1.004566210045662,
      "grad_norm": 5.591305732727051,
      "learning_rate": 3.329182233291823e-05,
      "loss": 0.5514,
      "step": 2420
    },
    {
      "epoch": 1.0087173100871731,
      "grad_norm": 2.7831006050109863,
      "learning_rate": 3.322263733222638e-05,
      "loss": 0.553,
      "step": 2430
    },
    {
      "epoch": 1.012868410128684,
      "grad_norm": 2.9220006465911865,
      "learning_rate": 3.3153452331534526e-05,
      "loss": 0.507,
      "step": 2440
    },
    {
      "epoch": 1.017019510170195,
      "grad_norm": 3.963641881942749,
      "learning_rate": 3.308426733084268e-05,
      "loss": 0.5579,
      "step": 2450
    },
    {
      "epoch": 1.0211706102117062,
      "grad_norm": 2.416908025741577,
      "learning_rate": 3.301508233015082e-05,
      "loss": 0.5464,
      "step": 2460
    },
    {
      "epoch": 1.025321710253217,
      "grad_norm": 2.2209174633026123,
      "learning_rate": 3.294589732945897e-05,
      "loss": 0.5997,
      "step": 2470
    },
    {
      "epoch": 1.0294728102947281,
      "grad_norm": 2.6523501873016357,
      "learning_rate": 3.287671232876712e-05,
      "loss": 0.4827,
      "step": 2480
    },
    {
      "epoch": 1.033623910336239,
      "grad_norm": 4.021456241607666,
      "learning_rate": 3.2807527328075276e-05,
      "loss": 0.4872,
      "step": 2490
    },
    {
      "epoch": 1.03777501037775,
      "grad_norm": 2.5249481201171875,
      "learning_rate": 3.2738342327383424e-05,
      "loss": 0.5165,
      "step": 2500
    },
    {
      "epoch": 1.0419261104192612,
      "grad_norm": 2.8554301261901855,
      "learning_rate": 3.266915732669157e-05,
      "loss": 0.4426,
      "step": 2510
    },
    {
      "epoch": 1.046077210460772,
      "grad_norm": 3.446474552154541,
      "learning_rate": 3.259997232599972e-05,
      "loss": 0.5261,
      "step": 2520
    },
    {
      "epoch": 1.0502283105022832,
      "grad_norm": 6.63249397277832,
      "learning_rate": 3.253078732530788e-05,
      "loss": 0.516,
      "step": 2530
    },
    {
      "epoch": 1.054379410543794,
      "grad_norm": 5.043332576751709,
      "learning_rate": 3.2461602324616026e-05,
      "loss": 0.5094,
      "step": 2540
    },
    {
      "epoch": 1.0585305105853051,
      "grad_norm": 4.553330898284912,
      "learning_rate": 3.2392417323924174e-05,
      "loss": 0.4113,
      "step": 2550
    },
    {
      "epoch": 1.0626816106268162,
      "grad_norm": 2.109650135040283,
      "learning_rate": 3.232323232323233e-05,
      "loss": 0.4867,
      "step": 2560
    },
    {
      "epoch": 1.066832710668327,
      "grad_norm": 2.617969274520874,
      "learning_rate": 3.225404732254048e-05,
      "loss": 0.4893,
      "step": 2570
    },
    {
      "epoch": 1.0709838107098382,
      "grad_norm": 3.71372652053833,
      "learning_rate": 3.218486232184863e-05,
      "loss": 0.5111,
      "step": 2580
    },
    {
      "epoch": 1.075134910751349,
      "grad_norm": 4.290191173553467,
      "learning_rate": 3.2115677321156776e-05,
      "loss": 0.499,
      "step": 2590
    },
    {
      "epoch": 1.0792860107928601,
      "grad_norm": 4.875636100769043,
      "learning_rate": 3.2046492320464924e-05,
      "loss": 0.6178,
      "step": 2600
    },
    {
      "epoch": 1.083437110834371,
      "grad_norm": 3.876492500305176,
      "learning_rate": 3.197730731977307e-05,
      "loss": 0.618,
      "step": 2610
    },
    {
      "epoch": 1.087588210875882,
      "grad_norm": 4.715938091278076,
      "learning_rate": 3.190812231908122e-05,
      "loss": 0.5847,
      "step": 2620
    },
    {
      "epoch": 1.0917393109173932,
      "grad_norm": 3.2122962474823,
      "learning_rate": 3.183893731838937e-05,
      "loss": 0.5768,
      "step": 2630
    },
    {
      "epoch": 1.095890410958904,
      "grad_norm": 3.346376657485962,
      "learning_rate": 3.1769752317697526e-05,
      "loss": 0.5175,
      "step": 2640
    },
    {
      "epoch": 1.1000415110004151,
      "grad_norm": 3.489844560623169,
      "learning_rate": 3.1700567317005674e-05,
      "loss": 0.4656,
      "step": 2650
    },
    {
      "epoch": 1.104192611041926,
      "grad_norm": 5.142818450927734,
      "learning_rate": 3.163138231631382e-05,
      "loss": 0.5063,
      "step": 2660
    },
    {
      "epoch": 1.108343711083437,
      "grad_norm": 5.815212249755859,
      "learning_rate": 3.156219731562197e-05,
      "loss": 0.5305,
      "step": 2670
    },
    {
      "epoch": 1.1124948111249482,
      "grad_norm": 2.125457525253296,
      "learning_rate": 3.149301231493013e-05,
      "loss": 0.4499,
      "step": 2680
    },
    {
      "epoch": 1.116645911166459,
      "grad_norm": 3.6747913360595703,
      "learning_rate": 3.1423827314238276e-05,
      "loss": 0.509,
      "step": 2690
    },
    {
      "epoch": 1.1207970112079702,
      "grad_norm": 2.598531723022461,
      "learning_rate": 3.1354642313546424e-05,
      "loss": 0.5454,
      "step": 2700
    },
    {
      "epoch": 1.124948111249481,
      "grad_norm": 3.8159067630767822,
      "learning_rate": 3.128545731285458e-05,
      "loss": 0.5135,
      "step": 2710
    },
    {
      "epoch": 1.129099211290992,
      "grad_norm": 3.280277729034424,
      "learning_rate": 3.121627231216273e-05,
      "loss": 0.4693,
      "step": 2720
    },
    {
      "epoch": 1.1332503113325032,
      "grad_norm": 2.37439227104187,
      "learning_rate": 3.114708731147088e-05,
      "loss": 0.49,
      "step": 2730
    },
    {
      "epoch": 1.137401411374014,
      "grad_norm": 8.941268920898438,
      "learning_rate": 3.107790231077902e-05,
      "loss": 0.5478,
      "step": 2740
    },
    {
      "epoch": 1.1415525114155252,
      "grad_norm": 3.172161102294922,
      "learning_rate": 3.1008717310087175e-05,
      "loss": 0.465,
      "step": 2750
    },
    {
      "epoch": 1.145703611457036,
      "grad_norm": 4.372394561767578,
      "learning_rate": 3.093953230939532e-05,
      "loss": 0.5254,
      "step": 2760
    },
    {
      "epoch": 1.1498547114985471,
      "grad_norm": 4.51657247543335,
      "learning_rate": 3.087034730870347e-05,
      "loss": 0.5225,
      "step": 2770
    },
    {
      "epoch": 1.154005811540058,
      "grad_norm": 3.657958745956421,
      "learning_rate": 3.080116230801162e-05,
      "loss": 0.4845,
      "step": 2780
    },
    {
      "epoch": 1.158156911581569,
      "grad_norm": 2.0525758266448975,
      "learning_rate": 3.0731977307319776e-05,
      "loss": 0.5328,
      "step": 2790
    },
    {
      "epoch": 1.1623080116230802,
      "grad_norm": 3.6827893257141113,
      "learning_rate": 3.0662792306627925e-05,
      "loss": 0.4176,
      "step": 2800
    },
    {
      "epoch": 1.166459111664591,
      "grad_norm": 2.6164309978485107,
      "learning_rate": 3.059360730593607e-05,
      "loss": 0.465,
      "step": 2810
    },
    {
      "epoch": 1.1706102117061021,
      "grad_norm": 2.8729286193847656,
      "learning_rate": 3.052442230524423e-05,
      "loss": 0.4254,
      "step": 2820
    },
    {
      "epoch": 1.1747613117476132,
      "grad_norm": 3.655730724334717,
      "learning_rate": 3.0455237304552374e-05,
      "loss": 0.5126,
      "step": 2830
    },
    {
      "epoch": 1.178912411789124,
      "grad_norm": 2.782867431640625,
      "learning_rate": 3.0386052303860523e-05,
      "loss": 0.5223,
      "step": 2840
    },
    {
      "epoch": 1.1830635118306352,
      "grad_norm": 3.3053581714630127,
      "learning_rate": 3.031686730316867e-05,
      "loss": 0.578,
      "step": 2850
    },
    {
      "epoch": 1.187214611872146,
      "grad_norm": 2.4032413959503174,
      "learning_rate": 3.0247682302476827e-05,
      "loss": 0.4353,
      "step": 2860
    },
    {
      "epoch": 1.1913657119136571,
      "grad_norm": 6.448305606842041,
      "learning_rate": 3.0178497301784975e-05,
      "loss": 0.5475,
      "step": 2870
    },
    {
      "epoch": 1.195516811955168,
      "grad_norm": 3.858548402786255,
      "learning_rate": 3.0109312301093124e-05,
      "loss": 0.5285,
      "step": 2880
    },
    {
      "epoch": 1.199667911996679,
      "grad_norm": 4.858962535858154,
      "learning_rate": 3.0040127300401273e-05,
      "loss": 0.4986,
      "step": 2890
    },
    {
      "epoch": 1.2038190120381902,
      "grad_norm": 2.360757350921631,
      "learning_rate": 2.9970942299709425e-05,
      "loss": 0.4883,
      "step": 2900
    },
    {
      "epoch": 1.207970112079701,
      "grad_norm": 3.652284622192383,
      "learning_rate": 2.9901757299017573e-05,
      "loss": 0.4959,
      "step": 2910
    },
    {
      "epoch": 1.2121212121212122,
      "grad_norm": 3.9303696155548096,
      "learning_rate": 2.9832572298325722e-05,
      "loss": 0.5115,
      "step": 2920
    },
    {
      "epoch": 1.2162723121627232,
      "grad_norm": 2.151797294616699,
      "learning_rate": 2.9763387297633877e-05,
      "loss": 0.4849,
      "step": 2930
    },
    {
      "epoch": 1.2204234122042341,
      "grad_norm": 3.5998244285583496,
      "learning_rate": 2.9694202296942026e-05,
      "loss": 0.4393,
      "step": 2940
    },
    {
      "epoch": 1.2245745122457452,
      "grad_norm": 3.647873878479004,
      "learning_rate": 2.9625017296250175e-05,
      "loss": 0.491,
      "step": 2950
    },
    {
      "epoch": 1.228725612287256,
      "grad_norm": 4.104638576507568,
      "learning_rate": 2.9555832295558323e-05,
      "loss": 0.5123,
      "step": 2960
    },
    {
      "epoch": 1.2328767123287672,
      "grad_norm": 3.8879618644714355,
      "learning_rate": 2.9486647294866475e-05,
      "loss": 0.5087,
      "step": 2970
    },
    {
      "epoch": 1.237027812370278,
      "grad_norm": 2.9498629570007324,
      "learning_rate": 2.9417462294174624e-05,
      "loss": 0.5211,
      "step": 2980
    },
    {
      "epoch": 1.2411789124117891,
      "grad_norm": 4.457188606262207,
      "learning_rate": 2.9348277293482773e-05,
      "loss": 0.4496,
      "step": 2990
    },
    {
      "epoch": 1.2453300124533002,
      "grad_norm": 3.371277332305908,
      "learning_rate": 2.927909229279092e-05,
      "loss": 0.5387,
      "step": 3000
    },
    {
      "epoch": 1.249481112494811,
      "grad_norm": 2.8865771293640137,
      "learning_rate": 2.9209907292099077e-05,
      "loss": 0.5465,
      "step": 3010
    },
    {
      "epoch": 1.2536322125363222,
      "grad_norm": 3.6447484493255615,
      "learning_rate": 2.9140722291407226e-05,
      "loss": 0.5061,
      "step": 3020
    },
    {
      "epoch": 1.257783312577833,
      "grad_norm": 4.657071113586426,
      "learning_rate": 2.9071537290715374e-05,
      "loss": 0.5667,
      "step": 3030
    },
    {
      "epoch": 1.2619344126193441,
      "grad_norm": 2.82145094871521,
      "learning_rate": 2.9002352290023526e-05,
      "loss": 0.5512,
      "step": 3040
    },
    {
      "epoch": 1.266085512660855,
      "grad_norm": 2.8189523220062256,
      "learning_rate": 2.8933167289331675e-05,
      "loss": 0.4674,
      "step": 3050
    },
    {
      "epoch": 1.270236612702366,
      "grad_norm": 2.9887049198150635,
      "learning_rate": 2.8863982288639824e-05,
      "loss": 0.4452,
      "step": 3060
    },
    {
      "epoch": 1.2743877127438772,
      "grad_norm": 6.290050983428955,
      "learning_rate": 2.8794797287947972e-05,
      "loss": 0.4939,
      "step": 3070
    },
    {
      "epoch": 1.278538812785388,
      "grad_norm": 2.6980485916137695,
      "learning_rate": 2.8725612287256128e-05,
      "loss": 0.5214,
      "step": 3080
    },
    {
      "epoch": 1.2826899128268991,
      "grad_norm": 3.136431932449341,
      "learning_rate": 2.8656427286564276e-05,
      "loss": 0.5495,
      "step": 3090
    },
    {
      "epoch": 1.2868410128684102,
      "grad_norm": 2.490834951400757,
      "learning_rate": 2.858724228587242e-05,
      "loss": 0.4893,
      "step": 3100
    },
    {
      "epoch": 1.290992112909921,
      "grad_norm": 4.337430000305176,
      "learning_rate": 2.851805728518057e-05,
      "loss": 0.5324,
      "step": 3110
    },
    {
      "epoch": 1.2951432129514322,
      "grad_norm": 3.1242997646331787,
      "learning_rate": 2.8448872284488726e-05,
      "loss": 0.4942,
      "step": 3120
    },
    {
      "epoch": 1.299294312992943,
      "grad_norm": 2.2274773120880127,
      "learning_rate": 2.8379687283796874e-05,
      "loss": 0.4384,
      "step": 3130
    },
    {
      "epoch": 1.3034454130344542,
      "grad_norm": 3.023829460144043,
      "learning_rate": 2.8310502283105023e-05,
      "loss": 0.4118,
      "step": 3140
    },
    {
      "epoch": 1.307596513075965,
      "grad_norm": 2.5447072982788086,
      "learning_rate": 2.8241317282413175e-05,
      "loss": 0.4482,
      "step": 3150
    },
    {
      "epoch": 1.3117476131174761,
      "grad_norm": 5.983236312866211,
      "learning_rate": 2.8172132281721324e-05,
      "loss": 0.5175,
      "step": 3160
    },
    {
      "epoch": 1.3158987131589872,
      "grad_norm": 6.880541801452637,
      "learning_rate": 2.8102947281029472e-05,
      "loss": 0.5724,
      "step": 3170
    },
    {
      "epoch": 1.320049813200498,
      "grad_norm": 2.925522804260254,
      "learning_rate": 2.803376228033762e-05,
      "loss": 0.4722,
      "step": 3180
    },
    {
      "epoch": 1.3242009132420092,
      "grad_norm": 6.545306205749512,
      "learning_rate": 2.7964577279645776e-05,
      "loss": 0.5242,
      "step": 3190
    },
    {
      "epoch": 1.3283520132835203,
      "grad_norm": 2.2871408462524414,
      "learning_rate": 2.7895392278953925e-05,
      "loss": 0.4411,
      "step": 3200
    },
    {
      "epoch": 1.3325031133250311,
      "grad_norm": 5.107059001922607,
      "learning_rate": 2.7826207278262074e-05,
      "loss": 0.5106,
      "step": 3210
    },
    {
      "epoch": 1.336654213366542,
      "grad_norm": 11.262338638305664,
      "learning_rate": 2.7757022277570222e-05,
      "loss": 0.3963,
      "step": 3220
    },
    {
      "epoch": 1.340805313408053,
      "grad_norm": 3.0895330905914307,
      "learning_rate": 2.7687837276878374e-05,
      "loss": 0.4716,
      "step": 3230
    },
    {
      "epoch": 1.3449564134495642,
      "grad_norm": 5.905062675476074,
      "learning_rate": 2.7618652276186523e-05,
      "loss": 0.5384,
      "step": 3240
    },
    {
      "epoch": 1.349107513491075,
      "grad_norm": 3.084061861038208,
      "learning_rate": 2.7549467275494672e-05,
      "loss": 0.4789,
      "step": 3250
    },
    {
      "epoch": 1.3532586135325861,
      "grad_norm": 3.5462963581085205,
      "learning_rate": 2.7480282274802827e-05,
      "loss": 0.5103,
      "step": 3260
    },
    {
      "epoch": 1.3574097135740972,
      "grad_norm": 4.329009532928467,
      "learning_rate": 2.7411097274110976e-05,
      "loss": 0.5753,
      "step": 3270
    },
    {
      "epoch": 1.361560813615608,
      "grad_norm": 5.1803717613220215,
      "learning_rate": 2.7341912273419124e-05,
      "loss": 0.4535,
      "step": 3280
    },
    {
      "epoch": 1.3657119136571192,
      "grad_norm": 3.8179423809051514,
      "learning_rate": 2.7272727272727273e-05,
      "loss": 0.49,
      "step": 3290
    },
    {
      "epoch": 1.36986301369863,
      "grad_norm": 3.47190523147583,
      "learning_rate": 2.7203542272035425e-05,
      "loss": 0.4914,
      "step": 3300
    },
    {
      "epoch": 1.3740141137401412,
      "grad_norm": 4.385995388031006,
      "learning_rate": 2.7134357271343574e-05,
      "loss": 0.4403,
      "step": 3310
    },
    {
      "epoch": 1.378165213781652,
      "grad_norm": 5.03425407409668,
      "learning_rate": 2.7065172270651722e-05,
      "loss": 0.4601,
      "step": 3320
    },
    {
      "epoch": 1.3823163138231631,
      "grad_norm": 4.051652908325195,
      "learning_rate": 2.699598726995987e-05,
      "loss": 0.5401,
      "step": 3330
    },
    {
      "epoch": 1.3864674138646742,
      "grad_norm": 3.8655834197998047,
      "learning_rate": 2.6926802269268027e-05,
      "loss": 0.4535,
      "step": 3340
    },
    {
      "epoch": 1.390618513906185,
      "grad_norm": 2.4460043907165527,
      "learning_rate": 2.6857617268576175e-05,
      "loss": 0.4916,
      "step": 3350
    },
    {
      "epoch": 1.3947696139476962,
      "grad_norm": 3.3797848224639893,
      "learning_rate": 2.6788432267884324e-05,
      "loss": 0.4906,
      "step": 3360
    },
    {
      "epoch": 1.3989207139892073,
      "grad_norm": 5.079919815063477,
      "learning_rate": 2.6719247267192476e-05,
      "loss": 0.484,
      "step": 3370
    },
    {
      "epoch": 1.4030718140307181,
      "grad_norm": 4.211511135101318,
      "learning_rate": 2.6650062266500625e-05,
      "loss": 0.4388,
      "step": 3380
    },
    {
      "epoch": 1.4072229140722292,
      "grad_norm": 4.602624893188477,
      "learning_rate": 2.6580877265808773e-05,
      "loss": 0.4765,
      "step": 3390
    },
    {
      "epoch": 1.41137401411374,
      "grad_norm": 4.662788391113281,
      "learning_rate": 2.6511692265116922e-05,
      "loss": 0.4623,
      "step": 3400
    },
    {
      "epoch": 1.4155251141552512,
      "grad_norm": 3.6016316413879395,
      "learning_rate": 2.6442507264425077e-05,
      "loss": 0.5004,
      "step": 3410
    },
    {
      "epoch": 1.419676214196762,
      "grad_norm": 2.7366676330566406,
      "learning_rate": 2.6373322263733226e-05,
      "loss": 0.4696,
      "step": 3420
    },
    {
      "epoch": 1.4238273142382731,
      "grad_norm": 5.770282745361328,
      "learning_rate": 2.6304137263041375e-05,
      "loss": 0.5219,
      "step": 3430
    },
    {
      "epoch": 1.4279784142797842,
      "grad_norm": 2.441094398498535,
      "learning_rate": 2.623495226234952e-05,
      "loss": 0.4402,
      "step": 3440
    },
    {
      "epoch": 1.432129514321295,
      "grad_norm": 2.657057285308838,
      "learning_rate": 2.6165767261657675e-05,
      "loss": 0.4641,
      "step": 3450
    },
    {
      "epoch": 1.4362806143628062,
      "grad_norm": 4.049190998077393,
      "learning_rate": 2.6096582260965824e-05,
      "loss": 0.4886,
      "step": 3460
    },
    {
      "epoch": 1.4404317144043173,
      "grad_norm": 2.494494915008545,
      "learning_rate": 2.6027397260273973e-05,
      "loss": 0.5744,
      "step": 3470
    },
    {
      "epoch": 1.4445828144458281,
      "grad_norm": 4.504703998565674,
      "learning_rate": 2.5958212259582128e-05,
      "loss": 0.4675,
      "step": 3480
    },
    {
      "epoch": 1.448733914487339,
      "grad_norm": 4.387242794036865,
      "learning_rate": 2.5889027258890273e-05,
      "loss": 0.5393,
      "step": 3490
    },
    {
      "epoch": 1.45288501452885,
      "grad_norm": 2.9786009788513184,
      "learning_rate": 2.5819842258198422e-05,
      "loss": 0.4241,
      "step": 3500
    },
    {
      "epoch": 1.4570361145703612,
      "grad_norm": 3.171816110610962,
      "learning_rate": 2.575065725750657e-05,
      "loss": 0.4881,
      "step": 3510
    },
    {
      "epoch": 1.461187214611872,
      "grad_norm": 4.072003364562988,
      "learning_rate": 2.5681472256814726e-05,
      "loss": 0.5372,
      "step": 3520
    },
    {
      "epoch": 1.4653383146533832,
      "grad_norm": 4.1055684089660645,
      "learning_rate": 2.5612287256122875e-05,
      "loss": 0.4388,
      "step": 3530
    },
    {
      "epoch": 1.4694894146948942,
      "grad_norm": 3.635331630706787,
      "learning_rate": 2.5543102255431023e-05,
      "loss": 0.5309,
      "step": 3540
    },
    {
      "epoch": 1.4736405147364051,
      "grad_norm": 5.258504867553711,
      "learning_rate": 2.5473917254739172e-05,
      "loss": 0.5544,
      "step": 3550
    },
    {
      "epoch": 1.4777916147779162,
      "grad_norm": 2.7480783462524414,
      "learning_rate": 2.5404732254047324e-05,
      "loss": 0.5588,
      "step": 3560
    },
    {
      "epoch": 1.481942714819427,
      "grad_norm": 3.8335678577423096,
      "learning_rate": 2.5335547253355473e-05,
      "loss": 0.5115,
      "step": 3570
    },
    {
      "epoch": 1.4860938148609382,
      "grad_norm": 5.351369380950928,
      "learning_rate": 2.526636225266362e-05,
      "loss": 0.5363,
      "step": 3580
    },
    {
      "epoch": 1.490244914902449,
      "grad_norm": 2.976675033569336,
      "learning_rate": 2.5197177251971777e-05,
      "loss": 0.4548,
      "step": 3590
    },
    {
      "epoch": 1.4943960149439601,
      "grad_norm": 2.504666566848755,
      "learning_rate": 2.5127992251279926e-05,
      "loss": 0.4361,
      "step": 3600
    },
    {
      "epoch": 1.4985471149854712,
      "grad_norm": 3.6445953845977783,
      "learning_rate": 2.5058807250588074e-05,
      "loss": 0.5417,
      "step": 3610
    },
    {
      "epoch": 1.502698215026982,
      "grad_norm": 3.2326903343200684,
      "learning_rate": 2.4989622249896226e-05,
      "loss": 0.5275,
      "step": 3620
    },
    {
      "epoch": 1.5068493150684932,
      "grad_norm": 2.323982000350952,
      "learning_rate": 2.492043724920437e-05,
      "loss": 0.4855,
      "step": 3630
    },
    {
      "epoch": 1.5110004151100043,
      "grad_norm": 3.308077573776245,
      "learning_rate": 2.4851252248512524e-05,
      "loss": 0.5015,
      "step": 3640
    },
    {
      "epoch": 1.5151515151515151,
      "grad_norm": 4.399362564086914,
      "learning_rate": 2.4782067247820672e-05,
      "loss": 0.4341,
      "step": 3650
    },
    {
      "epoch": 1.519302615193026,
      "grad_norm": 3.3171310424804688,
      "learning_rate": 2.4712882247128824e-05,
      "loss": 0.4147,
      "step": 3660
    },
    {
      "epoch": 1.523453715234537,
      "grad_norm": 3.6509106159210205,
      "learning_rate": 2.4643697246436973e-05,
      "loss": 0.485,
      "step": 3670
    },
    {
      "epoch": 1.5276048152760482,
      "grad_norm": 2.0368564128875732,
      "learning_rate": 2.4574512245745125e-05,
      "loss": 0.4992,
      "step": 3680
    },
    {
      "epoch": 1.531755915317559,
      "grad_norm": 7.529146671295166,
      "learning_rate": 2.4505327245053274e-05,
      "loss": 0.5345,
      "step": 3690
    },
    {
      "epoch": 1.5359070153590701,
      "grad_norm": 2.497404098510742,
      "learning_rate": 2.4436142244361422e-05,
      "loss": 0.4331,
      "step": 3700
    },
    {
      "epoch": 1.5400581154005812,
      "grad_norm": 4.1800432205200195,
      "learning_rate": 2.4366957243669574e-05,
      "loss": 0.4975,
      "step": 3710
    },
    {
      "epoch": 1.544209215442092,
      "grad_norm": 2.9349629878997803,
      "learning_rate": 2.4297772242977723e-05,
      "loss": 0.5045,
      "step": 3720
    },
    {
      "epoch": 1.5483603154836032,
      "grad_norm": 3.6447126865386963,
      "learning_rate": 2.4228587242285875e-05,
      "loss": 0.4663,
      "step": 3730
    },
    {
      "epoch": 1.5525114155251143,
      "grad_norm": 2.678473711013794,
      "learning_rate": 2.4159402241594024e-05,
      "loss": 0.5257,
      "step": 3740
    },
    {
      "epoch": 1.5566625155666252,
      "grad_norm": 2.372154712677002,
      "learning_rate": 2.4090217240902176e-05,
      "loss": 0.4602,
      "step": 3750
    },
    {
      "epoch": 1.560813615608136,
      "grad_norm": 2.5584843158721924,
      "learning_rate": 2.4021032240210324e-05,
      "loss": 0.4669,
      "step": 3760
    },
    {
      "epoch": 1.5649647156496471,
      "grad_norm": 2.9351401329040527,
      "learning_rate": 2.3951847239518473e-05,
      "loss": 0.5064,
      "step": 3770
    },
    {
      "epoch": 1.5691158156911582,
      "grad_norm": 2.7761528491973877,
      "learning_rate": 2.388266223882662e-05,
      "loss": 0.4317,
      "step": 3780
    },
    {
      "epoch": 1.573266915732669,
      "grad_norm": 3.70578932762146,
      "learning_rate": 2.3813477238134774e-05,
      "loss": 0.4548,
      "step": 3790
    },
    {
      "epoch": 1.5774180157741802,
      "grad_norm": 3.023688316345215,
      "learning_rate": 2.3744292237442922e-05,
      "loss": 0.4668,
      "step": 3800
    },
    {
      "epoch": 1.5815691158156913,
      "grad_norm": 3.631138801574707,
      "learning_rate": 2.3675107236751074e-05,
      "loss": 0.5686,
      "step": 3810
    },
    {
      "epoch": 1.5857202158572021,
      "grad_norm": 2.551762104034424,
      "learning_rate": 2.3605922236059223e-05,
      "loss": 0.4466,
      "step": 3820
    },
    {
      "epoch": 1.589871315898713,
      "grad_norm": 3.0315229892730713,
      "learning_rate": 2.3536737235367372e-05,
      "loss": 0.4764,
      "step": 3830
    },
    {
      "epoch": 1.5940224159402243,
      "grad_norm": 2.955089807510376,
      "learning_rate": 2.3467552234675524e-05,
      "loss": 0.5141,
      "step": 3840
    },
    {
      "epoch": 1.5981735159817352,
      "grad_norm": 2.38803768157959,
      "learning_rate": 2.3398367233983672e-05,
      "loss": 0.4553,
      "step": 3850
    },
    {
      "epoch": 1.602324616023246,
      "grad_norm": 2.01351261138916,
      "learning_rate": 2.3329182233291824e-05,
      "loss": 0.5,
      "step": 3860
    },
    {
      "epoch": 1.6064757160647571,
      "grad_norm": 2.6418347358703613,
      "learning_rate": 2.3259997232599973e-05,
      "loss": 0.4753,
      "step": 3870
    },
    {
      "epoch": 1.6106268161062682,
      "grad_norm": 4.747217178344727,
      "learning_rate": 2.3190812231908125e-05,
      "loss": 0.5402,
      "step": 3880
    },
    {
      "epoch": 1.614777916147779,
      "grad_norm": 3.0014986991882324,
      "learning_rate": 2.3121627231216274e-05,
      "loss": 0.477,
      "step": 3890
    },
    {
      "epoch": 1.6189290161892902,
      "grad_norm": 2.975444793701172,
      "learning_rate": 2.3052442230524422e-05,
      "loss": 0.4378,
      "step": 3900
    },
    {
      "epoch": 1.6230801162308013,
      "grad_norm": 3.7212331295013428,
      "learning_rate": 2.298325722983257e-05,
      "loss": 0.4294,
      "step": 3910
    },
    {
      "epoch": 1.6272312162723122,
      "grad_norm": 3.0202884674072266,
      "learning_rate": 2.2914072229140723e-05,
      "loss": 0.5074,
      "step": 3920
    },
    {
      "epoch": 1.631382316313823,
      "grad_norm": 3.3361990451812744,
      "learning_rate": 2.2844887228448875e-05,
      "loss": 0.4742,
      "step": 3930
    },
    {
      "epoch": 1.6355334163553341,
      "grad_norm": 3.974116802215576,
      "learning_rate": 2.2775702227757024e-05,
      "loss": 0.5264,
      "step": 3940
    },
    {
      "epoch": 1.6396845163968452,
      "grad_norm": 2.6266732215881348,
      "learning_rate": 2.2706517227065176e-05,
      "loss": 0.5348,
      "step": 3950
    },
    {
      "epoch": 1.643835616438356,
      "grad_norm": 2.8658087253570557,
      "learning_rate": 2.263733222637332e-05,
      "loss": 0.4524,
      "step": 3960
    },
    {
      "epoch": 1.6479867164798672,
      "grad_norm": 3.4651336669921875,
      "learning_rate": 2.2568147225681473e-05,
      "loss": 0.4244,
      "step": 3970
    },
    {
      "epoch": 1.6521378165213783,
      "grad_norm": 3.443150758743286,
      "learning_rate": 2.2498962224989622e-05,
      "loss": 0.5137,
      "step": 3980
    },
    {
      "epoch": 1.6562889165628891,
      "grad_norm": 1.7506388425827026,
      "learning_rate": 2.2429777224297774e-05,
      "loss": 0.4277,
      "step": 3990
    },
    {
      "epoch": 1.6604400166044002,
      "grad_norm": 4.870709419250488,
      "learning_rate": 2.2360592223605923e-05,
      "loss": 0.476,
      "step": 4000
    },
    {
      "epoch": 1.6645911166459113,
      "grad_norm": 2.2128655910491943,
      "learning_rate": 2.2291407222914075e-05,
      "loss": 0.4786,
      "step": 4010
    },
    {
      "epoch": 1.6687422166874222,
      "grad_norm": 2.4165539741516113,
      "learning_rate": 2.2222222222222223e-05,
      "loss": 0.4916,
      "step": 4020
    },
    {
      "epoch": 1.672893316728933,
      "grad_norm": 2.468628406524658,
      "learning_rate": 2.2153037221530372e-05,
      "loss": 0.5276,
      "step": 4030
    },
    {
      "epoch": 1.6770444167704441,
      "grad_norm": 5.909372806549072,
      "learning_rate": 2.2083852220838524e-05,
      "loss": 0.4658,
      "step": 4040
    },
    {
      "epoch": 1.6811955168119552,
      "grad_norm": 3.0192410945892334,
      "learning_rate": 2.2014667220146673e-05,
      "loss": 0.4778,
      "step": 4050
    },
    {
      "epoch": 1.685346616853466,
      "grad_norm": 2.403557300567627,
      "learning_rate": 2.1945482219454825e-05,
      "loss": 0.5164,
      "step": 4060
    },
    {
      "epoch": 1.6894977168949772,
      "grad_norm": 3.0352718830108643,
      "learning_rate": 2.1876297218762973e-05,
      "loss": 0.481,
      "step": 4070
    },
    {
      "epoch": 1.6936488169364883,
      "grad_norm": 4.090526580810547,
      "learning_rate": 2.1807112218071125e-05,
      "loss": 0.4728,
      "step": 4080
    },
    {
      "epoch": 1.6977999169779991,
      "grad_norm": 2.145803689956665,
      "learning_rate": 2.1737927217379274e-05,
      "loss": 0.4965,
      "step": 4090
    },
    {
      "epoch": 1.70195101701951,
      "grad_norm": 3.061441421508789,
      "learning_rate": 2.1668742216687423e-05,
      "loss": 0.5015,
      "step": 4100
    },
    {
      "epoch": 1.7061021170610213,
      "grad_norm": 3.752147912979126,
      "learning_rate": 2.159955721599557e-05,
      "loss": 0.5112,
      "step": 4110
    },
    {
      "epoch": 1.7102532171025322,
      "grad_norm": 3.3422789573669434,
      "learning_rate": 2.1530372215303723e-05,
      "loss": 0.4726,
      "step": 4120
    },
    {
      "epoch": 1.714404317144043,
      "grad_norm": 2.9641103744506836,
      "learning_rate": 2.1461187214611872e-05,
      "loss": 0.4681,
      "step": 4130
    },
    {
      "epoch": 1.7185554171855542,
      "grad_norm": 2.278608798980713,
      "learning_rate": 2.1392002213920024e-05,
      "loss": 0.4445,
      "step": 4140
    },
    {
      "epoch": 1.7227065172270652,
      "grad_norm": 3.4319615364074707,
      "learning_rate": 2.1322817213228176e-05,
      "loss": 0.41,
      "step": 4150
    },
    {
      "epoch": 1.7268576172685761,
      "grad_norm": 3.001133680343628,
      "learning_rate": 2.125363221253632e-05,
      "loss": 0.532,
      "step": 4160
    },
    {
      "epoch": 1.7310087173100872,
      "grad_norm": 3.2691376209259033,
      "learning_rate": 2.1184447211844473e-05,
      "loss": 0.4146,
      "step": 4170
    },
    {
      "epoch": 1.7351598173515983,
      "grad_norm": 2.0704023838043213,
      "learning_rate": 2.1115262211152622e-05,
      "loss": 0.4526,
      "step": 4180
    },
    {
      "epoch": 1.7393109173931092,
      "grad_norm": 2.0458595752716064,
      "learning_rate": 2.1046077210460774e-05,
      "loss": 0.4722,
      "step": 4190
    },
    {
      "epoch": 1.74346201743462,
      "grad_norm": 2.8366994857788086,
      "learning_rate": 2.0976892209768923e-05,
      "loss": 0.5019,
      "step": 4200
    },
    {
      "epoch": 1.7476131174761311,
      "grad_norm": 3.2579550743103027,
      "learning_rate": 2.0907707209077075e-05,
      "loss": 0.4725,
      "step": 4210
    },
    {
      "epoch": 1.7517642175176422,
      "grad_norm": 4.379652976989746,
      "learning_rate": 2.0838522208385224e-05,
      "loss": 0.5114,
      "step": 4220
    },
    {
      "epoch": 1.755915317559153,
      "grad_norm": 2.166032314300537,
      "learning_rate": 2.0769337207693372e-05,
      "loss": 0.4317,
      "step": 4230
    },
    {
      "epoch": 1.7600664176006642,
      "grad_norm": 2.945664644241333,
      "learning_rate": 2.070015220700152e-05,
      "loss": 0.5159,
      "step": 4240
    },
    {
      "epoch": 1.7642175176421753,
      "grad_norm": 3.420860767364502,
      "learning_rate": 2.0630967206309673e-05,
      "loss": 0.4949,
      "step": 4250
    },
    {
      "epoch": 1.7683686176836861,
      "grad_norm": 1.7721894979476929,
      "learning_rate": 2.0561782205617825e-05,
      "loss": 0.4559,
      "step": 4260
    },
    {
      "epoch": 1.7725197177251972,
      "grad_norm": 2.1961071491241455,
      "learning_rate": 2.0492597204925974e-05,
      "loss": 0.4477,
      "step": 4270
    },
    {
      "epoch": 1.7766708177667083,
      "grad_norm": 3.1930465698242188,
      "learning_rate": 2.0423412204234126e-05,
      "loss": 0.5124,
      "step": 4280
    },
    {
      "epoch": 1.7808219178082192,
      "grad_norm": 4.523341655731201,
      "learning_rate": 2.0354227203542274e-05,
      "loss": 0.432,
      "step": 4290
    },
    {
      "epoch": 1.78497301784973,
      "grad_norm": 3.2622928619384766,
      "learning_rate": 2.0285042202850423e-05,
      "loss": 0.4671,
      "step": 4300
    },
    {
      "epoch": 1.7891241178912412,
      "grad_norm": 4.29892110824585,
      "learning_rate": 2.021585720215857e-05,
      "loss": 0.4671,
      "step": 4310
    },
    {
      "epoch": 1.7932752179327522,
      "grad_norm": 4.290506362915039,
      "learning_rate": 2.0146672201466724e-05,
      "loss": 0.5046,
      "step": 4320
    },
    {
      "epoch": 1.797426317974263,
      "grad_norm": 3.5313756465911865,
      "learning_rate": 2.0077487200774872e-05,
      "loss": 0.453,
      "step": 4330
    },
    {
      "epoch": 1.8015774180157742,
      "grad_norm": 2.703151226043701,
      "learning_rate": 2.0008302200083024e-05,
      "loss": 0.4884,
      "step": 4340
    },
    {
      "epoch": 1.8057285180572853,
      "grad_norm": 2.2956504821777344,
      "learning_rate": 1.9939117199391173e-05,
      "loss": 0.4935,
      "step": 4350
    },
    {
      "epoch": 1.8098796180987962,
      "grad_norm": 3.2790794372558594,
      "learning_rate": 1.986993219869932e-05,
      "loss": 0.4558,
      "step": 4360
    },
    {
      "epoch": 1.814030718140307,
      "grad_norm": 2.8048112392425537,
      "learning_rate": 1.980074719800747e-05,
      "loss": 0.4206,
      "step": 4370
    },
    {
      "epoch": 1.8181818181818183,
      "grad_norm": 4.562915325164795,
      "learning_rate": 1.9731562197315622e-05,
      "loss": 0.4967,
      "step": 4380
    },
    {
      "epoch": 1.8223329182233292,
      "grad_norm": 2.1316170692443848,
      "learning_rate": 1.9662377196623774e-05,
      "loss": 0.3977,
      "step": 4390
    },
    {
      "epoch": 1.82648401826484,
      "grad_norm": 4.293644428253174,
      "learning_rate": 1.9593192195931923e-05,
      "loss": 0.5131,
      "step": 4400
    },
    {
      "epoch": 1.8306351183063512,
      "grad_norm": 4.13437557220459,
      "learning_rate": 1.9524007195240075e-05,
      "loss": 0.5144,
      "step": 4410
    },
    {
      "epoch": 1.8347862183478623,
      "grad_norm": 4.4387335777282715,
      "learning_rate": 1.9454822194548224e-05,
      "loss": 0.4702,
      "step": 4420
    },
    {
      "epoch": 1.8389373183893731,
      "grad_norm": 3.928175926208496,
      "learning_rate": 1.9385637193856372e-05,
      "loss": 0.4851,
      "step": 4430
    },
    {
      "epoch": 1.8430884184308842,
      "grad_norm": 4.335146903991699,
      "learning_rate": 1.931645219316452e-05,
      "loss": 0.4327,
      "step": 4440
    },
    {
      "epoch": 1.8472395184723953,
      "grad_norm": 3.875185489654541,
      "learning_rate": 1.9247267192472673e-05,
      "loss": 0.4896,
      "step": 4450
    },
    {
      "epoch": 1.8513906185139062,
      "grad_norm": 3.57842755317688,
      "learning_rate": 1.9178082191780822e-05,
      "loss": 0.4265,
      "step": 4460
    },
    {
      "epoch": 1.855541718555417,
      "grad_norm": 2.508277654647827,
      "learning_rate": 1.9108897191088974e-05,
      "loss": 0.4419,
      "step": 4470
    },
    {
      "epoch": 1.8596928185969281,
      "grad_norm": 1.753085732460022,
      "learning_rate": 1.9039712190397122e-05,
      "loss": 0.4545,
      "step": 4480
    },
    {
      "epoch": 1.8638439186384392,
      "grad_norm": 4.329154968261719,
      "learning_rate": 1.897052718970527e-05,
      "loss": 0.4404,
      "step": 4490
    },
    {
      "epoch": 1.86799501867995,
      "grad_norm": 2.7261669635772705,
      "learning_rate": 1.8901342189013423e-05,
      "loss": 0.4635,
      "step": 4500
    },
    {
      "epoch": 1.8721461187214612,
      "grad_norm": 2.192056655883789,
      "learning_rate": 1.8832157188321572e-05,
      "loss": 0.4237,
      "step": 4510
    },
    {
      "epoch": 1.8762972187629723,
      "grad_norm": 3.366236925125122,
      "learning_rate": 1.8762972187629724e-05,
      "loss": 0.4747,
      "step": 4520
    },
    {
      "epoch": 1.8804483188044832,
      "grad_norm": 2.831012487411499,
      "learning_rate": 1.8693787186937873e-05,
      "loss": 0.4122,
      "step": 4530
    },
    {
      "epoch": 1.884599418845994,
      "grad_norm": 3.7511518001556396,
      "learning_rate": 1.8624602186246025e-05,
      "loss": 0.5064,
      "step": 4540
    },
    {
      "epoch": 1.8887505188875053,
      "grad_norm": 3.593442916870117,
      "learning_rate": 1.8555417185554173e-05,
      "loss": 0.4349,
      "step": 4550
    },
    {
      "epoch": 1.8929016189290162,
      "grad_norm": 2.2117984294891357,
      "learning_rate": 1.8486232184862322e-05,
      "loss": 0.3843,
      "step": 4560
    },
    {
      "epoch": 1.897052718970527,
      "grad_norm": 3.3972513675689697,
      "learning_rate": 1.841704718417047e-05,
      "loss": 0.461,
      "step": 4570
    },
    {
      "epoch": 1.9012038190120382,
      "grad_norm": 2.971210241317749,
      "learning_rate": 1.8347862183478623e-05,
      "loss": 0.3656,
      "step": 4580
    },
    {
      "epoch": 1.9053549190535493,
      "grad_norm": 5.071174144744873,
      "learning_rate": 1.827867718278677e-05,
      "loss": 0.5005,
      "step": 4590
    },
    {
      "epoch": 1.9095060190950601,
      "grad_norm": 2.9394466876983643,
      "learning_rate": 1.8209492182094923e-05,
      "loss": 0.4645,
      "step": 4600
    },
    {
      "epoch": 1.9136571191365712,
      "grad_norm": 3.9474709033966064,
      "learning_rate": 1.8140307181403075e-05,
      "loss": 0.4691,
      "step": 4610
    },
    {
      "epoch": 1.9178082191780823,
      "grad_norm": 2.3412551879882812,
      "learning_rate": 1.8071122180711224e-05,
      "loss": 0.4283,
      "step": 4620
    },
    {
      "epoch": 1.9219593192195932,
      "grad_norm": 5.521317005157471,
      "learning_rate": 1.8001937180019373e-05,
      "loss": 0.4075,
      "step": 4630
    },
    {
      "epoch": 1.926110419261104,
      "grad_norm": 3.3319509029388428,
      "learning_rate": 1.793275217932752e-05,
      "loss": 0.4199,
      "step": 4640
    },
    {
      "epoch": 1.9302615193026154,
      "grad_norm": 3.251572847366333,
      "learning_rate": 1.7863567178635673e-05,
      "loss": 0.4359,
      "step": 4650
    },
    {
      "epoch": 1.9344126193441262,
      "grad_norm": 3.2913904190063477,
      "learning_rate": 1.7794382177943822e-05,
      "loss": 0.48,
      "step": 4660
    },
    {
      "epoch": 1.938563719385637,
      "grad_norm": 2.418109655380249,
      "learning_rate": 1.7725197177251974e-05,
      "loss": 0.4255,
      "step": 4670
    },
    {
      "epoch": 1.9427148194271482,
      "grad_norm": 3.5363433361053467,
      "learning_rate": 1.7656012176560123e-05,
      "loss": 0.4675,
      "step": 4680
    },
    {
      "epoch": 1.9468659194686593,
      "grad_norm": 4.129697799682617,
      "learning_rate": 1.758682717586827e-05,
      "loss": 0.466,
      "step": 4690
    },
    {
      "epoch": 1.9510170195101701,
      "grad_norm": 5.1724958419799805,
      "learning_rate": 1.751764217517642e-05,
      "loss": 0.4215,
      "step": 4700
    },
    {
      "epoch": 1.9551681195516812,
      "grad_norm": 2.6208887100219727,
      "learning_rate": 1.7448457174484572e-05,
      "loss": 0.4136,
      "step": 4710
    },
    {
      "epoch": 1.9593192195931923,
      "grad_norm": 3.7060048580169678,
      "learning_rate": 1.7379272173792724e-05,
      "loss": 0.4844,
      "step": 4720
    },
    {
      "epoch": 1.9634703196347032,
      "grad_norm": 3.005657196044922,
      "learning_rate": 1.7310087173100873e-05,
      "loss": 0.4955,
      "step": 4730
    },
    {
      "epoch": 1.967621419676214,
      "grad_norm": 3.3614261150360107,
      "learning_rate": 1.7240902172409025e-05,
      "loss": 0.4373,
      "step": 4740
    },
    {
      "epoch": 1.9717725197177252,
      "grad_norm": 2.143282651901245,
      "learning_rate": 1.7171717171717173e-05,
      "loss": 0.4561,
      "step": 4750
    },
    {
      "epoch": 1.9759236197592362,
      "grad_norm": 3.9337239265441895,
      "learning_rate": 1.7102532171025322e-05,
      "loss": 0.453,
      "step": 4760
    },
    {
      "epoch": 1.9800747198007471,
      "grad_norm": 4.027608394622803,
      "learning_rate": 1.703334717033347e-05,
      "loss": 0.464,
      "step": 4770
    },
    {
      "epoch": 1.9842258198422582,
      "grad_norm": 3.4445927143096924,
      "learning_rate": 1.6964162169641623e-05,
      "loss": 0.4801,
      "step": 4780
    },
    {
      "epoch": 1.9883769198837693,
      "grad_norm": 4.445283889770508,
      "learning_rate": 1.689497716894977e-05,
      "loss": 0.4626,
      "step": 4790
    },
    {
      "epoch": 1.9925280199252802,
      "grad_norm": 4.049284934997559,
      "learning_rate": 1.6825792168257924e-05,
      "loss": 0.4423,
      "step": 4800
    },
    {
      "epoch": 1.996679119966791,
      "grad_norm": 5.037245273590088,
      "learning_rate": 1.6756607167566072e-05,
      "loss": 0.4784,
      "step": 4810
    }
  ],
  "logging_steps": 10,
  "max_steps": 7227,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 8.529314154283008e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
